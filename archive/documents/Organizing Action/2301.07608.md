---
title: 2301.07608
author: Human-Timescale Adaptation in an
slug: 2301.07608
reconversion_status: ready_for_reconversion
---
## ![](_page_0_Picture_1.jpeg)

# Human-Timescale Adaptation in an Open-Ended Task Space

## Adaptive Agents Team<sup>1</sup>

Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.

![](_page_0_Picture_6.jpeg)

## ![](_page_0_Picture_7.jpeg)

Figure 1 | **Human timescale adaptation.** Example trajectories of our agent (AdA) solving a held-out task in a complex 3D environment within minutes of test-time experience without any further agent training. Initial trials (**Exploration**) show a policy that uncovers hidden environment dynamics. After just seconds of test-time experience (**Success**), AdA finds a valid solution to the task. Later (**Refinement**), it improves this solution, gradually finding a more rewarding behaviour. The solid white lines show agent movement. The dashed coloured lines show the agent carrying an object of the corresponding colour. For a full description of the task, see Figure B.1. Videos of AdA's behaviour are available on our microsite and accompanying results reel.

## <sup>&</sup>lt;sup>1</sup>Deep Mind

# **1. Introduction**

The ability to adapt in minutes is a defining characteristic of human intelligence and an important milestone on the path towards general intelligence. Given any level of bounded rationality, there will be a space of tasks in which it is impossible for agents to succeed by just generalising their policy zero-shot, but where progress is possible if the agent is capable of very fast in-context learning from feedback. To be useful in the real world, and in interaction with humans, our artificial agents should be capable of fast and flexible adaptation given only a few interactions, and should continue to adapt as more data becomes available. Operationalising this notion of adaptation, we seek to train an agent that, given few episodes in an unseen environment at test time, can accomplish a task that requires trial-and-error exploration and can subsequently refine its solution towards optimal behaviour.

Meta-RL has been shown to be effective for fast in-context adaptation (e.g. Yu et al. \(2020\); Zintgraf \(2022\)). However, meta-RL has had limited success in settings where the reward is sparse and the task space is vast and diverse \(Yang et al., 2019\). Outside RL, *foundation models* in semisupervised learning have generated significant interest \(Bommasani et al., 2021\) due to their ability to adapt in few shots from demonstrations across a broad range of tasks. These models are designed to provide a strong foundation of general knowledge and skills that can be built upon and adapted to new situations via fine-tuning or prompting with demonstrations \(Brown et al., 2020\). Crucial to this success has been attention-based memory architectures like Transformers \(Vaswani et al., 2017\), which show power-law scaling in performance with the number of parameters \(Tay et al., 2022\).

![](_page_1_Figure_4.jpeg)

Figure 2 | **Training our Adaptive Agent (AdA).** We train a large Transformer model with meta-RL in XLand. During training, tasks are uniformly sampled, and subsequently filtered to produce an ever-changing training pool of tasks at the frontier of the agent's capabilities. After training on these tasks, the agent is capable of adapting to unseen hand-authored tasks as effectively and efficiently as humans.

In this work, we pave the way for training an RL foundation model; that is, an agent that has been pre-trained on a vast task distribution and that, at test time, can adapt few-shot to a broad range of downstream tasks. We introduce *Adaptive Agent* (AdA), an agent capable of human-timescale adaptation in a vast open-ended task space with sparse rewards. AdA does not require any prompts \(Reed et al., 2022\), fine-tuning \(Lee et al., 2022\) or access to offline datasets \(Laskin et al., 2022; Reed et al., 2022\). Instead, AdA exhibits hypothesis-driven exploratory behaviour, using information gained on-the-fly to refine its policy and to achieve close to optimal performance. AdA acquires knowledge efficiently, adapting in minutes on challenging held-out sparse-reward tasks in a partially-observable 3D environment with a first-person pixel observation. A human study confirms that the timescale of

AdA's adaptation is comparable to that of trained human players. AdA's adaptation behaviour in a representative held-out task can be seen in Figure 1. AdA can also achieve improved performance through zero-shot prompting with first-person demonstrations, analogously to foundation models in the language domain.

We use Transformers as an architectural choice to scale in-context fast adaptation via model-based RL<sup>2</sup> \(Duan et al., 2017; Melo, 2022; Wang et al., 2016\). Foundation models typically require large, diverse datasets to achieve their generality \(Brown et al., 2020; Mahajan et al., 2018; Schuhmann et al., 2022; Sun et al., 2017; Zhai et al., 2022\). To make this possible in an RL setting, where agents collect their own data, we extend the recent XLand environment \(OEL Team et al., 2021\), producing a vast open-ended world with over 10<sup>40</sup> possible tasks. These tasks require a range of different online adaptation capabilities, including experimentation, navigation, coordination, division of labour and coping with irreversibility. Given the wide range of possible tasks, we make use of adaptive auto-curricula, which prioritise tasks at the frontier of an agent's capabilities \(Jiang et al., 2021a; OEL Team et al., 2021\). Finally, we make use of distillation \(Schmitt et al., 2018\), which enables scaling to models with over 500M parameters, to the best of our knowledge the largest model trained from scratch with RL at the time of publication \(Ota et al., 2021\). A high level overview of our method is shown in Figure 2.

Our main contributions are as follows:

- We introduce AdA, an agent capable of human-timescale adaptation in a wide range of challenging tasks.
- We train AdA using meta-RL at scale in an open-ended task space with an automated curriculum.
- We show that adaptation is influenced by memory architecture, curriculum, and the size and complexity of the training task distribution.
- We produce scaling laws in both model size and memory, and demonstrate that AdA improves its performance with zero-shot first-person prompting.

# **2. Adaptive Agent (AdA)**

To achieve human timescale adaptation across a vast and diverse task space, we propose a general and scalable approach for memory-based meta-RL, producing an *Adaptive Agent* (AdA). We train and test AdA in XLand 2.0, an environment supporting procedural generation of diverse 3D worlds and multi-player games, with rich dynamics that necessitate adaptation. Our training method combines three key components: a curriculum to guide the agent's learning, a model-based RL algorithm to train agents with large-scale attention-based memory, and distillation to enable scaling. An overview of our approach is shown in Figure 2. In the following sections, we describe each component and how it contributes to efficient few-shot adaptation.

## **2.1. Open-ended task space: XLand 2.0**

In order to demonstrate fast adaptation across an open-ended task space, we extend the procedurallygenerated 3D environment XLand \(OEL Team et al., 2021\), which we refer to here as XLand 1.0. In XLand, a task consists of a game, a world, and a list of co-player policies (if any). The game consists of a goal per player, defined as a boolean function (predicate) on the environment state. An agent receives reward if and only if the goal is satisfied. Goals are defined in a synthetic language, and the agent receives an encoding. The world specifies a static floor topology, objects the player can interact with, and spawn locations for players. The agent observes the world, and any co-players therein, via a first-person pixel observation. All fundamental details of the game, world and co-player system are

![](_page_3_Figure_1.jpeg)

Figure 3 | **XLand 2.0: a vast, smooth and diverse task space of adaptation problems.** Different tasks have different adaptation requirements, such as experimentation, tool use or division of labour. For instance, in a task requiring experimentation, a player might be required to identify which objects can usefully combine, avoiding dead-ends, and then optimise the way in which they combine objects, like a toy version of experimental chemistry. Each task can be run for one or more trials, where the environment is reset between trials, but agent memory is not. Highlighted are two example tasks, Wrong Pair Disappears and Pass Over Wall Repeatedly, showing the goal, initial objects, production rules ("rules" in the figure) and how agents need to interact with them to solve the task. For full task descriptions see Appendix F.1.

inherited from the original XLand; see OEL Team et al. \(2021\) for a full description and Appendix A.1 for details of the new features we added.

XLand 2.0 extends XLand 1.0 with a system called *production rules*. Each production rule expresses an additional environment dynamic, leading to a much richer and more diverse array of different transition functions than in XLand 1.0. The production rules system can be thought of as a domainspecific language (DSL) to express this diverse array of dynamics. Each production rule consists of:

- 1. A condition, which is a predicate, for example near(yellow sphere, black cube),
- 2. A (possibly empty) list of spawns, which are objects, like purple cube, black cube.

When condition is satisfied, the objects present in condition get removed from the environment, and the ones in spawns appear. Each game can have multiple production rules. Production rules can be observable to players, or partially or fully masked, depending on the task configuration. More precisely, there are three distinct mechanisms for hiding production rule information from the players:

1. Hiding a full production rule, where the player only gets information that a rule exists, but neither knows the condition nor what spawns.

- 2. Hiding an object, where a particular object is hidden from all production rules. The hidden objects are numbered such that if multiple objects are hidden, the agent can distinguish them.
- 3. Hiding a condition's predicate, where the agent gets to know the objects that need to satisfy *some* predicate, but it does not know which one. The hidden predicates are also numbered.

Instead of procedurally generating tasks on the fly, we pre-sample a large pool of tasks. For more details about the specific mechanism we use for pre-sampling tasks, see Appendix A.2. We visualise the XLand 2.0 task space in Figure 3.

### **2.2. Meta-RL**

We use a black-box meta-RL problem setting \(Duan et al., 2017; Wang et al., 2016\). We define the task space M to be a set of partially-observable Markov decision processes (POMDPs). For a given task ∈ M we define a *trial* to be any sequence of transitions from an initial state <sup>0</sup> to a terminal state . 1 In XLand, tasks terminate if and only if a certain time period ∈ [10s, 40s] has elapsed, specified per-task. The environment ticks at 30 frames-per-second and the agent observes every 4 th frame, so task lengths in units of timesteps lie in the range [75, 300].

An *episode* consists of a sequence of trials for a given task . At trial boundaries, the task is reset to an initial state. In our domain, initial states are deterministic except for the rotation of the agent, which is sampled uniformly at random. The trial and episode structure is depicted in Figure 3.

In black-box meta-RL training, an agent uses experience of interacting with a wide distribution of tasks to update the parameters of its neural network, which parameterises the agent's policy distribution over actions given a state observation. If an agent possesses dynamic internal state (memory), then meta-RL training endows that memory with an implicit online learning algorithm, by leveraging the structure of repeated trials \(Mikulik et al., 2020\).

At test time, this online learning algorithm enables the agent to adapt its policy without any further updates to the neural network weights. Therefore, the memory of the agent is not reset at trial boundaries, but is reset at episode boundaries. To generate an episode, we sample a pair (, ) where ∈ {1, 2, . . . 6}. As we will discuss later, at test time AdA is evaluated on unseen, held-out tasks across a variety of values, including on held-out not seen during training. For full details on AdA's meta-RL method, see Appendix D.1.

#### **2.3. Auto-curriculum learning**

Given the vastness and diversity of our pre-sampled task pool, it is challenging for an agent to learn effectively with uniform sampling. Most randomly sampled tasks are likely going to be too hard (or too easy) to benefit an agent's learning progress. Instead, we use automatic approaches to select "interesting" tasks at the frontier of the agent's capabilities, analogous to the "zone of proximal development" in human cognitive development \(Vygotsky, 1978\). We propose extensions to two existing approaches, both of which strongly improve agent performance and sample efficiency (see Section 3.3\), and lead to an emergent curriculum, selecting tasks with increasing complexity over time.

**No-op filtering.** We extend the dynamic task generation method proposed in OEL Team et al. \(2021, Section 5.2) to our setup. When a new task is sampled from the pool, it is first evaluated to assess

<sup>1</sup>Note that we use a reversed naming convention to Duan et al. \(2017\). In our convention, the term "trial" maps well onto the related concept in the human behavioural literature \(Barbosa et al., 2022\).

whether AdA can learn from it. We evaluate AdA's policy and a "No-op" control policy (which takes no action in the environment) for a number of episodes. The task is used for training if and only if the scores of the two policies meet a number of conditions. We expanded the list of conditions from the original no-op filtering and used normalised thresholds to account for different trial durations. See Appendix D.5 for further details.

**Prioritised level replay (PLR).** We modify "Robust PLR" (referred to here as *PLR*, Jiang et al. (2021a)) to fit our setup. By contrast to no-op filtering, PLR uses a *fitness score* (Schmidhuber, 1991) that approximates the agent's regret for a given task. We consider several potential estimates for agent regret, ranging from TD errors as used in Jiang et al. (2021b), to novel approaches using dynamics-model errors from AdA (see Appendix D.5 and Figure D.1).

PLR operates by maintaining a fixed-sized archive containing tasks with the highest fitness. We only train AdA on tasks sampled from the archive, which occurs with probability p. With probability 1-p, a new task is randomly sampled and evaluated, and the fitness is compared to the lowest value in the archive. If the new task has higher fitness, it is added to the archive, and the lowest fitness task is dropped. Thus, PLR can also be seen as a form of filtering, using a dynamic criteria (the lowest fitness value of the archive). It differs to no-op filtering in that tasks can be repeatedly sampled from the archive as long as they maintain high fitness. To apply PLR in our heterogeneous task space, we normalise fitness at each trial index by using rolling means and variances, and use the mean per-timestep fitness value rather than the sum, to account for varying trial duration. Finally, since we are interested in tasks at the frontier of an agent's capabilities after across-trial adaptation, we use only the fitness from the last trial. See Appendix D.5 for further details.

#### 2.4. RL agent

**Learning algorithm.** We use Muesli (Hessel et al., 2021) as our RL algorithm. We briefly describe the algorithm here, but refer the reader to the original publication for details. Taking a history-dependent encoding as input, in our case the output of an RNN or Transformer, AdA learns a sequence model (an LSTM) to predict the values $\hat{v_i}$ , action-distributions $\hat{\pi_i}$ and rewards $\hat{r_i}$ for the next I steps. Here, $i = 0, \ldots, I$ denotes the prediction i steps ahead. I is typically small and in our case I = 4. For each observed step t, the model is unrolled for I steps and updated towards respective targets:

$$\mathcal{L}_{r}^{t} = \sum_{i=0}^{I} (\hat{r}_{i}^{t} - r_{t+i})^{2}, \ \mathcal{L}_{v}^{t} = \sum_{i=0}^{I} (\hat{v}_{i}^{t} - G_{t+i})^{2}, \ \mathcal{L}_{\pi}^{t} = \sum_{i=0}^{I} \text{KL} \left( \pi_{\text{CMPO}}^{t+i} \parallel \hat{\pi}_{i}^{t} \right).$$
## (1)

Here, $r_{t+i}$ refers to the observed rewards. $G_{t+i}$ refers to value-targets which are obtained using Retrace (Munos et al., 2016) based on Q-values obtained from one-step predictions of the model.

The action-targets $\pi^t_{\text{CMPO}}$ are obtained by re-weighting the current policy<sup>2</sup> using clipped, normalised, exponentially transformed advantages. Muesli furthermore incorporates an additional auxiliary policy-gradient loss based on these advantages to help optimise immediate predictions of action-probabilities. Finally, Muesli maintains a target network which trails the sequence model and is used for acting and to compute Retrace targets and advantages.

**Memory architecture.** Memory is a crucial component for adaptation as it allows the agent to store and recall information learned and experienced in the past. In order for agents to effectively adjust to

<sup>&</sup>lt;sup>2</sup>The prior distribution is actually a mixture of the current estimate of the policy, the (outdated) policy used to produce the sample and the uniform distribution where the latter two are mixed in as regularisers.

![](_page_6_Figure_1.jpeg)

Figure 4 | **Agent architecture.** For each timestep, we embed and combine the pixel observation, goal, hand, trial and time information, production rules, previous action, and previous reward into a single vector. These observations embeddings pass in sequence to the Transformer-XL, whose output embeddings feed into an MLP value head, MLP policy head, and the Muesli LSTM model step (omitted in the diagram for brevity). See Appendix C.1 for more details about our agent architecture.

the changes in task requirements, memory should allow the agent to recall information from both the very recent and the more distant past. While slow gradient-based updates are able to capture the latter, they are often not fast enough to capture the former, i.e. fast adaptation. The majority of work on memory-based meta-RL has relied on RNNs as a mechanism for fast adaptation (Parisotto, 2021). In this work, we show that RNNs are not capable of adaptation in our challenging partially-observable embodied 3D task space. We experiment with two memory architectures to address this problem:

- 1. *RNN with Attention* stores a number of past activations (in our case 64) in an episodic memory and attends over it, using the current hidden state as query. The output of the attention module is then concatenated with the hidden state and fed into the RNN. We increase effective memory length of the agent by storing only every 8<sup>th</sup> activation in its episodic memory.<sup>3</sup>
- 2. *Transformer-XL (TXL)* (Dai et al., 2019) is a variant of the Transformer architecture (Vaswani et al., 2017) which enables the use of longer, variable-length context windows to increase the model's ability to capture long-term dependencies. To increase the stability of training Transformers with RL, we follow Parisotto et al. (2020) in performing normalisation *before* each layer, and use gating on the feedforward layers as in Shazeer (2020).

Both memory modules operate on a sequence of learned timestep embeddings, and produce a sequence of output embeddings that are fed into the Muesli architecture, as shown in Figure 4 with a Transformer-XL module. In Section 3.2 we show that both attention-based memory modules significantly outperform a vanilla RNN in tasks that require adaptation. Transformer-XL performs the best and therefore is used as the default memory architecture in all our experiments unless stated otherwise.

<sup>&</sup>lt;sup>3</sup>We arrived at these numbers as a compromise between performance and speed. Note that the resulting architecture is slower than an equivalently sized Transformer.

**Going beyond few shots.** We propose a simple modification to our Transformer-XL architecture to increase the effective memory length without additional computational cost. Since observations in visual RL environments tend to be highly temporally correlated, we propose sub-sampling the sequence as described for RNN with Attention, allowing the agent to attend over 4 times as many trials. To ensure that observations which fall between the sub-sampled points can still be attended to, we first encode the entire trajectory using an RNN with the intention of summarising recent history at every step. We show that the additional RNN encoding does not affect the performance of our Transformer-XL variant but enables longer range memory (see Section 3.7\).

## **2.5. Distillation**

For the first four billion steps of training, we use an additional distillation loss \(Czarnecki et al., 2019; Schmidhuber, 1992; Schmitt et al., 2018\) to guide AdA's learning with the policy of a pre-trained teacher, in a process known as kickstarting; iterating this process leads to a generational training regime \(OEL Team et al., 2021; Wang et al., 2021\). The teacher is pre-trained from scratch via RL, using an identical training procedure and hyperparameters as AdA, apart from the lack of initial distillation and a smaller model size (23M Transformer parameters for the teacher and 265M for multi-agent AdA). Unlike aforementioned prior work, we do not employ shaping rewards or Population Based Training (PBT, Jaderberg et al. \(2017\)) in earlier generations. During distillation, AdA acts according to its own policy and the teacher provides target logits given the trajectories observed by AdA. Distillation allows us to amortise an otherwise costly initial training period, and it allows the agent to overcome harmful representations acquired in the initial phases of training; see Section 3.6.

To integrate the distillation loss with Muesli, we unroll the model from every transition observed by the student. We minimise the KL-divergence between all of the action-probabilities predicted by the model and the action-probabilities predicted by the teacher's policy at the corresponding timestep. Analogously to Muesli's policy-loss L defined in \(1\), we define

$$\mathcal{L}_{\text{dist}} = \sum_{i=0}^{I} \text{KL}\left(\left.\tilde{\pi}_{0}^{t+i} \right\| \right. \hat{\pi}_{i}^{t}\right), \tag{2}$$

where ˜ corresponds to the predicted action-logits provided by the teacher given the same observed history. Furthermore, we found it useful to add additional 2 regularisation during distillation.

# **3. Experiments and Results**

We evaluate our agents in two distinct regimes: on a set of 1000 *test tasks* sampled from the same distribution as the training tasks, and on a set of 30 single-agent and 28 multi-agent *hand-authored probe tasks*. A rejection sampling procedure guarantees that the procedural test tasks and probe tasks are outside the training set. The probe tasks represent situations that are particularly intuitive to humans, and deliberately cover a wide range of qualitatively different adaptation behaviours. Example probe tasks are depicted in Figures B.1 to B.3 in the Appendix, and a full description of every probe task is available in Appendix F.

The total achievable reward on each task varies, so whenever we present aggregated results on the test or hand-authored task set, we normalise the total per-trial reward for each task against the reward obtained by fine-tuning AdA on the respective task set. We refer to this normalised reward as a *score*. We stipulate that an adaptive agent must have two capabilities: zero-shot generalisation and few-shot adaptation. Zero-shot *generalisation* is assessed by the score in the case of only being given 1 trial of interaction with a held-out task. Few-shot *adaptation* is assessed by the improvement

![](_page_8_Figure_1.jpeg)

Figure 5 | **Zero-shot generalisation and few-shot adaptation.** We report the distribution of normalised task scores over the single-agent test set when evaluated with various numbers of trials. On the -axis is the total last-trial reward relative to that of an agent fine-tuned on the test tasks (approximating "infinite trials" performance). Curves moving further towards the top right corner indicate better performance. When given more trials, the agent achieves higher scores in the last trial, showing test-time adaptation across most of the task distribution (shaded regions). The dashed line indicates the zero-shot performance of an agent trained in a regime where every episode consists of only a single trial.

in score as the agent is given progressively more trials () of interaction with the task. More precisely, for each we report the score in the last trial, showing whether or not an agent is able to make use of additional experience on-the-fly to perform better, i.e. measuring adaptation.

We aggregate scores across a task set using (one or more) percentiles. When presenting individual probe tasks we report unnormalised total last trial rewards per task for agents and for human players where applicable. For full details of our evaluation methodology see Appendix B.

The space of training configurations for AdA is large, comprising model size, auto-curriculum, memory architecture, memory length, number of tasks in the XLand task pool, single vs multi-agent tasks, distillation teacher, and number of training steps. We use a consistent training configuration within each experimental comparison, but different configurations across different experimental comparisons. We therefore caution the reader against directly comparing results between different sections. For convenience, all experimental configurations are tabulated in Appendix D.

#### **3.1. AdA shows human-timescale adaptation**

**Single-agent.** In Figure 5 we show the performance of AdA when trained in the single-agent setting described in Table 1. Examine first AdA's zero-shot performance ( = 1, red line). This matches the performance of a baseline agent, trained only in a regime where each episode consists of a single trial. In other words, AdA does not suffer any degradation in zero-shot performance, despite being

Table 1 | Experimental setup for agent experiments in Section 3.1.

| # players | Model parameters | Memory | Task pool | Curriculum | Teacher | Steps |
|-----------|-----------------------|--------|--------------|------------|---------|-------|
| 1 | 169M TXL / 353M total | 1800 | 25B | PLR D.5 | D.1 | 100B |
| 2 | 265M TXL / 533M total | 1800 | see App. D.3 | PLR D.5 | D.2 | 70B |

![](_page_9_Figure_3.jpeg)

Figure 6 | **Human-timescale adaptation.** We report median normalised last-trial score across 30 hand-authored tasks as a function of number of trials for AdA and human players. Both AdA and the human players improve their performance with increasing number of trials, indicating that AdA is capable of human-timescale adaptation. **(a)** shows the results using our standard per-task normalisation scheme. **(b)** re-normalises the results by the maximum score per player-type to account for systematic differences between the agent and human players. In particular, human players reported lag while playing which may have resulted in lower scores.

trained on a distribution over number of trials ∈ {1, 2, . . . 6}. Now turn your attention to AdA's few-shot performance ( ∈ {2, 3, 5, 8, 13}, orange to purple lines). Given more trials, AdA improves its performance on over 80% of the task set, clearly adapting at test time. The improvements are particularly strong when comparing zero-shot performance to the two trial setting, but AdA keeps on improving when given more trials.

We compare the performance of AdA to that of a set of human players on 30 held-out handauthored probe tasks, seeking to assess whether AdA adapts on the same timescale as humans. Figure 6a shows the median scores for AdA and for human players as a function of number of trials. Both AdA and human players were able to improve their score as they experienced more trials of the tasks, indicating that AdA exhibits human-timescale adaptation on this set of probe tasks. We provide more details of the scores obtained on each task in Figure F.1. This reveals a small set of tasks which humans can solve but AdA can't, such as the Spacer Tool task: in this task one object must be used as a tool to move another, a situation which is extremely rare in XLand. There are also tasks like Small Workstation, All Rules Visible which can be solved by AdA but not by humans, likely due to complex control requirements. The majority of tasks, however, show adaptation from both humans and AdA, with the slopes of AdA's score being as steep as, if not steeper than, those of the human players, especially for lower numbers of trials. For full details of our human experiment design, see Appendix B.4.

Figure 7 analyses the behaviour of AdA in more detail on a specific held-out task. The increase in score with a larger number of trials indicates that the task is solved more consistently and more quickly when given a larger number of trials. Examining the trajectories for different numbers of trials,

![](_page_10_Figure_1.jpeg)

Figure 7 | **Experimentation, success and refinement.** We report average performance and representative behaviour of AdA on the probe task Wrong Pair Disappears when evaluated with various numbers of trials. AdA's performance increases when given more trials, showing test-time adaptation. The top-down view images show representative last-trial trajectories when given different numbers of total trials. A corresponding [video](https://youtu.be/FCDLu4iTBGE) for the case = 3 shows the behaviour across all trials within one episode.

we can explain this effect in terms of the behaviour of AdA. When given 1 or 2 trials AdA's behaviour shows structured hypothesis-driven exploration: trying out different combinations of objects and coming across the solution or a dead end. Once the solution is found, AdA refines its strategy on subsequent trials, gathering the correct objects with more efficiency and combining them in the right way. Thus AdA is able to generate a higher last-trial score when provided with more trials for refinement. When given 8 trials, the last-trial performance is close to that of the fine-tuned agent. We observe this pattern of behaviour consistently across many of our held-out probe tasks; see videos on our [microsite.](http://sites.google.com/view/adaptive-agent/)

**Multi-agent.** We train a separate agent on a mixture of fully-cooperative multi-agent and singleagent tasks to explore adaptation in the multi-agent setting. In fully-cooperative multi-agent tasks, both players have the same goal. Such tasks typically have multiple Nash equilibria \(Dafoe et al., 2020\). When faced with a new problem, agents must adapt on-the-fly to agree on a single equilibrium of maximal mutual benefit \(Christianos et al., 2022; Hu et al., 2020; Stone et al., 2010\). This gives rise to a variety of interesting strategic novelties that are absent in the purely single-agent setting, including emergent division-of-labour and physical coordination. Both of these behaviours have received extensive study in the multi-agent RL literature (e.g. Gronauer and Diepold \(2022\); Strouse et al. \(2021\); Wang et al. \(2020b\); Yang et al. \(2020\)); here for the first time to our knowledge, we demonstrate that these behaviours can emerge at test time in few-shot on held-out tasks. Co-players for our training tasks are generated using fictitious self-play \(Heinrich et al., 2015\) and then curated using PLR, as in Samvelyan et al. \(2022\). For more details, see Table 1 and Appendix D.3.

Analogously with the single-agent setting, we find strong evidence of adaptation across almost 90% of the space of held-out test tasks (Figure E.1\). Futhermore, we evaluate the resulting agent on a held-out test set of cooperative multi-agent tasks in two ways: in self-play and in co-play with a random-action policy. As shown in Figure 8, self-play outperforms co-play with a random-action policy by a large margin both in a zero-shot and in a few-shot setting. This indicates that the agents are dividing the labour required to solve the tasks, thereby solving the task more quickly (or at all) and improving their shared performance.

Examples of emergent social behaviour in self-play are shown in Figures 9 and E.2. When given only a few trials, the agents explore the space of possible solutions, sometimes operating independently

![](_page_11_Figure_1.jpeg)

Figure 8 | **Two heads are better than one.** Cooperative self-play outperforms single-agent performance on the test set of two-player cooperative held-out tasks. For this evaluation we restrict ourselves to tasks whose goals and production rules do not refer to players and which are solvable by a single player (216/1000 test tasks). To produce the purple curve, we evaluate AdA twice per task when playing with a random-action policy co-player, once playing as the first and once as the second player, and take the maximum score over both evaluations before cross-task aggregation. This accounts for possible advantages playing as one player might have over playing as the other in a task. **(a)** Median score. **(b)** 20th percentile score.

![](_page_11_Figure_3.jpeg)

Figure 9 | **Multi-agent coordination.** We report average performance and representative behaviour of AdA on the probe task Pass Over the Wall Repeatedly when evaluated in self-play with various numbers of trials. AdA's performance increases when given more trials, showing test-time adaptation. The top-down view images show representative last-trial trajectories when given different numbers of total trials. A corresponding [video](https://youtu.be/Rnz2MFgeicc) for the case = 5 shows the behaviour across all trials within one episode.

and sometimes together. Given more trials, once the agents find a solution, they optimise their paths by coordinating physically and dividing labour to solve the task efficiently. This behaviour emerges from adaptation at test time and was not explicitly incentivised during training, other than through the high-level fully cooperative reward function. Videos of such behavior in a variety of tasks are available on our [microsite.](http://sites.google.com/view/adaptive-agent/)

![](_page_12_Figure_1.jpeg)

Figure 10 | **(a)** Adaptation over increasing numbers of trials for different choices of architectures. Incorporating attention modules is essential to achieve adaptation, with Transformer-XL architectures performing best. **(b)** Adaptation over increasing numbers of trials for different choices of curricula. No-op filtering and PLR greatly improve both zero-shot generalisation and few-shot adaptation over the uniform sampling baseline.

#### 3.2. Architecture influences performance

We now dive deeper into understanding which components of our method are critical, via a series of ablation studies. In these studies we use a single initialisation seed, because we see low variance across seeds when training AdA (see Appendix F.3). All ablations are in the single-agent setting, unless stated otherwise.

First, we empirically contrast different choices of architectures: Transformer-XL, RNN, and RNN with Attention. To implement the RNN, we use a GRU (Cho et al., 2014). To facilitate comparison, we match the total network size for all architectures. Table D.3 shows details on the experimental setup. Figure 10a shows that while the Transformer-XL is the best performing architecture in this comparison, incorporating a multi-head attention module into an RNN recovers most of the performance of the Transformer, highlighting the effectiveness of attention modules.

#### 3.3. Auto-curriculum learning improves performance

To establish the importance of automated curriculum learning, we compare adaptation when training with the curricula methods outlined in Section 2.3: no-op filtering and PLR. Figure 10b shows the median last-trial score of agents trained with different curricula. Both no-op filtering and PLR curricula strongly outperform a baseline trained with uniformly sampled tasks. Moreover, PLR outperforms No-op filtering, particularly at a higher number of trials, indicating that a regret-based curriculum is especially helpful for learning longer-term adaptation. In Appendix D.5 we detail training configuration, and also compare the sample efficiency of our methods, where we see that both auto-curriculum approaches are more sample-efficient than uniform sampling, in terms of both learning steps and FLOPs.

In Figure 11 we show the evolution of task complexity for both methods. In both cases, simpler tasks are initially prioritised, with a clear curriculum emerging. Neither method explicitly optimises to increase these metrics, yet the task complexity increases as a result of the agent's improving capabilities. See Figure D.3 for additional metrics of task complexity.

![](_page_13_Figure_1.jpeg)

Figure 11 | Emergent curricula for no-op filtering and PLR. Plots show a selection of task metrics for the dynamic training set, averaged over all tasks in the set, with standard error shaded. In all plots, a higher metric value corresponds to greater task difficulty. For example, tasks with a higher number of rules require more trial-and-error to find the correct rules to trigger. Horizontal lines show the same metric values averaged over the test (dashed) and hand-authored (dotted) evaluation task sets.

![](_page_13_Figure_3.jpeg)

Figure 12 | Scaling Transformer parameters increases both median **(a)** and 20th percentile **(b)** test score. Both axes are log-scaled, according to the functions log() and − log(1 − ), respectively, and the relationship between model size and performance appears roughly linear on this scale. The slope is steeper when evaluating higher numbers of trials, showing that scaling the model is particularly effective at encouraging stronger adaptation, as opposed to stronger zero-shot generalisation.

#### **3.4. Scaling the agent increases performance**

Methods that scale well are critical for continued progress in machine learning, and understanding how methods scale is important for deciding where to spend time and compute in the future. Scaling laws have been determined for many foundation models (see Section 4\), where performance is related to model size and other factors as a power law, which can be seen as a linear relationship on a log-log plot. Inspired by such analyses, we investigate how adaptation scales with Transformer model size and memory length.

**Scaling network size.** We show how performance scales with the size of AdA's Transformer model, experimenting with the model sizes shown in Table D.8. When investigating scaling laws for model size, we follow Kaplan et al. \(2020\) in measuring only Transformer (i.e. non-embedding) parameters, which range across 3 orders of magnitude, from 6M to 265M Transformer parameters (i.e. from 41M to 533M total parameters). A complete list of hyperparameters is shown in Table D.9.

![](_page_14_Figure_1.jpeg)

Figure 13 | Scaling Transformer-XL memory length increases both median **(a)** and 20th percentile **(b)** test score. Both axes are log-scaled, according to the functions log() and − log(1 − ), respectively, and the relationship between memory length and performance appears roughly linear on this scale. The slope is steeper when evaluating higher numbers of trials, showing that scaling the memory is particularly effective at encouraging stronger few-shot adaptation.

Figure 12 shows that larger networks increase performance, especially when given more test-time trials to adapt. Though larger models seem to help in the median test-set score (Figure 12a\), model scale particularly has impact on the lower percentiles of the test set (Figure 12b\). This indicates that larger models allow the agent to generalise its adaptation to a broader range of tasks. The roughly linear relationship between model size and performance on the log-log plot is indicative of a power law scaling relationship, albeit only shown across two to three orders of magnitude. That the curves are not exactly linear may be due to several factors: that we haven't trained to convergence (though performance increases had slowed for all models), and that we use a 23M parameter distillation teacher across experiments for all model sizes.

Appendix D.7 details the computational costs of the various model sizes, and shows FLOPs adjusted results. While larger models do indeed have better zero-shot score and adaptation than smaller ones for the same number of training steps, and are more sample efficient, the biggest model may not always be the best choice when compute cost is taken into account.

**Scaling memory length.** Performance also scales with the length of AdA's memory. The experimental setting is shown in Table D.10, where we examine the number of previous network activations we cache, investigating values from 100 to 700, which, with 6 Transformer-XL blocks, yields an effective timestep range of 600 to 4200 timesteps.4

Figure 13 shows that, as with model size, scaling memory length helps performance, especially in the lower test percentiles, pushing performance on the tails of the distribution. For any of our tasks, the maximum trial duration is 300 timesteps, so it is interesting that performance on, for example, 5 trials (1500 timesteps) continues to increase for "effective memory lengths" between 1800 and 4200. This indicates that it is easier for the Transformer-XL to make use of explicitly given memory activations rather than relying on theoretically longer-range information implicit in those activations.

<sup>4</sup>Transformer-XL enables the use of longer, variable-length context windows by concatenating a cached memory of previous attention layer inputs to the keys and values during each forward pass. Since inputs to intermediate layers are activations from the previous layer, which in themselves contain information about the past, caching activations theoretically allows for an effective memory horizon of × , where is the number of attention layers in the network.

![](_page_15_Figure_1.jpeg)

Figure 14 | Median **(a)** and 20th percentile **(b)** adaptation scales with the size of the task pool. The effect is especially prominent for larger models. We show the -axis on a logarithmic scale as in the other scaling experiments. Here, we plot number of trials on the -axis and examine the gaps between the curves for the two task distributions (triangle markers vs. circular markers).

### **3.5. Scaling the task pool increases performance**

Another important factor to scale is the amount of data a model is trained on. For example, Hoffmann et al. \(2022\) showed that in order to get the most out of scaling a language model, one must scale the amount of training data at the same rate as the number of parameters. In our case, relevant data come from interaction with different tasks, so we examine the effect of scaling the number and complexity of different tasks in the XLand pool.

**Scaling size of task pool.** Here we examine the effect of varying the number of training tasks from which the auto-curriculum can sample. Recall that in XLand, a task is the combination of a world (the physical layout of terrain and objects) and a game (specifying the goal and production rules). We investigate the effects of training on tasks sampled from a small pool of 200M distinct tasks (4,000 worlds × 50,000 games) compared with a large pool of 25B distinct tasks (50,000 worlds × 500,000 games). Table D.11 shows the full experimental setup for these comparisons.

Figure 14 shows higher test score for identically sized models on the larger task pool. As in the other scaling experiments, we especially see improved performance on the 20th percentile. The results are shown for two different sizes of models, with the larger Transformer yielding a larger gap when scaling the size of the task pool. This suggests that the large models are especially prone to overfitting to a smaller task pool.

**Scaling complexity of task pool.** One final axis along which it is possible to scale our method is the overall complexity of the task distribution. For example, tasks with a flat terrain will be, on average, less complex to solve than tasks with terrain variation. In Figure E.3, we show that low environment complexity can be a bottleneck to scaling, by comparing the effectiveness of model scaling between agents trained on two distributions of the same size but different complexity and evaluated on their respective test sets. Open-ended settings with unbounded environment complexity, such as multi-agent systems, may therefore be particularly important for scaling up adaptive agents.

![](_page_16_Figure_1.jpeg)

## ![](_page_16_Figure_2.jpeg)

Figure 15 | Adaptation over increasing numbers of trials when training from scratch or when kick-starting with distillation, for models with 23M and 265M Transformer parameters. Circle markers show training from scratch while triangle markers show training kickstarted with 4 billion frames of distillation. For this ablation, agents were trained in the multi-agent setup described in Section 3.1 and evaluated on the multi-agent test set after 22 billion total training frames. (a) Median score. (b) 20<sup>th</sup> percentile score.

![](_page_16_Figure_4.jpeg)

## ![](_page_16_Figure_5.jpeg)

Figure 16 | Normalised last-trial score for k = 13 using the 23M parameter Transformer-XL. The teacher is trained from scratch, while the otherwise identical student is distilled from a snapshot of the teacher, taken after 25 billion steps of training. The x-axis counts the combined amount of experience, including experience used to train the teacher. The comparison shows that distillation can greatly increase the performance of the student, even if the combined amount of experience and updates are equivalent. This is true for median score (a), but even more so for the $20^{th}$ percentile (b).

#### 3.6. Distillation improves performance and enables scaling agents

All of the scaling comparison experiments shown in the previous section use an identical distillation teacher for the first frames of training, as detailed in Appendix D.6. Now, we look at the role distillation plays in scaling. In short, we find that kickstarting training with a distillation period is crucial when scaling up model size. As shown in Figure 15, training a 265M parameter Transformer model without distillation results in poor performance compared to a much smaller 23M parameter Transformer trained in the same way. However, when training with distillation from a 23M parameter teacher for the first 4 billion training frames, the 265M model clearly outperforms the 23M variant. See experiment details in Appendix D.11.

Additionally, we find that even when the model size is the same for both student and teacher, we observe large gains from distillation, for a constant total frame budget (Figure 16\). We speculate that this is due to bad representations learned early on by the student agent \(Cetin et al., 2022; Nikishin et al., 2022\), which can be avoided by using distillation. This is also consistent with findings in offline RL, where additional data is often required to effectively scale the model \(Reid et al., 2022\). The effect is largest for the first round of distillation, with diminishing returns in subsequent rounds of distillation (Figure E.5\).

## **3.7. Training on more trials with skip memory enables many-shot adaptation**

So far, we have considered the few-shot regime in which we train on 1 to 6 trials and evaluate up to 13 trials. In this section, we evaluate AdA's ability to adapt over longer time horizons. We find that when trained with ∈ {1, 2, . . . 6}, agents do not continue to adapt past 13 trials; however, this long-term adaptation capability is greatly improved by increasing the maximum number of trials during training to 24 and increasing the effective length of the memory accordingly. These results show that our method naturally extends to many-shot timescales, with episodes lasting in excess of 30 minutes.5 In this section, we ablate both factors separately, and show that both are important for long-range adaptation. The training configuration (which is identical to that of the memory scaling experiments save for the number of training steps) is detailed in Table D.14.

As we noted in Section 3.4, increasing the memory length leads to increased capacity that benefits the agent even when the entire episode fits in memory, but also comes at the cost of increased computation. To disentangle these factors, we propose a simple change to the memory architecture described in Section 2.4 which increases effective memory length without increasing computational cost. We use a GRU to encode trajectories before feeding them to the Transformer-XL. This allows us to sub-sample timesteps from the encoded trajectories, enabling the agent to attend over 4 times as many trials without additional computation. We show that the additional GRU on its own does not affect the performance of the agent greatly.

As can be seen in Figure 17a, increasing the number of trials in the training distribution significantly boosts performance in later trials, especially when the memory length is scaled accordingly. In other words, the adaptation strategy learned by AdA benefits from experiencing a large number of trials, rather than just very recent ones. Therefore we can conclude that AdA is capable of adaptation based on long-term knowledge integrated into memory across many trials, as opposed to merely encoding a simple meta-strategy that only depends on the trajectory from the previous trial.

Increasing the number of trials in training leads to better adaptation even in the absence of increased memory. This indicates that the agent is able to learn better exploration and refinement strategies when afforded longer training episodes consisting of more trials. Note that increasing effective memory without increasing the number of training trials does not improve performance, as the agent has not been trained to make use of the additional memory capacity.

#### **3.8. AdA can leverage prompting with first-person demonstrations**

To determine whether AdA can learn in zero-shot from first-person demonstrations, we prompted it with a first-person demonstration by a fine-tuned teacher, as follows. The teacher took control of the avatar in the first trial, while AdA continued to receive observations as usual, conditioning its

<sup>5</sup>48 trials of a 40s task lasts for 32 minutes. By contrast, the average length of a Starcraft 2 game is between 10 and 15 minutes, and Alpha Star acted less frequently per-second than AdA does \(Vinyals et al., 2019\).

![](_page_18_Figure_1.jpeg)

Figure 17 | (a) Ablation showing the $20^{th}$ percentile of test scores as we vary the maximum number of training trials (from a k = 6 baseline to k = 24) and increase the effective memory size via subsampling (from 1800 steps to 7200 steps). Together, these factors enable the agent to adapt over a larger number of trials (lasting over 30 minutes). Increasing the number of training trials has the biggest effect and is a prerequisite for sub-sampling to be effective. This figure furthermore shows that adding an RNN encoder to facilitate sub-sampling does not by itself greatly affect performance. (b) Median hand-authored task score of AdA prompted with a first-person demonstration in the first trial of each episode, compared with an unprompted baseline. The prompted score lies strictly above the baseline which indicates that AdA is able to use information from a demonstration prompt to improve its performance. However, the score lies below that of the demonstration which suggests that it is not able to make perfect use of the demonstration.

Transformer memory. AdA was then allowed to proceed on its own for the remaining trials and its scores recorded in the usual manner.

Figure 17b shows the median score on our hand-authored test set of prompted AdA compared to an unprompted baseline. Prompted AdA is unable to exactly mimic the teacher's demonstration in the second trial of a median task, shown by a drop in score. It does, however, outperform an unprompted baseline across all numbers of trials, indicating that it is able to profitably incorporate information from the demonstration into its policy. This process is analogous to prompting of large language models, where the agent's memory is primed with an example of desired behaviour from which it continues. We note that AdA was never trained with such off-policy first-person demonstrations, yet its in-context learning algorithm is still able to generalise to these.

In Figure F.4 we provide prompting results for all single-agent hand-authored tasks and discuss the circumstances under which prompting is effective. In Appendix F.4 we also provide early results investigating prompting with human demonstrations on a subset of tasks. These reveal remarkable success in some cases, but also confirm that human demonstrations cannot overcome inherent limitations of AdA's task distribution. Two videos compare the behaviour when prompted and when not prompted on the task Object permanence: yellow cube.

#### 4. Related Work

In this work, we leverage advances in attention-based models for meta-learning in an open-ended task space. Our agent learns a form of in-context RL algorithm, while also automatically curating the training task distribution; thus we combine two pillars of an AI generating algorithm (AI-GA, Clune

\(2019\)). The most similar work to ours is OEL Team et al. \(2021\), which also considers training in a vast multi-agent task space with auto-curricula and generational learning. A key difference in our work is that we focus on *adaptation* (vs. zero-shot performance), and make use of large Transformer models. Akkaya et al. \(2019\) also demonstrated the effectiveness of adaptive curricula while meta-learning a policy to control a robot hand, however they focused on a specific sim-to-real setting rather than a more generally capable agent. We now summarise literature related to each component of our work in turn.

**Procedural environment generation.** We make use of procedural content generation (PCG) to generate a vast, diverse task distribution. PCG has been studied for many years in the games community \(Risi and Togelius, 2020; Togelius and Schmidhuber, 2008\) and more recently has been used to create testbeds for RL agents \(Cobbe et al., 2018; Justesen et al., 2018; Raileanu and Rocktäschel, 2020\). Indeed, in the past few years a series of challenging PCG environments have been proposed \(Chevalier-Boisvert et al., 2018; Cobbe et al., 2020; Deitke et al., 2022; Hafner, 2022; Juliani et al., 2019; Küttler et al., 2020; Samvelyan et al., 2021\), mostly focusing on testing and improving generalisation in RL \(Bhatt et al., 2022; Fontaine et al., 2021; Kirk et al., 2021\). More recently there has been increased emphasis on open-ended worlds: Albrecht et al. \(2022\) proposed Avalon, a 3D world supporting complex tasks, while Minecraft \(Johnson et al., 2016\) has been proposed as a challenge for Open-Endedness and RL \(Fan et al., 2022; Grbic et al., 2021; Kanervisto et al., 2022\), but unlike XLand it does not admit control of the full simulation stack, thereby limiting the smoothness of the task space.

**Open-ended learning.** A series of recent works have demonstrated the effectiveness of agentenvironment co-adaptation with a distribution of tasks \(Parker-Holder et al., 2022; Wang et al., 2019, 2020a\). Our approach bears resemblance to the unsupervised environment design (UED, Dennis et al. \(2020\)) paradigm, since we seek to train a generalist agent without knowledge of the test tasks. One of the pioneering methods in this space was PAIRED \(Dennis et al., 2020\), which seeks to generate tasks with an RL-trained adversary. We build on Prioritised Level Replay \(Jiang et al., 2021a, b\), a method which instead curates randomly sampled environments which have high regret. Our work also relates to curriculum learning \(Campero et al., 2021; Fang et al., 2021; Matiisen et al., 2020; Mu et al., 2022; OpenAI et al., 2021; Portelas et al., 2019; Sukhbaatar et al., 2018\), with the key difference that these methods typically have a specific downstream goal or task in mind. There have also been works training agents with auto-curricula over co-players, although these typically focus on singleton environments \(Berner et al., 2019; Vinyals et al., 2019\) or uniformly sampled tasks \(Baker et al., 2020; Cultural General Intelligence Team et al., 2022; Jaderberg et al., 2019; Liu et al., 2019\). Similar to XLand 2.0's production rule system, Zhong et al. \(2020\) train agents to generalise to unobserved environment dynamics. However, they investigate zero-shot generalisation where the agent has to infer underlying environment dynamics from language descriptions, whereas AdA agents discovery these rules at test time via on-the-fly hypothesis-driven exploration over multiple trials.

**Adaptation.** This work focuses on few-shot adaptation in control problems, commonly framed as *meta-RL*. We focus on *memory-based* meta-RL, and build upon the work of Duan et al. \(2017\) and Wang et al. \(2016\), who showed that if an agent observes rewards and terminations, and the memory does not reset, a memory-based policy can implement a learning algorithm. This has proven to be an effective approach that can learn Bayes-optimal strategies \(Mikulik et al., 2020; Ortega et al., 2019\) and may have neurological analogues \(Wang et al., 2018\). Indeed, our agents learn conceptual exploration strategies, something that would require the outer learner of a meta-gradient approach to estimate the return of the inner learner \(Stadie et al., 2018\). Solutions in this space either rely on high-variance Monte Carlo returns \(Garcia and Thomas, 2019; Stadie et al., 2018; Vuorio et al., 2021\) or history-dependent estimators \(Zheng et al., 2020\). Our work is also inspired by Alchemy \(Wang et al., 2021\), a meta-RL benchmark domain whose mechanics have inspired the production rules in our work. The authors use memory-based meta-RL with a small Transformer, but find that the agent's performance is only marginally better than that of a random heuristic. Transformers have also been shown to be effective for meta-RL on simple domains \(Melo, 2022\) and for learning RL algorithms \(Laskin et al., 2022\) from offline data. Other approaches for meta-RL include meta-gradients \(Andrychowicz et al., 2016; Finn et al., 2017; Flennerhag et al., 2022; Xu et al., 2018\), which can be efficient but often suffer from instability and myopia \(Flennerhag et al., 2022; Metz et al., 2021; Vuorio et al., 2021\), and latent-variable based approaches \(Finn et al., 2018; Humplik et al., 2019; Rakelly et al., 2019; Zintgraf et al., 2019\). Adaptation also plays a critical role in robotics, with agents trained to adapt to varying terrain \(Clavera et al., 2019; Kumar et al., 2021\) or damaged joints \(Cully et al., 2015\).

**Transformers in RL and beyond.** Transformer architectures have recently shown to be highly effective for *offline* RL \(Chen et al., 2021; Janner et al., 2021; Reed et al., 2022\), yet successes in the *online* setting remain limited. One of the few works to successfully train Transformer-based policies was Parisotto et al. \(2020\), who introduced several heuristics to stabilise training in a simpler, smaller-scale setting. Indeed, while we make use of a similar Transformer-XL architecture \(Dai et al., 2019; Vaswani et al., 2017\), we demonstrate scaling laws for online meta-RL that resemble those seen in other communities, such as language \(Brown et al., 2020; Devlin et al., 2019; Kaplan et al., 2020; Rae et al., 2021\). Similarly, Melo \(2022\) use Transformers for fast adaptation in a smaller-scale meta-RL setting, interpreting the self-attention mechanism as a means of building an episodic memory from timestep embeddings, through the recursive application of Transformer layers. Transformer architectures have also been used in meta-learning outside of RL, for example learning general-purpose algorithms \(Kirsch et al., 2022\) or hyperparameter optimisers \(Chen et al., 2022\). Transformers are also ubiquitous in modern large language models, which have been shown to be few-shot learners \(Brown et al., 2020\).

## **5. Conclusion**

Adaptation to new information across a range of timescales is a crucial ability for generally intelligent agents. Foundation models in particular have demonstrated an ability to acquire a large knowledgebase of information, and apply this rapidly to new scenarios. Thus far, they have relied mainly on supervised and self-supervised learning. As such, they require access to large datasets. An alternative to collecting datasets is to have an agent learn from its own experience via reinforcement learning, provided that sufficiently rich physical worlds or open-ended simulations are available. This raises the question: can large-scale, generally adaptive models be trained with RL?

In this paper, we demonstrate, for the first time to our knowledge, an agent trained with RL that is capable of rapid in-context adaptation across a vast, open-ended task space, at a timescale that is similar to that of human players. This *Adaptive Agent* (AdA) explores held-out tasks in a structured way, refining its policy towards optimal behaviour given only a few interactions with the task. Further, AdA is amenable to contextual first-person prompting, strengthening its few-shot performance, analogous to prompting in large language models. AdA shows scaleable performance as a function of number of parameters, context length and richness of the training task distribution.

Our training method is based on black-box meta-RL, previously thought of as hard to scale. We show that state-of-the-art automatic curriculum techniques can shape the data distribution to provide sufficient signal for learning to learn in an open-ended task space. Moreover, we demonstrate that attention-based architectures can take advantage of this signal much more effectively than purely recurrent networks, illustrating the importance of co-adapting data-distribution and agent architecture for facilitating rapid adaptation. Finally, distillation enables us to realise the potential of large-scale Transformer architectures.

The future of AI research will inevitably involve training increasingly large models with increasingly general and adaptive capabilities. In this direction, we have provided a recipe for training a 500M parameter model, which we hope can pave the way for further advances at the intersection of RL and foundation models. AdA shows rapid and scalable adaptation of myriad kinds, from tool use to experimentation, from division of labour to navigation. Given scaling law trends, such models may in future become the default foundations for few-shot adaptation and fine-tuning on useful control problems in the real world.

## **6. Authors and Contributions**

We list authors alphabetically by last name. Please direct all correspondence to Feryal Behbahani [\(feryal@deepmind.com\)](mailto:feryal@deepmind.com) and Edward Hughes [\(edwardhughes@deepmind.com\)](mailto:edwardhughes@deepmind.com).

#### **6.1. Core contributors**

- **Jakob Bauer**: technical leadership, curriculum research, infrastructure engineering, task authoring, paper writing
- **Kate Baumli**: agent research, scaling, agent analysis, task authoring, paper writing
- **Feryal Behbahani**: research vision, team leadership, agent research, paper writing
- **Avishkar Bhoopchand**: technical leadership, evaluation research, infrastructure engineering, task authoring, paper writing
- **Michael Chang**: visualisation, agent analysis, human experiments
- **Adrian Collister**: XLand development, human experiments
- **Edward Hughes**: research vision, team leadership, evaluation research, paper writing
- **Sheleem Kashem**: infrastructure engineering, curriculum research, human experiments
- **Jack Parker-Holder**: curriculum research, paper writing
- **Yannick Schroecker**: agent research, scaling, task authoring, agent analysis, paper writing
- **Jakub Sygnowski**: infrastructure engineering, curriculum research, agent analysis, paper writing
- **Alexander Zacherl**: design leadership, agent analysis, task authoring, visualisation, human experiments
- **Lei Zhang**: curriculum research, agent analysis, paper writing

## **6.2. Partial contributors**

- **Nathalie Bradley-Schmieg**: project management
- **Natalie Clay**: QA testing, human experiments
- **Vibhavari Dasagi**: evaluation research
- **Lucy Gonzalez**: project management
- **Karol Gregor**: agent research
- **Maria Loks-Thompson**: XLand development, human experiments
- **Hannah Openshaw**: project management
- **Shreya Pathak**: agent analysis

- **Nicolas Perez-Nieves**: agent analysis, task authoring
- **Nemanja Rakicevic**: curriculum research, agent analysis
- **Tim Rocktäschel**: strategic advice, paper writing
- **Sarah York**: QA testing, human experiments

## **6.3. Sponsors**

- **Satinder Baveja**: strategic advice • **Karl Tuyls**: strategic advice

# **7. Acknowledgements**

We thank Max Jaderberg for early guidance on the project vision. We are grateful to Wojciech Marian Czarnecki for an early version of the production rules formalism and Catarina Barros for a prototype implementation. We thank Dawid Górny for support on implementing visualisation tools. We are grateful to Alex Platonov for artistic rendering of the figures and accompanying videos. We thank Nathaniel Wong, Tom Hudson and the Worlds Team for their engineering support. Further, we thank Andrew Bolt, Max Cant, Valentin Dalibard, Richard Everett, Nik Hemmings, Shaobo Hou, Jony Hudson, Errol King, George-Cristian Muraru, Alexander Neitz, Valeria Oliveira, Doina Precup, Drew Purves, Daniel Tanis, Roma Patel, and Marcus Wainwright for useful discussions and support. We are grateful to Sebastian Flennerhag and Raia Hadsell for reviewing a draft of the paper.

## **References**

- R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. G. Bellemare. Deep reinforcement learning at the edge of the statistical precipice. *CoRR*, abs/2108.13264, 2021.
- I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. Mc Grew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al. Solving rubik's cube with a robot hand. *ar Xiv preprint ar Xiv:1910.07113*, 2019.
- J. Albrecht, A. J. Fetterman, B. Fogelman, E. Kitanidis, B. Wróblewski, N. Seo, M. Rosenthal, M. Knutins, Z. Polizzi, J. B. Simon, and K. Qiu. Avalon: A benchmark for RL generalization using procedurally generated worlds. In *Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track*, 2022.
- M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and N. De Freitas. Learning to learn by gradient descent by gradient descent. *Advances in neural information processing systems*, 29, 2016.
- I. Babuschkin, K. Baumli, A. Bell, S. Bhupatiraju, J. Bruce, P. Buchlovsky, D. Budden, T. Cai, A. Clark, I. Danihelka, C. Fantacci, J. Godwin, C. Jones, R. Hemsley, T. Hennigan, M. Hessel, S. Hou, S. Kapturowski, T. Keck, I. Kemaev, M. King, M. Kunesch, L. Martens, H. Merzic, V. Mikulik, T. Norman, J. Quan, G. Papamakarios, R. Ring, F. Ruiz, A. Sanchez, R. Schneider, E. Sezener, S. Spencer, S. Srinivasan, L. Wang, W. Stokowiec, and F. Viola. The Deep Mind JAX Ecosystem, 2020. URL <http://github.com/deepmind>.
- B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. Mc Grew, and I. Mordatch. Emergent tool use from multi-agent autocurricula. In *International Conference on Learning Representations*, 2020.

- D. Balduzzi, K. Tuyls, J. Perolat, and T. Graepel. Re-evaluating evaluation. *Advances in Neural Information Processing Systems*, 31, 2018.
- J. Barbosa, H. Stein, S. Zorowitz, Y. Niv, C. Summerfield, S. Soto-Faraco, and A. Hyafil. A practical guide for studying human behavior in the lab. *Behavior Research Methods*, pages 1–19, 2022.
- C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman, T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang. Dota 2 with large scale deep reinforcement learning. *CoRR*, abs/1912.06680, 2019.
- V. Bhatt, B. Tjanaka, M. C. Fontaine, and S. Nikolaidis. Deep surrogate assisted generation of environments. In *Advances in Neural Information Processing Systems*, 2022.
- R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. S. Chatterji, A. S. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. D. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. S. Krass, R. Krishna, R. Kuditipudi, and et al. On the opportunities and risks of foundation models. *CoRR*, abs/2108.07258, 2021.
- J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Vander Plas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+Num Py programs, 2018. URL <http://github.com/google/jax>.
- T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. Mc Candlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, *Advances in Neural Information Processing Systems*, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.
- A. Campero, R. Raileanu, H. Kuttler, J. B. Tenenbaum, T. Rocktäschel, and E. Grefenstette. Learning with AMIGo: Adversarially motivated intrinsic goals. In *International Conference on Learning Representations*, 2021.
- M. Carroll, R. Shah, M. K. Ho, T. L. Griffiths, S. A. Seshia, P. Abbeel, and A. Dragan. On the utility of learning about humans for human-ai coordination, 2019.
- E. Cetin, P. J. Ball, S. Roberts, and O. Celiktutan. Stabilizing off-policy deep reinforcement learning from pixels. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, *Proceedings of the 39th International Conference on Machine Learning*, volume 162 of *Proceedings of Machine Learning Research*, pages 2784–2810. PMLR, 17–23 Jul 2022.
- L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, *Advances in Neural Information Processing Systems*, volume 34, 2021.
- Y. Chen, X. Song, C. Lee, Z. Wang, Q. Zhang, D. Dohan, K. Kawakami, G. Kochanski, A. Doucet, M. Ranzato, S. Perel, and N. de Freitas. Towards learning universal hyperparameter optimizers with transformers. In *Neural Information Processing Systems (NeurIPS) 2022*, 2022.

- M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for OpenAI Gym. <https://github.com/maximecb/gym-minigrid>, 2018.
- K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder-decoder approaches. *ar Xiv preprint ar Xiv:1409.1259*, 2014.
- F. Christianos, G. Papoudakis, and S. V. Albrecht. Pareto actor-critic for equilibrium selection in multi-agent reinforcement learning. *ar Xiv*, 2022. doi: 10.48550/ARXIV.2209.14344.
- I. Clavera, A. Nagabandi, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. In *International Conference on Learning Representations*, 2019.
- J. Clune. AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence. *CoRR*, abs/1905.10985, 2019.
- K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in reinforcement learning. *CoRR*, abs/1812.02341, 2018.
- K. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging procedural generation to benchmark reinforcement learning. In *Proceedings of the 37th International Conference on Machine Learning*, pages 2048–2056, 2020.
- A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret. Robots that can adapt like animals. *Nature*, 521: 503–507, 2015.
- Cultural General Intelligence Team, A. Bhoopchand, B. Brownfield, A. Collister, A. D. Lago, A. Edwards, R. Everett, A. Frechette, Y. G. Oliveira, E. Hughes, K. W. Mathewson, P. Mendolicchio, J. Pawar, M. Pislar, A. Platonov, E. Senter, S. Singh, A. Zacherl, and L. M. Zhang. Learning robust real-time cultural transmission without human data, 2022.
- W. M. Czarnecki, R. Pascanu, S. Osindero, S. Jayakumar, G. Swirszcz, and M. Jaderberg. Distilling policy distillation. In *The 22nd International Conference on Artificial Intelligence and Statistics*, pages 1331–1340. PMLR, 2019.
- A. Dafoe, E. Hughes, Y. Bachrach, T. Collins, K. R. Mc Kee, J. Z. Leibo, K. Larson, and T. Graepel. Open problems in cooperative AI. *CoRR*, abs/2012.08630, 2020.
- Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 2978–2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285.
- M. Deitke, E. Vander Bilt, A. Herrasti, L. Weihs, J. Salvador, K. Ehsani, W. Han, E. Kolve, A. Farhadi, A. Kembhavi, and R. Mottaghi. ProcTHOR: Large-scale embodied AI using procedural generation. In *Advances in Neural Information Processing Systems*, 2022. doi: 10.48550/ARXIV.2206.06994.
- M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. In *Advances in Neural Information Processing Systems*, volume 33, 2020.
- J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)*. Association for Computational Linguistics, 2019.

- Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. RL<sup>2</sup> : Fast reinforcement learning via slow reinforcement learning, 2017.
- L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In *Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track*, 2022.
- K. Fang, Y. Zhu, S. Savarese, and F.-F. Li. Adaptive procedural task generation for hard-exploration problems. In *International Conference on Learning Representations*, 2021.
- G. Farquhar, K. Baumli, Z. Marinho, A. Filos, M. Hessel, H. P. van Hasselt, and D. Silver. Self-consistent models and values. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, *Advances in Neural Information Processing Systems*, volume 34, pages 1111–1125, 2021.
- A. Filos, E. Vértes, Z. Marinho, G. Farquhar, D. Borsa, A. L. Friesen, F. M. P. Behbahani, T. Schaul, A. Barreto, and S. Osindero. Model-value inconsistency as a signal for epistemic uncertainty. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and S. Sabato, editors, *International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA*, volume 162 of *Proceedings of Machine Learning Research*, pages 6474–6498. PMLR, 2022.
- C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In *Proceedings of the 34th International Conference on Machine Learning, ICML, Sydney, NSW, Australia, 6-11 August*, volume 70 of *Proceedings of Machine Learning Research*. PMLR, 2017.
- C. Finn, K. Xu, and S. Levine. Probabilistic model-agnostic meta-learning. *Advances in neural information processing systems*, 31, 2018.
- S. Flennerhag, Y. Schroecker, T. Zahavy, H. van Hasselt, D. Silver, and S. Singh. Bootstrapped meta-learning. In *International Conference on Learning Representations*, 2022.
- M. Fontaine, Y.-C. Hsu, Y. Zhang, B. Tjanaka, and S. Nikolaidis. On the importance of environments in human-robot coordination. 07 2021.
- F. Garcia and P. S. Thomas. A meta-mdp approach to exploration for lifelong reinforcement learning. *Advances in Neural Information Processing Systems*, 32, 2019.
- D. Grbic, R. Palm, E. Najarro, C. Glanois, and S. Risi. *Evo Craft: A New Challenge for Open-Endedness*, pages 325–340. 04 2021.
- S. Gronauer and K. Diepold. Multi-agent deep reinforcement learning: a survey. *Artificial Intelligence Review*, 55(2):895–943, 2022.
- D. Hafner. Benchmarking the spectrum of agent capabilities. In *International Conference on Learning Representations*, 2022.
- K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 770–778, 2016. doi: 10.1109/ CVPR.2016.90.
- J. Heinrich, M. Lanctot, and D. Silver. Fictitious self-play in extensive-form games. In *International conference on machine learning*, pages 805–813. PMLR, 2015.
- D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). *ar Xiv: Learning*, 2016.

- M. Hessel, I. Danihelka, F. Viola, A. Guez, S. Schmitt, L. Sifre, T. Weber, D. Silver, and H. Van Hasselt. Muesli: Combining improvements in policy optimization. In *International Conference on Machine Learning*. PMLR, 2021.
- J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. *ar Xiv preprint ar Xiv:2203.15556*, 2022.
- H. Hu, A. Peysakhovich, A. Lerer, and J. Foerster. "other-play"for zero-shot coordination. In *Proceedings of Machine Learning and Systems 2020*, pages 9396–9407, 2020.
- J. Humplik, A. Galashov, L. Hasenclever, P. A. Ortega, Y. W. Teh, and N. Heess. Meta reinforcement learning as task inference. *ar Xiv preprint ar Xiv:1905.06424*, 2019.
- M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, et al. Population based training of neural networks. *ar Xiv preprint ar Xiv:1711.09846*, 2017.
- M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G. Castañeda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, N. Sonnerat, T. Green, L. Deason, J. Z. Leibo, D. Silver, D. Hassabis, K. Kavukcuoglu, and T. Graepel. Human-level performance in 3d multiplayer games with population-based reinforcement learning. *Science*, 364(6443):859–865, 2019.
- M. Janner, Q. Li, and S. Levine. Offline reinforcement learning as one big sequence modeling problem. In *Advances in Neural Information Processing Systems*, 2021.
- M. Jiang, M. Dennis, J. Parker-Holder, J. Foerster, E. Grefenstette, and T. Rocktäschel. Replay-guided adversarial environment design. In *Advances in Neural Information Processing Systems*, 2021a.
- M. Jiang, E. Grefenstette, and T. Rocktäschel. Prioritized level replay. In *The International Conference on Machine Learning*, 2021b.
- M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The Malmo platform for artificial intelligence experimentation. In *Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence*. AAAI Press, 2016.
- A. Juliani, A. Khalifa, V. Berges, J. Harper, E. Teng, H. Henry, A. Crespi, J. Togelius, and D. Lange. Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning. In *IJCAI*, 2019.
- N. Justesen, R. R. Torrado, P. Bontrager, A. Khalifa, J. Togelius, and S. Risi. Procedural level generation improves generality of deep reinforcement learning. *CoRR*, abs/1806.10729, 2018.
- A. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang, W. Hong, Z. Huang, H. Chen, G. Zeng, Y. Lin, V. Micheli, E. Alonso, F. Fleuret, A. Nikulin, Y. Belousov, O. Svidchenko, and A. Shpilman. Minerl diamond 2021 competition: Overview, results, and lessons learned, 2022.
- J. Kaplan, S. Mc Candlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. *CoRR*, abs/2001.08361, 2020.
- R. Kirk, A. Zhang, E. Grefenstette, and T. Rocktäschel. A survey of generalisation in deep reinforcement learning. *CoRR*, abs/2111.09794, 2021.
- L. Kirsch, J. Harrison, J. Sohl-Dickstein, and L. Metz. General-purpose in-context learning by metalearning transformers. *ar Xiv*, 2022.

- A. Kumar, Z. Fu, D. Pathak, and J. Malik. RMA: Rapid motor adaptation for legged robots. In *Robotics: Science and Systems*, 2021.
- H. Küttler, N. Nardelli, A. H. Miller, R. Raileanu, M. Selvatici, E. Grefenstette, and T. Rocktäschel. The Net Hack Learning Environment. In *Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)*, 2020.
- M. Laskin, L. Wang, J. Oh, E. Parisotto, S. Spencer, R. Steigerwald, D. Strouse, S. Hansen, A. Filos, E. Brooks, M. Gazeau, H. Sahni, S. Singh, and V. Mnih. In-context reinforcement learning with algorithm distillation, 2022.
- K.-H. Lee, O. Nachum, S. Yang, L. Lee, C. D. Freeman, S. Guadarrama, I. Fischer, W. Xu, E. Jang, H. Michalewski, and I. Mordatch. Multi-game decision transformers. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, *Advances in Neural Information Processing Systems*, 2022.
- S. Liu, G. Lever, N. Heess, J. Merel, S. Tunyasuvunakool, and T. Graepel. Emergent coordination through competition. In *International Conference on Learning Representations*, 2019.
- D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li, A. Bharambe, and L. van der Maaten. Exploring the limits of weakly supervised pretraining. In *Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II*, page 185–201, Berlin, Heidelberg, 2018. Springer-Verlag. ISBN 978-3-030-01215-1.
- T. Matiisen, A. Oliver, T. Cohen, and J. Schulman. Teacher-student curriculum learning. *IEEE Trans. Neural Networks Learn. Syst.*, 31(9):3732–3740, 2020.
- L. C. Melo. Transformers are meta-reinforcement learners. In *International Conference on Machine Learning*, pages 15340–15359. PMLR, 2022.
- L. Metz, C. D. Freeman, S. S. Schoenholz, and T. Kachman. Gradients are not all you need. *ar Xiv preprint ar Xiv:2111.05803*, 2021.
- V. Mikulik, G. Delétang, T. Mc Grath, T. Genewein, M. Martic, S. Legg, and P. Ortega. Meta-trained agents implement bayes-optimal agents. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, *Advances in Neural Information Processing Systems*, volume 33, pages 18691–18703. Curran Associates, Inc., 2020.
- J. Mu, V. Zhong, R. Raileanu, M. Jiang, N. Goodman, T. Rocktäschel, and E. Grefenstette. Improving intrinsic exploration with language abstractions. In *Advances in Neural Information Processing Systems*, 2022.
- R. Munos, T. Stepleton, A. Harutyunyan, and M. Bellemare. Safe and efficient off-policy reinforcement learning. *Advances in neural information processing systems*, 29, 2016.
- E. Nikishin, M. Schwarzer, P. D'Oro, P.-L. Bacon, and A. Courville. The primacy bias in deep reinforcement learning. In *International Conference on Machine Learning*, pages 16828–16847. PMLR, 2022.
- OEL Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. Trebacz, M. Jaderberg, M. Mathieu, N. Mc Aleese, N. Bradley-Schmieg, N. Wong, N. Porcel, R. Raileanu, S. Hughes-Fitt, V. Dalibard, and W. M. Czarnecki. Open-ended learning leads to generally capable agents. *CoRR*, abs/2107.12808, 2021.

- OpenAI, M. Plappert, R. Sampedro, T. Xu, I. Akkaya, V. Kosaraju, P. Welinder, R. D'Sa, A. Petron, H. P. de Oliveira Pinto, A. Paino, H. Noh, L. Weng, Q. Yuan, C. Chu, and W. Zaremba. Asymmetric self-play for automatic goal discovery in robotic manipulation, 2021.
- P. A. Ortega, J. X. Wang, M. Rowland, T. Genewein, Z. Kurth-Nelson, R. Pascanu, N. Heess, J. Veness, A. Pritzel, P. Sprechmann, et al. Meta-learning of sequential strategies. *ar Xiv preprint ar Xiv:1905.03030*, 2019.
- K. Ota, D. K. Jha, and A. Kanezaki. Training larger networks for deep reinforcement learning, 2021.
- E. Parisotto. *Meta Reinforcement Learning through Memory*. PhD thesis, Carnegie Mellon University Pittsburgh, PA, 2021.
- E. Parisotto, F. Song, J. Rae, R. Pascanu, C. Gulcehre, S. Jayakumar, M. Jaderberg, R. L. Kaufman, A. Clark, S. Noury, et al. Stabilizing transformers for reinforcement learning. In *International conference on machine learning*, pages 7487–7498. PMLR, 2020.
- J. Parker-Holder, M. Jiang, M. Dennis, M. Samvelyan, J. Foerster, E. Grefenstette, and T. Rocktäschel. Evolving curricula with regret-based environment design. In *The International Conference on Machine Learning*, 2022.
- M. Pislar, D. Szepesvari, G. Ostrovski, D. L. Borsa, and T. Schaul. When should agents explore? In *International Conference on Learning Representations*, 2022.
- R. Portelas, C. Colas, K. Hofmann, and P. Oudeyer. Teacher algorithms for curriculum learning of deep RL in continuously parameterized environments. In L. P. Kaelbling, D. Kragic, and K. Sugiura, editors, *3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings*, volume 100 of *Proceedings of Machine Learning Research*, pages 835–853. PMLR, 2019.
- J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. *ar Xiv preprint ar Xiv:2112.11446*, 2021.
- R. Raileanu and T. Rocktäschel. Ride: Rewarding impact-driven exploration for procedurally-generated environments. In *International Conference on Learning Representations*, 2020.
- K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In *International conference on machine learning*, pages 5331–5340. PMLR, 2019.
- S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-maron, M. Giménez, Y. Sulsky, J. Kay, J. T. Springenberg, T. Eccles, J. Bruce, A. Razavi, A. Edwards, N. Heess, Y. Chen, R. Hadsell, O. Vinyals, M. Bordbar, and N. de Freitas. A generalist agent. *Transactions on Machine Learning Research*, 2022.
- M. Reid, Y. Yamada, and S. S. Gu. Can wikipedia help offline reinforcement learning? *CoRR*, 2022.
- S. Risi and J. Togelius. Increasing generality in machine learning through procedural content generation. *Nature Machine Intelligence*, 2, 08 2020. doi: 10.1038/s42256-020-0208-z.
- M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro, F. Petroni, H. Kuttler, E. Grefenstette, and T. Rocktäschel. Minihack the planet: A sandbox for open-ended reinforcement learning research. In *Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track*, 2021.

- M. Samvelyan, A. Khan, M. D. Dennis, M. Jiang, J. Parker-Holder, J. N. Foerster, R. Raileanu, and T. Rocktäschel. MAESTRO: Open-ended environment design for multi-agent reinforcement learning. In *Deep Reinforcement Learning Workshop NeurIPS 2022*, 2022.
- T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. In *The International Conference on Learning Representations*, 2015.
- J. Schmidhuber. Curious model-building control systems. In *[Proceedings] 1991 IEEE International Joint Conference on Neural Networks*, pages 1458–1463 vol.2, 1991. doi: 10.1109/IJCNN.1991.170605.
- J. Schmidhuber. Learning complex, extended sequences using the principle of history compression. *Neural Computation*, 4(2):234–242, 1992. doi: 10.1162/neco.1992.4.2.234.
- S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Doersch, W. M. Czarnecki, J. Z. Leibo, H. Kuttler, A. Zisserman, K. Simonyan, et al. Kickstarting deep reinforcement learning. *ar Xiv preprint ar Xiv:1803.03835*, 2018.
- C. Schuhmann, R. Beaumont, R. Vencu, C. W. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. R. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In *Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track*, 2022.
- N. Shazeer. Glu variants improve transformer. *ar Xiv preprint ar Xiv:2002.05202*, 2020.
- B. C. Stadie, G. Yang, R. Houthooft, X. Chen, Y. Duan, Y. Wu, P. Abbeel, and I. Sutskever. Some considerations on learning to explore via meta-reinforcement learning. *ar Xiv preprint ar Xiv:1803.01118*, 2018.
- P. Stone, G. A. Kaminka, S. Kraus, and J. S. Rosenschein. Ad hoc autonomous agent teams: Collaboration without pre-coordination. In M. Fox and D. Poole, editors, *Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010*. AAAI Press, 2010.
- D. Strouse, K. Mc Kee, M. Botvinick, E. Hughes, and R. Everett. Collaborating with humans without human data. *Advances in Neural Information Processing Systems*, 34:14502–14515, 2021.
- S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. In *International Conference on Learning Representations*, 2018.
- C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In *2017 IEEE International Conference on Computer Vision (ICCV)*, pages 843–852, 2017.
- Y. Tay, M. Dehghani, S. Abnar, H. W. Chung, W. Fedus, J. Rao, S. Narang, V. Q. Tran, D. Yogatama, and D. Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling?, 2022.
- J. Togelius and J. Schmidhuber. An experiment in automatic game design. In *2008 IEEE Symposium On Computational Intelligence and Games*, pages 111–118, 2008. doi: 10.1109/CIG.2008.5035629.
- A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. *Advances in neural information processing systems*, 30, 2017.

- O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, . D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, Ç. Gülçehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. Wünsch, K. Mc Kinney, O. Smith, T. Schaul, T. P. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. *Nat.*, 575(7782): 350–354, 2019. doi: 10.1038/s41586-019-1724-z.
- R. Vuorio, J. A. Beck, G. Farquhar, J. N. Foerster, and S. Whiteson. No dice: An investigation of the bias-variance tradeoff in meta-gradients. In *Deep RL Workshop NeurIPS 2021*, 2021.
- L. Vygotsky. Interaction between learning and development. *Readings on the Development of Children*, pages 34–40, 1978.
- J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to reinforcement learn. *ar Xiv preprint ar Xiv:1611.05763*, 2016.
- J. X. Wang, Z. Kurth-Nelson, D. Kumaran, D. Tirumala, H. Soyer, J. Z. Leibo, D. Hassabis, and M. Botvinick. Prefrontal cortex as a meta-reinforcement learning system. *Nature neuroscience*, 21 (6):860–868, 2018.
- J. X. Wang, M. King, N. Porcel, Z. Kurth-Nelson, T. Zhu, C. Deck, P. Choy, M. Cassin, M. Reynolds, F. Song, et al. Alchemy: A structured task distribution for meta-reinforcement learning. *arxiv*, 2021.
- R. Wang, J. Lehman, J. Clune, and K. O. Stanley. Paired open-ended trailblazer (POET): endlessly generating increasingly complex and diverse learning environments and their solutions. *CoRR*, abs/1901.01753, 2019.
- R. Wang, J. Lehman, A. Rawal, J. Zhi, Y. Li, J. Clune, and K. Stanley. Enhanced POET: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. In H. D. III and A. Singh, editors, *Proceedings of the 37th International Conference on Machine Learning*, volume 119 of *Proceedings of Machine Learning Research*, pages 9940–9951. PMLR, 13–18 Jul 2020a.
- T. Wang, H. Dong, V. Lesser, and C. Zhang. Roma: Multi-agent reinforcement learning with emergent roles. *ar Xiv preprint ar Xiv:2003.08039*, 2020b.
- Z. Xu, J. Modayil, H. P. van Hasselt, A. Barreto, D. Silver, and T. Schaul. Natural value approximators: Learning when to trust past estimates. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc., 2017.
- Z. Xu, H. P. van Hasselt, and D. Silver. Meta-gradient reinforcement learning. *Advances in neural information processing systems*, 31, 2018.
- J. Yang, B. Petersen, H. Zha, and D. Faissol. Single episode policy transfer in reinforcement learning. In *International Conference on Learning Representations*, 2019.
- J. Yang, A. Li, M. Farajtabar, P. Sunehag, E. Hughes, and H. Zha. Learning to incentivize other learning agents. In *Advances in Neural Information Processing Systems*. ar Xiv, 2020. doi: 10.48550/ARXIV. 2006.06051.
- W. Yu, J. Tan, Y. Bai, E. Coumans, and S. Ha. Learning fast adaptation with meta strategy optimization. *IEEE Robotics and Automation Letters*, 5(2):2950–2957, 2020.

- X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In *2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, Los Alamitos, CA, USA, jun 2022. IEEE Computer Society.
- Z. Zheng, J. Oh, M. Hessel, Z. Xu, M. Kroiss, H. Van Hasselt, D. Silver, and S. Singh. What can learned intrinsic rewards capture? In *International Conference on Machine Learning*, pages 11436–11446. PMLR, 2020.
- V. Zhong, T. Rocktäschel, and E. Grefenstette. RTFM: generalising to new environment dynamics via reading. In *8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*, 2020.
- L. Zintgraf. *Fast adaptation via meta reinforcement learning*. PhD thesis, University of Oxford, 2022.
- L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson. Varibad: A very good method for bayes-adaptive deep rl via meta-learning. *ar Xiv preprint ar Xiv:1910.08348*, 2019.

# **Appendix**

## **A. Environment Details**

#### **A.1. XLand 2.0**

In this section we describe the differences between XLand 2.0 and the original XLand environment of OEL Team et al. \(2021\). We modify the configuration space as follows:

- We introduce a new relation touching(a, b). It is satisfied if objects a and b are in contact, as determined by Unity's collision detection with a distance threshold of 1 millimeter.
- We exclude all relations that refer to floors. As a more flexible alternative we introduce the option of spawning objects in a permanently frozen state, rendering them immobile. Frozen objects can be used as anchor points in the environment which require players to navigate to them by including them in goals or production rules.
- We only use predicates consisting of a single relation or its negation, excluding conjunctions or disjunctions of multiple relations. Note that production rules (Section 2.1\) allow us to specify tasks which require the player to sequentially satisfy predicates, or to give them multiple ways to reach a desired state.

For the reader's convenience, Tables A.1 and A.2 respectively list all shapes and colours we used for objects in XLand. Table A.3 lists all predicates we used for goals and production rules.

Table A.1 | Shapes used for objects.

| Shape name |
|------------|
| Wall |
| Cube |
| Sphere |
## | Pyramid |

Table A.2 | Colours used for objects.

| Colour name |
|-------------|
| Black |
| Purple |
## | Yellow |

Table A.3 | Predicates used for goals and production rules.

| Predicate name | Meaning |
|----------------|---------------------------------------------------------------------------|
| touching(a, b) | Whether a<br>and b<br>are in contact. |
| near(a, b) | Whether a<br>and b<br>are at most 1m apart. |
| hold(a, b) | Whether player a<br>holds b. |
| see(a, b) | If<br>a<br>is a player, whether it can see<br>b. If not, whether the line |
| | connecting the centres of mass of a<br>and b<br>is not obstructed by |
| | another object. |
| not(p) | Whether predicate p<br>is not satisfied. |

#### A.2. Pre-sampling tasks for training

The space of goals and production rules can generate at least $10^{40}$ distinct tasks, even given quite restrictive bounds on the number of objects and rules.<sup>6</sup> For convenience, we pre-sample a subset of this space using the method described below. In Section 3.5 we evaluate the effect of the size of the sampled set. For each task we sample a world using the procedure outlined in OEL Team et al. (2021) and combine it with a game and production rules as follows.

**Single-player tasks.** We start by uniformly sampling a player's goal, consisting of a predicate with optional negation and two objects. Then, for a fixed number of steps (which we sample uniformly between 1 and 4), we add new production rules, such that they need to be triggered in sequence to get the objects present in the goal. We initialise the world to contain the objects present in the condition of the first production rule, together with up to 10 *distractor* objects, not present in any production rule from this chain (nor in the goal).

Next, we introduce *dead-end* production rules. We sample them such that their condition contains either distractor objects or ones that are ultimately necessary for satisfaction of the goal, yet the spawns are always distractors. As such, triggering a dead end may put the game in an unsolvable state. Including them in the training games creates pressure on the agent to avoid indiscriminately triggering all production rules. Finally, we sample a hiding mask, specifying which part of the production rules will be visible or hidden from the player. The sampling of both the mechanism for hiding (described in Section 2.1) and the actual parts to hide is uniform random.

**Multi-player tasks.** For this work we restrict our multi-player games to fully cooperative two-player games. Such games are known to be particularly challenging, as they have multiple equilibria, and thus feature an equilibrium selection problem (Dafoe et al., 2020). To sample such a game, we start by sampling a single-player game as outlined above and randomly replace references to the first player in the goal or production rules with references to player two. We copy the goal and sample a new production rule hiding mask for player two, resulting in a fully cooperative two-player game with potentially asymmetric information provided about the task's production rules.

#### **B.** Evaluation

#### **B.1.** Test scores

We evaluate our agents on a suite of 1000 held out test tasks sampled from the same distribution as the training games, using held-out world topologies. The procedure for pre-generating the XL and task pool is detailed in Appendix A.2. Rejection sampling ensures that no game (goal and production rules) in the test set is contained in the training set.

In XLand, rewards are obtained on every frame in which a goal is satisfied, making total rewards incomparable between tasks. To account for this, we fine-tune AdA on the test-task set. We compute the fine-tuned agent's maximum total last-trial reward (over any number of trials up to 13) and use this as an estimate of the maximum possible reward obtainable in a single trial of each test task. We call this quantity the normaliser. We define the *test score* $S_m^i$ of an agent i on task m with k trials to be the total reward obtained in trial k divided by the normaliser. This normalises rewards roughly to the

<sup>&</sup>lt;sup>6</sup>This is an order of magnitude lower-bound estimate, assuming 4 shapes, 3, colours, 7 predicates, a maximum of 5 production rules with a maximum of 3 objects on the right-hand side, 7 blanking options, and a maximum of 20 objects in the scene.

interval [0, 1]. Note that it is possible for an agent under evaluation to obtain a score greater than 1, both due to noise in the evaluation process and the fact that the agent under evaluation may be better than the one used for creating the normaliser.

When reporting the scores of our agents on a game with trials, we always use the total reward of the last trial. This is a good measure of whether the agent has successfully navigated the explorationexploitation tradeoff in a novel MDP, given knowledge of the number of trials . If an agent is capable of adaptation, we expect to see the performance in the last ( th) trial increase as a function of : that is to say, the agent is able to make use of additional experience on-the-fly to perform better. We evaluate on ∈ {1, 2, 3, 5, 8, 13}, where 8 and 13 are held out values of that were not seen during training.

To aggregate the scores of an agent across games, we use a fixed (usually 20th) percentile score \(Agarwal et al., 2021\). This gives us a lower bound guarantee on the performance of the agent across most of the tasks: for example, if the 20th percentile score of an agent is 0.5, then the agent gets the score of at least 0.5 on 80% of the games. Using an aggregation method like this (as opposed to an average) allows us to concentrate on the coverage of many tasks, as opposed to focusing the effort on improving the performance on outlier tasks. Empirically, we find that our results are robust across a range of different percentiles.

#### **B.2. Hand-authored probe tasks**

Evaluation using test tasks can only give us information with respect to the pre-sampled distribution in Appendix A.2. While this is certainly vast and diverse, an arbitrary task sampled from the test set is not necessarily easily understandable for humans. So in addition to quantitative evaluation on 1000 test tasks we also investigate specific human-level capabilities of our agent on two sets of 30 single-agent and 28 multi-agent probe tasks. These are based on situations that are intuitive to humans and which require qualitatively different behaviours, which we can inspect in more detail "with the naked eye". A full description of all 58 probe tasks can be found in Appendix F. Representative single-agent and multi-agent probe tasks are described in detail in Figures B.1 to B.3.

#### **B.3. Adaptation metric**

We introduce an *adaptation metric* to rank our agents based on their few-shot adaptation capability across the hand-authored probe task set. We collect last-trial total reward for all (, ) pairs where is a probe task and ∈ {1, 2, 3, 5, 8, 13}. We normalise the per-task scores just as for the test set above. We then aggregate over tasks using the 50th percentile (median). Finally, we aggregate over by saying that agent ranks higher than agent if and only if 's task-aggregated scores are a Pareto improvement over 's. That is to say, we would like agents that are both capable of high-quality zero-shot generalisation where possible ( = 1), and also that can use additional trial information to efficiently improve their policy in few-shots > 1; we don't "trade-off " between these.

A convenient way of using the Pareto improvement criterion to compute a scalar metric is the Nash average method of Balduzzi et al. \(2018\). We construct a competitive meta-game of "agents vs. ", and compute the maximum entropy Nash equilibrium for the game. The Nash payoff is then used as the adaptation metric, and agents are ranked by this metric. As desired, this metric has the property that if neither agent nor agent Pareto-dominate the other, the and receive the same Nash payoff and are therefore ranked equally. This adaptation metric was used as the means of selecting hyperparameters for training our best performing agent in Section 3.1.

![](_page_35_Figure_1.jpeg)

## ![](_page_35_Figure_2.jpeg)

Figure B.1 | **Wrong Pair Disappears**: The player's goal is to hold a black cube, which does not exist among the initial objects. But there are two (hidden) production rules. The player needs to identify the correct world state which triggers the rule that creates the cube and not the one which destroys the necessary inputs. All this is embedded in a challenging world layout with one-way drops and limited visibility.

## ![](_page_35_Figure_4.jpeg)

## ![](_page_35_Picture_5.jpeg)

Figure B.2 | **Pyramid in a Haystack**: To create the necessary yellow pyramid, the player needs to find and hold the purple cube. There are several distractor objects and distractor rules in this world, requiring the player to deal not just with a hard exploration challenge but also a very noisy environment.

![](_page_35_Figure_7.jpeg)

## ![](_page_35_Picture_8.jpeg)

Figure B.3 | **Push, don't lift**: The vast majority of training and evaluation tasks require lifting objects. Here two hidden rules destroy any object when lifted. In order to create the goal state, some "lateral thinking" is necessary: the player needs to identify that pushing the cubes with their body is possible.

## ![](_page_36_Figure_1.jpeg)

Figure B.4 | **Irreversible production for two**: Both players score when the first player holds a yellow sphere. There is no yellow sphere initially, but it can be produced from executing the first, second and fourth production rules in order. The other two rules are dead ends, destroying key input objects. Note that some input objects exist multiple times in the initial state, so there are multiple solution paths.

![](_page_36_Figure_3.jpeg)

Figure B.5 | The human player interface. **(a)** Prior to each trial players are told how many trials they have remaining on the current task, and are given unlimited time to read the goal and production rules (subject to any hiding). **(b)** During the trial players observe the same first-person camera view as agents, but at a higher 800 × 600 pixel resolution. The goal, production rules, current score, and time remaining in the trial are displayed via UI elements.

#### **B.4. Human data collection**

To provide a benchmark for human-timescale adaptation, we collected score data from a pool of 100 human players on the 30 single-agent probe tasks. Before attempting the probe tasks, each player completed a graded training curriculum of 23 tasks to acquire familiarity with the mouseand-keyboard control scheme and user interface (Figure B.5\), and the particular game mechanics of XLand. Since humans cannot undergo a short-term memory "reset" on episode boundaries, individual players attempted each of the probe tasks for a single only, with ∈ {1, 2, 3, 5, 8}. All players experienced a variety of across the task set. We assigned each player a unique ordering of the 30 probe tasks to average out any knowledge transfer from earlier tasks to later, and, within those orderings, we imposed separation between tasks with known similarity. Technical problems (e.g. internet dropout) prevented completion of 3.4% of the 3000 episodes, leaving an average of 19.3 samples per (task, ) pair, and a minimum of 17 samples for any individual (task, ).

# **C. Agent Details**

Here we provide technical details of our agent architecture, observation and action specifications.

## **C.1. Agent Architecture**

Here we provide an overview of the agent architecture, and provide important hyperparameters for the agent, noting that we performed minimal hyperparameter tuning in this work.

**Observation encoder.** The first-person view RGB observation (Table C.1\) is passed through a Res Net \(He et al., 2016\) with [16, 32, 32] convolutional channels, each consisting of 2 × 2 blocks and a final output size of 256. Max-pooling is used, in addition to scalar residual multipliers. relu activations are used throughout.

The goal observation is passed through a goal embedder, which is the same in OEL Team et al. \(2021\). This maps each of the 6 goal elements (negation, predicate, shape of object 1, colour of object 1, shape of object 2, colour of object 2) in the goal representation to a dense embedding vector of size 8. These are concatenated together and passed through a 3-layer MLP of size [8, 8, 8], resulting in a final goal embedding of size 8.

Production rules are encoded in the same manner as the goal, but mapped through a larger final MLP of shape [512, 256] resulting in a final production rule embedding vector of size 256.

The encoded RGB, goal, and production rules observations are concatenated together with all remaining scalar and vector observations (including previous reward, previous action, proprioception observations, and trial timing information) and passed though a final MLP. This results in an encoded observation vector of a size matching the hidden dimension of the Transformer memory.

**Transformer memory.** We use a Transformer-XL with causal masking \(Dai et al., 2019\). For the actor step we use a context window of 1, and in the learner step we use a rollout context window of 80. The Transformer-XL memory uses 300 previous cached activations (1800 effective timesteps) of memory in all experiments unless otherwise stated. We apply layer normalisation before attention operations as in Parisotto et al. \(2020\), use gating in the feedforward component as in Shazeer \(2020\), and apply relative positional embeddings as in Dai et al. \(2019\). As is common with Transformers, we use the gelu activation function throughout \(Hendrycks and Gimpel, 2016\).

**Muesli sequence model and prediction heads.** Next, we take the Transformer-XL output embedding, and use two MLPs of width 1000 to produce the hidden and cell values for the initial state of the Muesli model LSTM. On top of the hidden value, we apply MLP heads of width 1000 for the policy and value respectively. The policy MLP is followed by 6 linear softmaxed outputs corresponding to 6 action groups in a decomposed action space for the policy (as in OEL Team et al. \(2021\)). The value MLP is followed by a 601-unit binned logit prediction as in Hessel et al. \(2021\). Finally, the Muesli sequence model is unrolled for a further 4 steps, starting from the LSTM state embedding and producing a 1000-dimensional output vector on each LSTM step. This feeds into a further 1000-dimensional MLP followed by a 601-unit binned reward prediction.

#### **C.2. Observations**

We summarise all the observations received by the agent when running inference in Table C.1. In the descriptions, "legacy reasons" refers to an observation format that was inherited from OEL Team et al. \(2021\).

Table C.1 | Agent observations.

| Observation name | Shape | Meaning |
|----------------------|------------|-------------------------------------------------------------------------------------------------|
| RGB | 72×96×3 | RGB values of the first-person view of the agent. |
| IS HOLDING | 1 | Integer in {0,<br>1}<br>indicating whether the agent is hold |
| | | ing an object. |
| HAND DISTANCE | 1 | Distance to the held object as a fraction of the agent's |
| | | maximum reach (0 while no object is held). |
| HAND FORCE | 1 | The force exerted by the agent on the held object |
| | | as a fraction of its maximum grip force (0 while no |
| | | object is held). |
| LAST ACTION | 10 | Last action performed by the agent. |
| GOAL ATOMS | 6 ×<br>6 | Agent-readable description of the goal. For legacy |
| | | reasons, only the first row is non-zero. The first row |
| | | (6 numbers) describes the goal with elements: |
| | | 1: whether the goal is negated or not, |
| | | 2: index of the binary predicate, |
| | | 3-4: shape and colour of the first objects, |
| | | 5-6: shape and colour of the second object. |
| GOAL SOP MATRIX | 6 ×<br>6 | Always the same, kept for legacy reasons. |
| ATOM REWARDS | 6 | The reward of the agent in the previous frame (1 |
| | | number), padded with zeros for legacy reasons. |
| OPTION REWARDS | 6 | Same as ATOM REWARDS, kept for legacy reasons. |
| PRODUCTION RULES | 16 ×<br>26 | A description of up to 16 production rules. A single |
| | | ×<br>production rule is described as:<br>3<br>6<br>=<br>18<br>numbers |
| | | describing up to three object-predicate-object trig |
| | | gers and 4 ×<br>2 =<br>8 numbers describing up to 4 spawn |
| | | objects. We only ever use a single object-predicate |
| | | object trigger for all tasks in this paper. Hiding (Sec |
| | | tion<br>2.1) is implemented by adding extra predicate |
| | | and shape indices meaning hidden production rule, |
| | | first hidden object, etc. |
| REWARD | 1 | The environment reward obtained in the previous |
| | | step. |
| TRIALS REMAINING | 5 | One-hot encoding of the number of trials remaining |
| | | in the episode, up to a maximum of 5 trials remaining. |
| MORE THAN 5 TRIALS | 1 | {0,<br>1}<br>Integer in<br>indicating whether there are more |
| TIME UNTIL LAST | 1 | than 5 trials remaining in the episode.<br>Time remaining (in seconds) until the final trial in |
| TRIAL | | the current episode. |
| TIME LEFT IN CURRENT | 1 | Time remaining (in seconds) until the end of the |
| TRIAL | | current trial. |
## | | | |

| DURATION OF LAST | 1 | The duration (in seconds) of the final trial in this |
|------------------|---|----------------------------------------------------------|
| TRIAL | | episode. In our setting, all trials for the same task |
| | | have the same duration. |
| DURATION OF NEXT | 1 | The duration (in seconds) of the next trial in the |
| TRIAL | | current episode. In our setting, all trials for the same |
| | | task have the same duration. |

# **D. Training Details**

#### **D.1. Meta-RL**

AdA is trained with a meta-RL setup, in which episodes of experience for the agent comprise multiple trials of interaction with the task environment, where the task is reset on trial boundaries. In this setting, it is known that the agent's policy can converge to Bayes-optimal behaviour, experimenting on-the-fly to reduce its epistemic uncertainty, and reusing discovered information to achieve goals increasingly efficiently. It is important that the agent's memory is not reset at trial boundaries, but only at episode boundaries. Similarly, the agent trains with a fixed discount factor ≠ 0 throughout the episode, including on trial boundaries. For training, we sample (, ) pairs according to a factorised distribution (M, ), the parameters of which are controlled by an automatic curriculum (Section 2.3\). The MDPs are all drawn from a procedurally generated domain M, called XLand (Section 2.1\). We choose as our space of trials = {1, 2, . . . 6}, and provide our agent with as a conditioning input. After training, our agent is capable of few-shot adaptation across a wide-range of MDPs, including in held-out ∈ M on which <sup>M</sup> puts no probability mass, and when > 6.

#### **D.2. Single-agent training**

Our single-agent training setup uses a task pool generated as described in Section 2.1. The experimental setup for the single-agent distillation teacher is summarised in Table D.1. Single-agent training used an earlier version of XLand 2.0 than multi-agent experiments, without the frozen objects described in Section A.1. Frozen objects were also therefore excluded from the test and hand-authored probe task sets. AdA was implemented using JAX \(Bradbury et al., 2018\) and the Deep Mind JAX Ecosystem \(Babuschkin et al., 2020\) and trained on 64 Google TPUv3 devices. The wall-clock time for training this version of AdA from scratch was approximately 5 weeks: 1 week to train the teacher, and 4 weeks to train AdA. Even after this amount of training, AdA had not reached convergence, illustrating the benefits of open-ended learning methods.

Table D.1 | Distillation teacher for the single-agent experiments in Section 3.1.

| Model Parameters | Memory | Task pool | Curriculum | Teacher | Steps |
|---------------------|--------|-----------|------------|---------|-------|
| 23M TXL / 76M total | 1800 | 25B | PLR D.5 | None | 25B |

#### **D.3. Multi-agent training**

Starting from the 25B-sized task pool used for single-agent training, we generate a two-player task pool of the same size by the procedure described in Appendix A.2. For all our multi-agent experiments, we use a half-half mixture of single-player and two-player tasks, as described in Section 2.1. For each task we decide whether to spawn some of the initial objects permanently frozen (see Appendix A.1\)

with 50% probability. For tasks with frozen objects, we iterate over the initially spawned object types and freeze all spawned instances of this type with a probability of 20%, while ensuring that the task remains solvable.

During training we uniformly sample a co-player policy from a pool generated using fictitious self-play. The co-player pool is initialised with a random-action policy. Every 500M training frames we add a snapshot of the learning agent to the pool, thereby adding more and more capable co-players over time. Finally we apply the PLR auto-curriculum method (see Section 2.3) to curate the tasks (worlds, games and co-players) using the agent's TD-error based fitness. The experimental setup for the multi-agent distillation teacher is summarised in Table D.2.

Table D.2 | Distillation teacher for the multi-agent experiments in Section 3.1.

| Model Parameters | Memory | Task pool | Curriculum | Teacher | Steps |
|---------------------|--------|-------------|------------|---------|-------|
| 23M TXL / 76M total | 1800 | see Sec D.3 | PLR D.5 | None | 22B |

#### **D.4.** Architecture experiments

Table D.3 shows the experimental setup for the experiments comparing different memory architectures in Section 3.2.

Table D.3 | Experimental setup for comparing different memory architectures.

| Architecture | Parameters | Memory | Task pool | Curriculum | Teacher | Steps |
|---------------------------|------------|--------|-----------|------------|---------|-------|
| Transformer-XL | | 1800 | | | | |
| <b>GRU</b> with Attention | 76M total | - | 25B | No-op | None | 50B |
## | GRU | | - | | | | |

#### D.5. Auto-curriculum learning

**No-op filtering details.** Here we provide additional details of No-op filtering. For each task from the XLand training pool, we evaluate the learning agent (without sending any experience to the learner) and no-op policy on the task for 10 independent episodes, each of length 1 trial, producing scores $\{R_0, \ldots, R_9\}$ , $\{R'_0, \ldots, R'_9\}$ , respectively. We admit the proposal task for training if it satisfies the following criteria:

- 1. $\max R_i' \le \epsilon_1$ (No-op is not too good.)
- 2. $|\{i: R_i \ge \epsilon_2\}| \le \epsilon_3$ (Agent is not too good.)
- 3. $|\{i: R_i \ge \max R_i' + \epsilon_0\}| \ge \epsilon_4$ or $|\{i: R_i \le \min R_i' \epsilon_0\}| \ge \epsilon_5$ (Agent is sufficiently different from no-op.)
- 4. $\max R_i \min R_i \ge \epsilon_6$ (Agent scores have sufficient variance.)

The $\epsilon_i$ 's are thresholds and become hyperparameters in our training setup. Since different tasks have different durations in general, we use relative thresholds defined as a fraction of trial duration for $\epsilon_0$ , $\epsilon_1$ , $\epsilon_2$ , $\epsilon_6$ and absolute thresholds for the rest. Once a task is admitted for training, it is run using the full number of trials specified by the task and for 30 episodes. All experience from these runs are sent to the learner. See Table D.4 for the hyperparameters used.

Table D.4 | No-op filtering hyperparameters.

| Parameter | Value | Relative to trial duration |
|-----------|-------|----------------------------|
| 𝜖0 | 0.01 | Y |
| 𝜖1 | 1.1 | Y |
| 𝜖2 | 0.4 | Y |
| 𝜖3 | 5 | N |
| 𝜖4 | 1 | N |
| 𝜖5 | 3 | N |
## | 𝜖6 | 0.01 | Y |

**PLR details.** Here we provide additional details for Prioritised Level Replay (PLR), used in training AdA. PLR uses a *fitness score* that approximates the agent regret for a given task \(Jiang et al., 2021a, b\). PLR maintains an archive P of tasks to replay with fixed maximum size. With probability (referred to as the *replay probability*) a task is sampled from P while taking into account the fitness score and staleness of each task (see Jiang et al. \(2021b\), Section 3) to train the learning agent. The staleness represents how much time has passed since the task was last sampled, and ensures that all tasks in P have accurate scores. The final probabilities are computed by combining the fitness and staleness scores, with staleness weighted using the parameter ∈ [0, 1].

Tasks are added to P by first sampling with probability 1− a proposal task from the training task set and evaluating its fitness score. If the fitness score is greater than the minimum fitness score of all tasks in P, the proposal task is added to P. If the new size of P exceeds the fixed maximum size, the lowest fitness task is dropped. Note that in PLR a task can potentially be trained on indefinitely, if it never leaves P.

We found that using last-trial fitness led to better empirical performance and sample efficiency than first or average trial fitness. This is likely because in earlier trials, error-based fitness is higher as the agent is pursuing exploratory behavior, which should not be taken as a sign that the policy is sub-optimal. However, high error-based fitness in the last trial likely indicates a sub-optimal policy when solving the task after time for adaptation, analogous to the regret approximation in the original single-trial PLR.

In order to use the last-trial fitness as our objective we need to make a number of changes to the original PLR framework, which was designed for single trials in a more homogeneous domain. We denote the per-step fitness score at the th step of trial by ,. First, to avoid adding a bias towards longer trial durations we use the *average* per-trial fitness score ˜ , Í ˜,/ where is the number of steps per trial. Next, to ensure we do not prioritise lower values of , which tend to have a higher average last-trial fitness score, we then normalise ˜ by , ( ˜ − )/ where and are rolling per-trial means and variances for each trial index, calculated from all evaluated tasks. Finally, we can define the fitness for PLR to be , the normalised last-trial fitness score.

As described in Jiang et al. \(2021a, b\), PLR contains the following hyper-parameters: replay probability , maximum replay size max, minimum replay size min (we set = 0 if |P | < min), train the total number of trials for which to run a training task before re-sampling, and the staleness coefficient. See Table D.5 for the hyper-parameters used. We conducted a grid search over the replay probability ∈ {0.1, 0.2, 0.5}, size of the replay pool max ∈ {1000, 10000, 50000} and staleness coefficient ∈ {0.1, 0.2}, and in all cases set min to be 90% of max.

Table D.5 | PLR hyperparameters.

| Parameter | Value |
|----------------|-------|
| p | 0.2 |
| $N_{ m max}$ | 1000 |
| $N_{ m min}$ | 900 |
| $N_{ m train}$ | 30 |
## | S | 0.2 |

**PLR fitness metric.** It remains for us to define the per-step fitness score $f_{i, k}$ . For this, we use the simplest regret-like metric, the 1-step TD-error (Jiang et al., 2021b; Schaul et al., 2015). Concretely, we estimate the *TD-error fitness* based on the immediate value-predictions of the model: $|r_t + \gamma \hat{v}_0^{t+1} - \hat{v}_0^t|$ . In some settings this may be undesirable, for example, TD-errors typically increase as the agent achieves higher rewards. Therefore, we also propose to compute fitness metrics based on the Muesli dynamics model. Rather than simply using the accuracy of the model prediction, we look at the impact of the prediction on the value function and action predictions. We define the *value-model fitness* as $|\hat{v}_0^{t+1} - \hat{v}_1^t|$ , the difference between the value estimate at the predicted next state and the true next state. We also define a value-agnostic metric, the *action-model fitness* as follows: $JS(\hat{\pi}_0^{t+1}, \hat{\pi}_1^t)$ , i.e. the difference between the action predictions at the predicted next state and the actual next state, where difference is measured with the Jensen-Shannon divergence (Farquhar et al., 2021; Filos et al., 2022; Pislar et al., 2022; Xu et al., 2017).

In Figure D.1 we show training curves for TD-error fitness, value-model fitness, and action-model fitness. Table D.6 shows the experimental setup for these experiments. We see that both TD-error and action-model fitness metrics outperform the value-model fitness. We chose TD-error for our PLR training runs because it has better asymptotic performance in both the zero-shot and the few-shot setting, and because it was shown to perform well in previous work (Jiang et al., 2021a).

## ![](_page_42_Figure_5.jpeg)

Figure D.1 | PLR fitness metric comparison for zero-shot generalisation (k = 1) and few-shot adaptation (k = 13). We compare the TD-error fitness used in our main agents against two approaches using the Muesli dynamics model. We see that action-model fitness matches TD-error fitness in few-shot performance, with weaker zero-shot performance.

Task pool Fitness function Model parameters Memory Curriculum **Teacher** Steps TD error 1800 25B PLR 25B Value model 23M TXL / 76M total None Action model

Table D.6 | Experimental setup for comparing different PLR fitness functions.

![](_page_43_Figure_3.jpeg)

Figure D.2 | (a) Sample efficiency in steps for different choices of curricula. Both No-op and PLR significantly improves sample efficiency over uniform sampling of tasks. Few-shot denotes k = 13 score and zero-shot denotes k = 1 score. (b) Sample efficiency in FLOPs for different choices of curricula. No-op has an initial advantage over PLR, but PLR outperforms No-op later in training.

**Curriculum efficiency.** Next, we compare training sample efficiency for baseline uniform sampling and the different curriculum methods, in units of both learner steps and FLOPS. Figure D.2 shows the median last-trial scores for few-shot (k = 13) and zero-shot evaluation tasks as a function of learner steps. We see that both No-op filtering and PLR curricula strongly improve training sample efficiency over uniform sampling, with PLR being more efficient early in the training process. When we plot the same few-shot median performance as a function of FLOPS, we see that No-op has a slight early advantage, but PLR outperforms No-op later in training. The initial advantage for No-op may be because No-op expends more FLOPS (10 evaluations per task vs. 1 in PLR) for task evaluation, which finds higher quality training tasks at the start of training.

**Emergent curricula.** In Figure D.3 we show task metrics analysing the tasks selected by PLR and No-op filtering, expanding upon the results shown in Figure 11.

#### D.6. Distillation teacher for scaling experiments

In all of our scaling experiments (Sections 3.4 and 3.5), we distill the policy from an identical teacher snapshot to ensure our experiments are comparable. Training details for the teacher are detailed in Table D.7. This teacher is used to kickstart our agents for their first 4B frames of training.

#### D.7. Scaling the network

Table D.8 shows the experimental setup for the model size scaling experiments in Section 3.4. More details about the number of parameters for the various model sizes can be found in Table D.9.

![](_page_44_Figure_1.jpeg)

Figure D.3 | Emergent curricula for No-op filtering and PLR. Plots show the full set of task metrics for the dynamic training set, averaged over all tasks in the set, with standard error shaded. In all plots, a higher metric value corresponds to greater task difficulty. Horizontal lines show the same metric values averaged over the test (dashed) and hand-authored (dotted) evaluation task sets.

Table D.7 | Distillation teacher for scaling experiments.

| Model parameters | Memory | Task pool | Curriculum | Teacher | Steps |
|---------------------|--------|-----------|------------|---------|-------|
| 23M TXL / 76M total | 1800 | 200M | No-op | None | 23B |

Table D.8 | Experimental setup for model size scaling.

| Model parameters | Memory | Task pool | Curriculum | Teacher | Steps |
|-----------------------|--------|-----------|------------|-----------|-------|
| 6M TXL / 41M total | | | | | |
| 23M TXL / 76M total | | | | | |
| 42M TXL / 112M total | | | | | |
| 57M TXL / 141M total | 1800 | 25B | No-op | Table D.7 | 75B |
| 75M TXL / 175M total | | | | | |
| 169M TXL / 353M total | | | | | |
## | 265M TXL / 533M total | | | | | |

Transformer-XL memory is a cached memory of previous attention layer inputs, concatenated to the keys and values during each forward pass. Inputs to intermediate layer are activations from the previous layer, which in themselves contain information about the past. Caching activations this way theoretically allows for an effective memory horizon of x , where is the number of attention layers in the network. Therefore, to avoid implicitly scaling effective Transformer-XL memory length, in our model size scaling experiments, we fix the number of layers in the Transformer, and scale parameters only by altering the Transformer embedding size ( $d_{\text{model}}$ ), with the feed-forward size fixed at $4d_{\text{model}}$ , as is standard in Transformer architectures (Vaswani et al., 2017).

Table D.9 | Transformer hyperparameters for different model sizes.

| Model parameters | Embedding size | Blocks | Key size | Value size | Heads | FFW size |
|-----------------------|----------------|--------|----------|------------|-------|----------|
| 6M TXL / 41M total | 288 | 6 | 48 | 48 | 6 | 1152 |
| 23M TXL / 76M total | 576 | 6 | 48 | 48 | 12 | 2304 |
| 42M TXL / 112M total | 768 | 6 | 32 | 32 | 24 | 3072 |
| 57M TXL / 141M total | 896 | 6 | 32 | 32 | 28 | 3584 |
| 75M TXL / 175M total | 1024 | 6 | 32 | 32 | 32 | 4096 |
| 169M TXL / 353M total | 1536 | 6 | 48 | 48 | 32 | 6144 |
| 265M TXL / 533M total | 1920 | 6 | 48 | 48 | 40 | 7680 |

#### D.8. Scaling the memory length

Table D.10 shows the details of the experimental setup for the memory length scaling experiments in Section 3.4. We show the effective memory timesteps for each experiment, computed as the number of cached network activations times the number of transformer blocks (6).

Table D.10 | Experimental setup for scaling the memory length.

| <b>Model Parameters</b> | Memory | Training task pool | Curriculum | Teacher | Training steps |
|-------------------------|--------|--------------------|------------|-----------|----------------|
| | 600 | | | | |
| 23M TXL / 76M total | 1800 | 200M | No-op | Table D.7 | OED. |
| 23W TAL / 76W total | 3000 | 200W | No-op | Table D./ | 25B |
## | | 4200 | | | | |

#### D.9. Scaling the size of the task pool

Table D.11 shows the details of the experimental setup for scaling the size of the task pool in Section 3.5.

Table D.11 | Experimental setup for scaling the task pool size.

| Model parameters | Memory | Training task pool | Curriculum | Teacher | Training steps |
|----------------------|--------|--------------------|------------|-----------|----------------|
| 23M TXL / 76M total | | 200M | | | |
| 23W TAL / / OW total | 1000 | 25B | No on | Table D.7 | ) DED |
| 75M TVI / 175M total | 1800 | 200M | No-op | Table D./ | 25B |
## | 75M TXL / 175M total | | 25B | | | |

#### D.10. Scaling the complexity of the task pool

Table D.12 shows the details of the experimental setup for scaling the complexity of the task pool in Appendix E.3. In this experiment, the distillation teachers are different for the two agents we compare. Therefore we cannot disentangle the effects of distillation and task complexity. Nevertheless, the results remain indicative of the importance of task complexity. The teacher for the task distribution across multiple world topologies is trained as in Table D.7. The teacher for the task distribution in a single room comes from a long lineage (> 6 generations) of distillation teachers starting with agents trained on XL and 1.0 (OEL Team et al., 2021).

Table D.12 | Experimental setup for scaling the complexity of the task distribution.

| Model parameters | Memory | Task pool | Curriculum | Steps |
|----------------------|--------|----------------------------------------------|------------|-------|
| 6M TXL / 41M total | | | | |
| 23M TXL / 76M total | | | | |
| 42M TXL / 112M total | 1800 | 4k worlds $\times$ 50k games | No-op | 23B |
| 57M TXL / 141M total | | | | |
| 75M TXL / 175M total | | | | |
| 6M TXL / 41M total | | | | |
| 23M TXL / 76M total | 1800 | 1 world $\times$ 5k inits $\times$ 50k games | No on | 23B |
| 42M TXL / 112M total | 1800 | 1 World × 3k lilits × 30k gailles | No-op | 230 |
## | 57M TXL / 141M total | | | | |

#### D.11. Distillation enables scaling agents

Table D.13 shows the experimental setup for the distillation experiments in Section 3.6.

Table D.13 | Experimental setup for distillation experiments.

| Model Parameters | Memory | Training task pool | Curriculum | Teacher | Steps |
|---------------------------|--------|--------------------|------------|-----------|-------|
| TXL 23M TXL / 76M total | | | PLR (D.5) | Table D.2 | |
| TAL 25W TAL / 70W total | 1800 | See Sec D.3 | | None | 22B |
| TXL 265M TXL / 533M total | 1000 | See Sec D. S | PLK (D.3) | Table D.2 | ZZD |
## | TAL 203W TAL / 333W total | | | | None | |

#### D.12. Training on more trials with skip memory

Table D.14 shows the details of the experimental setup for the memory scaling experiments in Section 3.7. This is the same setup as in Table D.14, except for the number of training steps and the variation of memory architecture and training trials discussed in the main text.

Table D.14 | Experimental setup for experiments training on more trials with skip memory.

| <b>Model Parameters</b> | Memory | Training task pool | Curriculum | Teacher | Steps |
|-------------------------|---------------------------------------------------------------------------|--------------------|------------|-----------|-------|
| 23M TXL / 76M total | $ \begin{array}{c} 1800 \\ 1800 \times 4 = 7200 \end{array} $ | 200M | No-op | Table D.7 | 50B |

# **E. Additional Experiments**

## **E.1. Multi-agent adaptation**

## ![](_page_47_Figure_3.jpeg)

Figure E.1 | We report the distribution of normalised task scores over the multi-agent task test set when evaluated with various numbers of trials. All tasks are evaluated in cooperative self-play. On the -axis is the total last-trial reward relative to that of an agent fine-tuned on the test tasks (approximating "infinite trials" performance). Curves moving further towards the top right corner indicate better performance. When given more trials, the agent achieves higher scores in the last trial, showing test-time adaptation across most of the task distribution (shaded regions).

![](_page_47_Figure_5.jpeg)

Figure E.2 | Average performance and representative behaviour of AdA on the probe task Irreversible Production for Two when evaluated in self-play with various numbers of trials. AdA's performance increases when given more trials, showing test-time adaptation. The top-down view images show representative last-trial trajectories when given different numbers of total trials. A corresponding [video](https://youtu.be/h Bc-7YMVo0U) for the case = 8 shows the behaviour across all trials within one episode.

Figure E.1 demonstrates adaptation across a wide range of percentiles on a test-task set of multiagent tasks. Figure E.2 demonstrates last-trial performance of AdA in one particular probe task. To generate these plots, AdA was trained as described in Section 3.1 (Multi-agent).

![](_page_48_Figure_1.jpeg)

Figure E.3 | A comparison of the first-trial score in episodes with 1 trial and episodes with 8 trials. The lines are almost perfectly overlapping, which indicates that our agent does not leverage numberof-trials conditioning information to adjust its policy to a more exploratory one in early trials when more trials are available.

## **E.2. Conditioning on number of shots doesn't affect agents' performance**

Figure E.3 shows the score obtained by AdA for each percentile, in trial 1 of episodes with only 1 trial ( = 1) and in trial 1 of episodes with 8 trials ( = 8) in our held-out test set. The overlap of these lines indicates that AdA does not use the trial conditioning information it observes to adjust its behaviour in any way that affects its score. If the agent were to follow a more exploratory policy when it has more trials, we might expect the scores of trial 1 with = 8 to be lower than the score of trial 1 with = 1.

This may be the optimal policy for our XLand 2.0 tasks, or it may reveal a limitation of our training procedure. One can imagine a scenario in which, knowing that there are 8 trials in total, a Bayes-optimal policy chooses to display a less rewarding and more exploratory behaviour in trial 1, compared to how it would behave if told that there was only a single trial in which to collect reward. For instance, an agent may be able to guarantee a deterministic reward later, having discovered some key information, at the cost of foregoing an stochastic reward early on. We did not directly incentivise this behaviour in our training process. In fact, we may have discouraged it, since AdA learns from all rewards in an episode (not just in the last trial), and with a discount factor smaller than 1, which could lead to myopic behaviour.

#### **E.3. Scaling complexity of the task pool**

One final axis along which it is possible to scale our method is the overall complexity of the task distribution. We compare our main task distribution, as described in Section 2.1 and Appendix A.2 to a subset which maintains the use of the same goals, production rules, and objects, but eliminates any navigational complexity by having a single world topology: an empty room. Recall that we count the number of tasks as the product of number of worlds and the number of games. To disentangle the effects of scaling complexity versus scaling the sheer number of tasks, we add 5,000 unique object initialisation points for the empty room. These serve as the proxy "4,000 worlds" and are, by design, much less diverse and complex than the 4,000 worlds in the main training pool.7 For more details of the experimental setup, see Appendix D.10 and Table D.12.

<sup>7</sup>Note that the distribution over world topologies we use here is smaller than the distribution used in the model scaling experiments in Section 3.4, and results are therefore not comparable across these sections.

![](_page_49_Figure_1.jpeg)

## ![](_page_49_Figure_2.jpeg)

- (a) Task distribution with only empty room inhibits scaling (median).
- (b) Task distribution with only empty room inhibits scaling (20th percentile).

## ![](_page_49_Figure_5.jpeg)

## ![](_page_49_Figure_6.jpeg)

- (c) Task distribution with many world topologies facilitates scaling (median).
- (d) Task distribution with many world topologies facilitates scaling (20th percentile).

Figure E.4 | The benefit of scaling model size is bottlenecked if the distribution is not complex enough, even if the total number of tasks is accounted for.

In Figure E.4, we show that low environment complexity can be a bottleneck to scaling, by comparing the effectiveness of model scaling between agents trained on the two distributions, and each evaluated on their respective test sets. On both the median (Figure E.4a\) and 20th (Figure E.4b\) percentiles in the empty room, we see that past a certain point (42M Transformer parameters), scaling model size begins to reduce performance. By contrast, in the distribution with many world topologies (Figures E.4c and E.4d\), increased model size continues to improve performance far beyond this, showing improvements through at least 75M Transformer parameters. Open-ended settings with unbounded environment complexity, such as multi-agent systems, may therefore be particularly important for scaling up adaptive agents.

## **E.4. Computational cost**

In the scaling experiments (Sections 3.4 and 3.5\), we compare agents after they have been trained for an equivalent number of steps. While this controls for sample efficiency of models, here we provide an analysis of the computational cost in FLOPs for a given experiment, and reproduce some of our scaling results, controlling for for compute cost. We see that bigger is not always better from this perspective. For each model size and memory length we use JAX \(Bradbury et al., 2018\) cost analysis to estimate the number of FLOPs per frame of the learner step and actor step (Table E.1\).

![](_page_50_Figure_1.jpeg)

Figure E.5 | Scaling Transformer model size controlling for the total number of FLOPs for the learner and actors, including auto-curriculum evaluation actors.

![](_page_50_Figure_3.jpeg)

Figure E.6 | Scaling Transformer-XL memory controlling for the total number of FLOPs for the learner and actors, including auto-curriculum evaluation actors.

Table E.1 | FLOPs per frame for different model sizes and memory lengths.

| Model parameters | Memory | Learner FLOPs per frame | Actor FLOPs per frame |
|-----------------------|--------|-------------------------|-----------------------|
| 6M TXL / 41M total | | 1,138,005,632 | 736,987,392 |
| 23M TXL / 76M total | | 1,380,255,078 | 2,580,445,440 |
| 42M TXL / 112M total | | 1,623,461,663 | 4,489,239,552 |
| 57M TXL / 141M total | 1800 | 1,821,422,230 | 6,063,742,976 |
| 75M TXL / 175M total | | 2,048,253,663 | 7,879,863,808 |
| 169M TXL / 353M total | | 3,243,189,525 | 17,560,326,144 |
| 265M TXL / 533M total | | 4,443,008,234 | 27,358,181,376 |
| | 600 | 1,278,491,404 | 963,739,136 |
| 23M TXL / 76M total | 3000 | 1,481,009,258 | 4,181,042,688 |
## | | 4200 | 1,582,270,213 | 5,789,702,144 |

We multiply the values in Table E.1 by the number of learner/actor steps for each experiment, then, for a given comparison, we take the largest such value common to all experiments (usually associated with the smallest model) as the total FLOPs, and make the comparison of each model at this number of FLOPs. The results for the FLOPs-matched model scaling experiments are shown in Figure E.5. We see a reduction in performance as the model size grows beyond a "sweet spot" around 57M Transformer parameters (141M total parameters). Results for the FLOPs-matched memory scaling experiments in Figure E.6 show that there is still benefit to increasing context lengths given a fixed computational budget. Details of the compute used for these experiments can be found in Tables E.2 (model size) and E.3 (memory).

We note that Table E.1 indicates poor scaling of actor step FLOPs with model size, and suspect this could be due to poor optimisation of a single-step query of the Transformer on TPU, compared to the operation batching achieved with a rollout length of 80 on the learner. In order to account for this and for potential discrepancies in number of actor steps due to differences in curriculum evaluation based on model quality, we also provide plots which only account for the learner step FLOPs for each model: Figures E.7 (model size), and E.8 (memory length). These might be more informative, and show that performance still increases as a function of model size and memory length, albeit not as steeply as when controlling for sample efficiency directly. Details of the compute used for these experiments is shown in Tables E.4 (model size) and E.5 (memory length).

![](_page_51_Figure_3.jpeg)

Figure E.7 | Scaling Transformer-XL model size controlling for the number of learner FLOPs.

![](_page_51_Figure_5.jpeg)

Figure E.8 | Scaling Transformer-XL memory length controlling for the number of learner FLOPs.

Table E.2 | Corresponding learner steps given total FLOPs for each model size.

| Model parameters | Memory | Total FLOPs | Learner steps |
|-----------------------|--------|----------------------|---------------|
| 6M TXL / 41M total | | | 97B |
| 23M TXL / 76M total | | | 44B |
| 42M TXL / 112M total | | | 27B |
| 57M TXL / 141M total | 1800 | $2.0 \times 10^{20}$ | 23B |
| 75M TXL / 175M total | | | 18B |
| 169M TXL / 353M total | | | 9B |
## | 265M TXL / 533M total | | | 5B |

Table E.3 | Corresponding learner steps given total FLOPs for each memory length.

| Model parameters | Memory | Total FLOPs | Learner steps |
|---------------------|--------|------------------------|---------------|
| 23M TXL / 76M total | 600 | | 31B |
| | 1800 | $7.9 \times 10^{19}$ | 17B |
| | 3000 | 7.9 x 10 <sup>-3</sup> | 11B |
## | | 4200 | | 8B |

Table E.4 | Corresponding learner steps given learner-step-only FLOPs for each model size.

| Model parameters | Memory | Learner step FLOPs | Learner steps |
|-----------------------|--------|----------------------|---------------|
| 6M TXL / 41M total | | | 92B |
| 23M TXL / 76M total | | | 76B |
| 42M TXL / 112M total | | | 65B |
| 57M TXL / 141M total | 1800 | $1.0 \times 10^{20}$ | 58B |
| 75M TXL / 175M total | | | 51B |
| 169M TXL / 353M total | | | 32B |
## | 265M TXL / 533M total | | | 23B |

Table E.5 | Corresponding learner steps given learner-step-only FLOPs for each memory length.

| Model parameters | Memory | Learner step FLOPs | Learner steps |
|---------------------|--------|----------------------|---------------|
| 23M TXL / 76M total | 600 | | 39B |
| | 1800 | $3.9 \times 10^{19}$ | 36B |
| | 3000 | | 29B |
## | | 4200 | | 25B |

#### E.5. Repeated distillation

In Section 3.6, we show that distilling an agent into an identical student can lead to large increases in the agent's performance. Here, we investigate the potential benefits of applying the procedure repeatedly. To this end, we continue the experiment shown in Figure 16 and add a third generation, using a snapshot taken from the previous student after 25 billion frames, equivalent to 50 billion frames of total experience when taking the teacher's experience into account. Figure E.9 shows that applying this procedure repeatedly can indeed lead to additional benefits; however, we observe diminishing returns with successive generations.

![](_page_53_Figure_1.jpeg)

## ![](_page_53_Figure_2.jpeg)

Figure E.9 | Normalised few-shot score over three generations using the 23M parameter Transformer-XL. The first and second generations correspond to the agents shown in Figure 16. The third generation is distilled from the second after it has been trained for 25 billion steps. The x-axis counts the combined amount of experience, starting from the experience collected by the original teacher. The third generation shows additional gains over the second, but to a lesser degree than the gap between the first and second generations.

# F. Human-Timescale Adaptation

In this section we provide more details regarding our claim of human-timescale adaptation.

#### F.1. Probe tasks

Tables F.1 and F.2 describe in detail the single-agent and multi-agent probe tasks respectively. Unless noted otherwise, all probe tasks are set in complex worlds with many objects in them, and use multiple production rules, which are fully hidden from the players.

Table F.1 | Single-agent probe tasks

| Name | Description |
|---------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Wrong pair<br>disappears | The player's goal is to hold a black cube, which does not exist among the initial objects. There are two (hidden) rules. The player needs to identify the correct world state which triggers the rule that creates the cube and not the one which destroys the necessary inputs. All this is embedded in a challenging world layout with one-way drops and limited visibility. |
| Wrong pair<br>disappears, partial<br>hiding | 'Wrong pair disappears', but instead of hiding all rules completely we only hide the input objects for both rules (outputs and conditions are fully visible). |
| Irreversible production | Similar to 'Wrong pair disappears', but this task has multiple dead ends (rules which create unsolvable states). |
| Irreversible production, all rules visible | 'Irreversible production', but with all rules fully visible to the player. |

| Push, don't lift | The vast majority of training and evaluation tasks require lifting objects.<br>Here two hidden rules destroy any object when lifted. In order to create<br>the goal state, some "lateral thinking" is necessary: the player needs to<br>identify that pushing the cubes with their body is possible. |
|-----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Push, don't lift, with<br>distractors | Similar to 'Push, don't lift', but here a large number of distractor objects<br>in the world make this a much more challenging exploration task for any<br>player ignoring the objects mentioned in the goal. |
| Spacer tool | Two objects need to be brought close together, but lifting them or touching<br>them (with the avatar's body) destroy them. The solution is to use another<br>object in the world as a tool to push them together. |
| Transform to<br>transport | Again, two objects need to be brought close to each other, but lifting and<br>touching them destroys them. The solution here is to exploit a set of rules<br>that can turn one of the objects into something that can be carried safely,<br>and then turn it back into the original object once it is in the right place. |
| Small workstation | A very hard object handling task.<br>8 objects near the player's spawn<br>position need to be combined in different ways and 5 rules need to be<br>triggered (some multiple times) to create the goal object. This task is set<br>on top of a tall plateau and it is very easy to fail by touching an object<br>too hard and throwing it off the edge of the plateau. |
| Small workstation, all<br>rules visible | The same as 'Small workstation', but all rules are visible to the player. |
| Crafting pyramid | Eight objects need to be recursively combined first into four, then two<br>and then ultimately one final object. This requires triggering a chain of 7<br>rules. This seems easy for humans. But the lack of intermediate reward<br>makes this a hard hierarchical credit assignment task for agents. |
| Crafting tree, all rules<br>visible | Similar to the crafting trees in video games like Minecraft, this multi<br>step task requires triggering different rules in a chain to create the goal<br>object. All objects exist in the world multiple times, making many different<br>solutions viable. |
| Crafting tree, hidden<br>shortcut | Identical to 'Crafting tree, all rules visible', but one additional (hidden)<br>rule exists. This allows the player to take a shortcut that lets them finish<br>the task faster than executing only the visible rules. |
| Antimatter | In a world full of yellow and black spheres, the goal is for no pair of black<br>and yellow spheres to "see each other" (no direct line of sight). Beyond<br>moving the objects and blocking line of sight with the avatar there exists<br>a production rule which destroys any yellow sphere and black sphere pair<br>which touch (similar in spirit to matter and antimatter particles). This is<br>all embedded in a world requiring advanced navigation skills. |
| Antimatter with<br>creation | Similar to 'Antimatter', only here a third object (purple pyramids) exists<br>which duplicates any sphere touching it. In addition, this task is set on<br>two plateaus, making it easier to break line of sight and reducing the<br>navigation challenge. |

| Pyramid in a haystack | To create the necessary yellow pyramid, the player needs to find and hold<br>the purple cube. There are several distractor objects and distractor rules<br>in this world, requiring the player to deal not just with a hard exploration |
|---------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| | challenge but also a very noisy environment. |
| Protect the egg | The player is tasked to hold the single yellow sphere in the world. A large<br>number of other spheres exist in the world. These destroy the yellow<br>sphere on collision. As these touch each other or get near the player they<br>get duplicated. This can lead to a constantly growing number of objects,<br>filling up the world. |
| 3 spheres jumbled | 3 spheres exist in the world. Holding one of them creates the goal object.<br>Only one sphere can be reached within the 10-second time limit, meaning<br>that the optimal policy on the first trial is to choose uniformly at random. |
| Two doors | The goal object is hidden behind one of the two large objects (the "doors")<br>positioned at opposite ends of the world. Only one of them can be reached<br>in time, so the player needs to decide between exploring one of them per<br>trial. |
| Same signature:<br>match colours | All 'Same signature' tasks have the exact same world layout, goal and<br>number of fully-hidden rules. This means they look exactly the same to a<br>player starting out. This one only requires two objects of matching colour<br>to be brought close together to create the goal object, using only one<br>production rule out of three. |
| Same signature: three<br>steps | This 'Same signature' task variant requires the player to trigger all three<br>(hidden) production rules to create the goal object. |
| Same signature: two<br>dead ends | In this 'Same signature' task variant, two of the tree rules are dead ends,<br>leading to an unsolvable world state. Only one rule is helpful (and in fact<br>required) to solve the task. |
| Same signature:<br>destroy to protect | To solve this 'Same signature' task variant the player first needs to destroy<br>a black sphere (by getting near it) before creating the goal object (a yellow<br>cube). Otherwise when the black sphere "sees" the yellow cube, both<br>get destroyed. This is a very hard credit assignment challenge even for<br>human players (it is hard to notice what is going on). |
| Don't act | This task is pre-solved: the player is getting reward from the very begin<br>ning. If they lift or touch one of the two objects in the world, the object<br>gets destroyed and reward is now impossible. |
| Don't peek | This task is 'pre-solved': the player is getting reward from the very be<br>ginning.<br>However, the player will destroy any object they look at, at<br>which point reward is impossible. So the optimal strategy is to not look<br>at anything but the sky. |
| Navigation: find the<br>cube | This memory task is not using production rules. It uses a large world with<br>the goal object (a cube) hidden after a very winding path. |

| Navigation: find the<br>cube with teaser | This memory task is not using production rules. It is set in a large world<br>with the goal object (a cube) hidden after a very winding path. The object<br>is visible from the spawn point but out of sight after starting to traverse<br>the terrain. |
|------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Navigation: hold up<br>high | This memory task is not using production rules. The goal object (a pyra<br>mid) is hidden on top of a plateau and can only be seen when nearly<br>there. |
| Object permanence:<br>yellow cube | This memory task is not using production rules. The goal object (a yellow<br>cube) is visible from the spawn point. After moving for a bit, a decision<br>between two paths has to be made, with the correct path being to the<br>right. At this point the cube is no longer visible. |
| Object permanence:<br>black cube | This memory task is not using production rules. This has same world<br>layout as the task above, only here the player is asked to find the black<br>cube, which requires going to the left. |

Table F.2 | Multi-agent probe tasks

| Name | Description |
|--------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Pass over the wall | The players are separated by an opaque wall. They cannot see each other,<br>only their half of the world. The solution requires "passing" the accessible<br>objects on either player's side to other player to combine them into the<br>goal object. Then the players must make sure the correct player holds<br>this object. |
| Pass over the wall<br>repeatedly | Similar to 'Pass over the wall' but here 2 out of 3 initial objects are 'frozen'<br>(cannot be moved). This prescribes a very specific solution strategy that<br>requires the players to pass 3 objects over the wall in a specific order. |
| Coordinated<br>production | This task requires each player to be near a sphere to turn this sphere<br>into a pyramid, and then for both pyramids to be touching each other<br>to create the goal object. The spheres are slightly hidden in a complex<br>world, requiring some exploration. |
| Coordinated<br>production with<br>deadends | Like 'Coordinated production' but here each player will destroy one of<br>the initial objects if they get near it. These dead-end rules make this a<br>much harder exploration problem. |
| Coordinated exchange | This requires each player to create a new object by holding an existing<br>object, then to hold the object created by the other player to turn it into<br>another intermediate object and finally for both objects to be combined<br>into the goal object. While the world is fully accessible to both players,<br>this can only be solved if both players actively participate. |
| Overcooked:<br>coordination ring | Inspired by the video game Overcooked<br>(Carroll et al.,<br>2019;<br>Strouse<br>et al.,<br>2021), in this task both players need to "serve tomato soup to a<br>hungry patron". This is implemented as a repeatable four-step production<br>rule chain which requires both players to traverse their shared space (a<br>circular layout) carefully in order to not block the other player. |

| Overcooked:<br>coordination ring, all<br>rules visible | While in 'Overcooked: Coordination ring' all production rules are hidden<br>from both players, here they are fully visible (to match the dynamics of<br>the original Overcooked game). |
|----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Overcooked: cramped<br>room | Similar to 'Overcooked: coordination ring' but with a different layout for<br>the shared space and a different number of initial objects, using different<br>shapes and colours. |
| Overcooked: cramped<br>room, all rules visible | While in 'Overcooked: cramped room' all production rules are hidden<br>from both players, here they are fully visible (to match the dynamics of<br>the original Overcooked game). |
| Overcooked: forced<br>coordination | Similar to the other 'Overcooked' task variants, but here both players<br>are restricted to only a certain part of the world and so are forced to<br>coordinate to solve this task. No player can solve this alone since they<br>cannot reach all initial objects. |
| Overcooked: forced<br>coordination, all rules<br>visible | While in 'Overcooked: forced coordination' all production rules are hidden<br>from both players, here they are fully visible (to match the dynamics of<br>the original Overcooked game). |
| Kickball | This task is set in large world with two frozen pyramids on opposite<br>sides of the world. Both players want to bring all of the plentiful purple<br>spheres to the yellow pyramid. But lifting them destroys the pyramids so<br>they need to "kick" them by bouncing them off the avatar. Think "soccer<br>practice". |
| Lemon eater | The first player destroys all yellow spheres (of which there are many)<br>when bumping into them. This is also the goal for both players. So they<br>need to cooperate to bring all yellow spheres to the first player as quickly<br>as possible. |
| Careful lemon eater | Like 'Lemon eater' but any collision between two spheres turns them from<br>yellow to purple. Purple spheres are "not edible" and so the players need<br>to be careful to not create those, otherwise they will lose out on reward. |
| Lemon eater and<br>maker | Like 'Lemon eater' but we start out with only purple spheres. Only the sec<br>ond player can turn purple into yellow spheres by lifting them, effectively<br>having to "create food" for the first player. |
| Antimatter for two | Identical in nature to the single-player 'Antimatter' task but set in a differ<br>ent world layout and with two players who share the same goal. |
| Antimatter with<br>creation for two | Identical in nature to the single-player 'Antimatter with creation' task but<br>set in a different world layout and with two players who share the same<br>goal. |
| Antimatter with<br>copies for two | Identical to 'Antimatter for two' but here any two spheres of the same<br>colour colliding leads to the creation of another sphere of that colour.<br>This can set in motion runaway growth in the number of spheres, making<br>it very hard to solve the task. |
| Irreversible<br>production for two | Identical in nature to the single-player 'Irreversible production' task but<br>set in a different world layout and with two players who share the same<br>goal. |

| Irreversible<br>production for two, all<br>rules visible | Like 'Irreversible production for two' but with all production rules visible<br>to both players. |
|----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Wrong pair<br>disappears for two | Identical in nature to the single-player 'Wrong pair disappears' task but<br>set in a different world layout and with two players who share the same<br>goal. |
| Wrong pair<br>disappears for two,<br>partial hiding | Like 'Wrong pair disappears for two' but with only the input objects of the<br>productions rules being hidden, the outputs and condition being visible. |
| Crafting pyramid for<br>two | Identical in nature to the single-player 'Crafting pyramid' task but set in a<br>different world layout and with two players who share the same goal. |
| Information<br>asymmetry | A simple world in which the players are asked to execute a two-step<br>production rule in the presence of multiple dead ends. While the first<br>player knows all the rules, they are completely hidden from the second<br>player. This task is intended to measure a specific flavor of third-person<br>imitation. |
| Information<br>asymmetry with<br>repetition | While 'Information asymmetry' only allows for up to four completions,<br>this task variant is (given unlimited time) infinitely repeatable, providing<br>more opportunities for imitation. |
| Combine outputs | Similar in nature to 'Coordinated production' but set in a larger world and<br>(through the use of frozen objects) requiring both players to repeatedly<br>navigate quite far to create input objects for shared creation. |
| Two machines | Many objects of different colours and shapes litter this world. The frozen<br>yellow pyramid on one end of the world transforms these objects into<br>an intermediary object. The players then need to bring the intermediary<br>object to the frozen black pyramid on the other end of the world to "cash<br>it in" for instantaneous reward, at which point the intermediary object is<br>destroyed. Therefore to get more reward, the players must repeat this<br>process. |
| Two machines with<br>information<br>asymmetry | Like 'Two machines', but all rules are visible to one player, and all rules<br>are hidden from the other player. This creates an information asymmetry<br>and thus an opportunity for third-person imitation. |

#### **F.2. Comparing human and agent scores on every probe task**

In Figure F.1 we show the raw last-trial total reward for humans and our agent as a function of number of trials across every one of the 30 evaluation probe tasks.

#### **F.3. Quantifying stochasticity**

**Task variation.** Figure F.2a shows that there is fairly high variance in the score obtained by a single agent over 50 repetitions of a single task. This is due to random spawn rotations of the avatar following every environment reset and stochasticity in the agent's policy. To address this we run 50 repetitions of every (task, ) combination and average the score over these repetitions. This reduces the standard deviation to that shown in Figure F.2b. Here, the standard deviation of the mean task

![](_page_59_Figure_1.jpeg)

Figure F.1 | Comparison of AdA against 19 human players on each of the 30 held-out single-agent hand-authored tasks. The performance of a baseline agent trained to optimise zero-shot performance is shown as a black dashed line and indicates that AdA does not sacrifice its zero-shot generalisation to achieve adaptation. The reward of an agent fine-tuned on the hand-authored tasks is also shown as a red dashed line to provide some indication of the maximum reward achievable in a trial of each task.

![](_page_59_Figure_3.jpeg)

Figure F.2 | **(a)** Mean single-repetition score and 95% confidence intervals over 50 samples. **(b)** Mean of the aggregated 50-repetition score and 95% confidence intervals over 50 samples.

score reduced from a maximum of 0.43 with 1 repetition, to a maximum of 0.06 with 50 repetitions.

## ![](_page_60_Figure_2.jpeg)

Figure F.3 | 95% bootstrap confidence intervals around the median and 20th percentile over our test set of 5 agents trained with different initialisation seeds. The maximum observed standard deviation was 0.04 around the median and 0.02 around the 20th percentile.

**Agent initialisation variation.** After accounting for task variation, Figure F.3 shows the variance due to agent initialisation seed during training. We plot the score as a function of on our test set for the 76M total parameter version of AdA with 5 different initialisation seeds. The maximum standard deviation observed for any number of trials in the median was 0.04 and for the 20th percentile was 0.02. This low initialisation seed variance led us to run our ablations with one initialisation seed to save on compute. We note that the results shown in our ablation section have significantly larger than one standard deviation differences.

#### **F.4. Prompting through first-person demonstrations**

Figure F.4 shows the performance of AdA prompted with a fine-tuned agent compared to an unprompted baseline on each of the 30 single agent hand-authored probe tasks. The figure reveals a set of tasks on which AdA is able to leverage information in the prompt, resulting in perfect or near perfect scores. There are also tasks where AdA does not seem to be able to do this. In all but one case, prompting does not hurt performance.

Analysing the tasks in Figure F.4 suggests that prompting is useful for short navigational tasks such as Navigation: hold up high in which the agent follows a short and simple path to reach the goal object. Prompting does not, however, improve performance for longer and more complex navigation tasks like Navigation: find the cube, likely due to the full demonstration being too long to fit in the agent's memory context.

We observe a similar pattern in tasks involving production rules. For tasks with up to 2 production rules in the solution path, such as Same signature: match colors, we observe the unprompted agent exploring different objects to determine the correct rule to trigger. When prompted with a demonstration it subsequently triggers the correct rule immediately and achieves a perfect score. An exception to this is Same signature: destroy to protect where one of the production rules involves destroying an object, which the agent does not appear to remember from the demonstration. For tasks using 3 or more production rules like Same signature: three steps (3 production rules in the solution path) and Small workstation (5 production rules), the agent tends to only remember a subset of the rules to trigger and continues engaging in exploratory behaviour following

![](_page_61_Figure_1.jpeg)

Figure F.4 | A comparison of a prompted agent and unprompted baseline for our full set of 30 hand-authored single-agent tasks. The dashed red lines indicate the score obtained by the teacher providing the first-person demonstration to the prompted agent in the first trial.

the demonstration. The performance on these tasks tend to match the unprompted baseline.

Another factor appearing to influence the effectiveness of prompting is the topology and configuration of objects in the world, as seen in the Small workstation tasks. While the teacher demonstrations for these tasks present a clean trajectory, the agent subsequently knocks into and displaces distractor objects, leading to environment states not observed during the demonstration (and thus not recallable from memory). On the other hand, a favourable configuration of objects appears to make the demonstration easier to learn from, as observed in Irreversible production. Here the objects required to trigger the last production rule are positioned close together. The agent shows optimal behaviour here despite the task requiring 3 production rules on the solution path. The agent also appears unable to infer from prompting certain more subtle requirements like relative positioning of objects or tool use. This is observed in tasks like Antimatter in which prompted AdA is able to trigger the destruction rule but we did not observe it immediately hiding objects from each other.

Prompting may also help eliminate biases that the agent may have acquired during training. This is reflected in the lower score obtained by the unprompted agent for small in Object permanence: yellow cube compared to Object permanence: black cube. These tasks are identical except for the goal being to navigate to a yellow cube on the right, or a black cube on the left respectively. This suggests that the agent may have acquired a bias during training to either prefer black cubes or to navigate towards objects on its left. The significantly higher prompted scores on Object

![](_page_62_Figure_1.jpeg)

Figure F.5 | Performance of AdA on 6 hand-authored tasks when prompted with an expert first-person human demonstration, compared with an unprompted baseline.

![](_page_62_Picture_3.jpeg)

Figure F.6 | Top-down views depicting the behaviour of AdA with and without a human expert prompt on the task Object permanence: yellow cube. On its own, AdA appears to have a bias of navigating to the black cube which is a dead end in this task. When prompted with a human (or fine-tuned) expert trajectory, AdA is able to overcome this bias and navigate to the yellow cube in the second trial.

permanence: yellow cube for small suggest that a prompt may help the agent overcome these biases.

**Prompting with human demonstrations.** We prompted AdA with expert human demonstrations in a small selection of 6 hand-authored tasks, depicted in Figure F.5. These tasks were chosen to be a mixture of tasks where AdA excelled with fine-tuned teacher prompting, where it failed with fine-tuned teacher prompting and where even the fine-tuned teacher failed.

The results show the same pattern as those obtained when prompting with a fine-tuned teacher. In both Navigation: hold up high and Object permanence: yellow cube, prompted AdA achieved close to optimal performance, exceeding both the baseline and the human demonstration. Figure F.6 depicts the latter behaviour in detail. AdA continues to fail to learn from a demonstration in Navigation: find the cube with teaser and in both Spacer tool and Transform to transport, which our fine-tuned teacher also failed at. A successful human demonstration did not unlock any capabilities AdA was previously not capable of demonstrating, suggesting that these tasks are perhaps too far out-of-distribution with respect to AdA's training tasks. The fact that prompting with off-policy human demonstrations is partially successful is worthy of note, and opens an exciting area of future research.
