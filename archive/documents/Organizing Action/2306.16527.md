---
title: 2306.16527
author: R. Beaumont
slug: 2306.16527
reconversion_status: ready_for_reconversion
---
# OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents

$\begin{array}{cccccccccccccccccccccccccccccccccccc$ 

hugo@huggingface.co

<sup>1</sup>Hugging Face <sup>2</sup>Sorbonne Université <sup>3</sup>Stanford University

#### Abstract

Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELISC dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself<sup>1</sup>.

#### 1 Introduction

Recent systems demonstrate the effectiveness of training large multimodal models such as Flamingo on naturally occurring multimodal documents (Alayrac et al., 2022; Aghajanyan et al., 2022; Huang et al., 2023). A multimodal document is a succession of text paragraphs interleaved by images, such as web pages that contain images. Models trained on these web documents outperform vision and language models trained solely on image-text pairs on various benchmarks. They can also generate long and coherent text about a set of multiple images.

While these results are compelling, they have not been replicable. The datasets used in these works are not publicly available, and relatively little information is known about their creation process and composition. This state motivates the creation of large-scale collections of high-quality multimodal web documents to support the creation of the next generation of models.

We take inspiration from existing large open image-text datasets such as LAION (Schuhmann et al., 2022) and COYO (Byeon et al., 2022), comprised of hundreds of millions of image-text

$_1 \rm Dataset:$ https://huggingface.co/datasets/Hugging FaceM4/0BELISCReproduction code: https://github.com/huggingface/0BELISC

Figure 1: A comparison of extraction from the same web document. For image-text pairs, the alt-text of images is often short or non-grammatical. For OBELISC, the extracted multimodal web document interleaves long-form text with the images on the page.

pairs obtained through web crawling. These datasets have been critical to developing and replicating numerous recent multimodal models (Radford et al., 2021; Wang et al., 2022; Yu et al., 2022; Wang et al., 2022; Liu et al., 2023). While this approach allows for building extremely large and diverse training datasets, we note several limitations to using only image-text pairs. From a language perspective, these datasets rely primarily on alt-text, meaning the text given is brief, captures an approximate snapshot of the image's content, and often lacks grammatical correctness. From a document perspective, image-text pairs remove an image from its natural context on a page and its relationship with other documents.

In this work, we introduce OBELISC<sup>2</sup>, an openly-accessible curated web-scale dataset consisting of 141 million multimodal English web documents which contain 353 million associated images and 115 billion tokens. OBELISC collects full multimodal documents interleaving text and images as shown in Figure 1. We describe the dataset creation process, outline the ltering and curation steps and shed light on the dataset's content and limitations. To demonstrate the viability of OBELISC, we train an 80 billion parameter multimodal model and show competitive performance against large-scale multimodal models such as Flamingo (Alayrac et al., 2022).

## 2 Related Works

Image-text pairs datasets The largest multimodal datasets, such as LAION (Schuhmann et al., 2021, 2022), Conceptual Captions (Sharma et al., 2018; Changpinyo et al., 2021), ALIGN (Jia et al., 2021), COYO (Byeon et al., 2022), and Data Comp (Gadre et al., 2023), contain billions of image-text pairs and are usually obtained through web-crawling and alt-text extraction. A variety of multimodal models have been trained on this type of dataset: multimodal encoder models which use a contrastive objective (Radford et al., 2021; Wang et al., 2022), image generation based on Transformers or diusion processes (Nichol et al., 2022; Ramesh et al., 2022; Rombach et al., 2021; Saharia et al., 2022). While the scale of these datasets makes them attractive candidates for training, our work focuses on extracting images and the textual context in which they appear instead of extracting the associated alternative text.

Web document datasets Insights from scaling language models (Kaplan et al., 2020; Homann et al., 2022) emphasize the need for increasingly bigger datasets. For instance,

<sup>2</sup>Open Bimodal Examples from Large f Iltered Snapshots of Commoncrawl

LLaMA (Touvron et al., 2023) was trained on a dataset of 1.4T tokens created exclusively from openly accessible English web content. The authors noticed that an even bigger dataset would have benefited the model. To address that need, multiple web-scale datasets have been introduced and made available: c4 (Raffel et al., 2019), ROOTS (Laurençon et al., 2022), Pile (Gao et al., 2020), OSCAR (Ortiz Suárez et al., 2020). Although OBELISC falls in the same category of making accessible large collections of curated web documents, the additional extraction of images changes the nature of the resulting dataset. It allows training models with additional vision capabilities.

Multimodal web document datasets The recent most performant vision and language models are trained on large sets of multimodal web documents. For instance, Flamingo (Alayrac et al., 2022), an 80 billion multimodal model, was trained on a mix of 2.1 billion image-text pairs, 27 million video-text pairs, and 43 million multimodal web documents. The latter called M3W, includes 185 million images. Similarly, KOSMOS-1 (Huang et al., 2023) was trained on a mixture containing 71 million multimodal web documents. However, in both cases, the dataset is not publicly available, and little information is accessible as to the dataset's content, the strategies employed to create that dataset (including filtering strategies), and the quality of the resulting web documents, which ultimately hinders further research.

Concurrently to our work, the Multimodal C4 dataset (Zhu et al., 2023) was recently made accessible. It consists of 103 million multimodal web documents that include 585 million images. Although there are similarities between our datasets, it is important to highlight particular distinctions. First, our dataset is based on more recent documents from February 2020 to February 2023, whereas mmc4 uses documents from April 2019. Additionally, our filtering heuristics appear to be more comprehensive: we leverage the HTML DOM trees to filter out undesirable texts and images, whereas mmc4 uses the HTML to find images in order to merge them with the original C4 dataset by solving a bipartite assignment problem based on a CLIP model similarities. Last, we implement additional deduplication steps at the image, document, and paragraph levels.

## 3 Creation of the Multimodal Web Document Dataset

## ![](_page_2_Figure_4.jpeg)

Figure 2: Overview of the steps involved in creating OBELISC.

This section provides an overview of the critical choices of the creation and filtering process. Figure 2 gives a high-level summary of the main steps involved. Many details are omitted from this section, and we invite the reader to refer to the appendix A.1 for completeness.

#### 3.1 Collecting a Large Number of HTML Files

First, we collect a vast amount of raw web documents by considering the 25 most recent Common Crawl dumps at the time of the creation, spanning from February 2020 to January/February 2023<sup>3</sup>. We extract the main text from the documents while discarding documents with text of insucient quality. This process results in 41.2 billion documents.

To lter out non-English content, we apply the Fast Text classier (Joulin et al., 2017) to the extracted text, which removes 63.6% of the documents. We perform a Min Hash (Broder, 1997) deduplication to remove duplicate content. Additionally, we lter out documents with signicant proportions of repeated paragraphs and n-grams, following the methodology used in Massive Text (Rae et al., 2022). Previous studies (Lee et al., 2022; Abbas et al., 2023) have demonstrated the prevalence of duplication in crawled data and the benets of training on deduplicated data.

Similar to Brown et al. (2020), we employ a logistic regression classier with hashed token frequencies to ensure high-quality text. This classier, trained using curated datasets like Wikipedia or Open Web Text (Gokaslan and Cohen, 2019) as positive examples and documents sampled from Common Crawl as negative ones, is fast and eective at detecting humanwritten text. After these steps, we are left with 1.1 billion documents and their HTML sources from the associated Common Crawl WARC les.

#### 3.2 Simplifying HTML Files

The original HTML content of a document contains a wealth of valuable information that proves highly benecial in the process of ltering out undesirable text and images. Therefore, we prioritize pre-processing the raw HTML into simplied HTML, making the subsequent extraction of textual and visual elements more ecient.

To this aim, we devise multiple pre-processing strategies for an HTML DOM tree. By manually inspecting instances of all HTML nodes, we dierentiate nodes likely to contain relevant texts or images from those that should be discarded, and we formulate specic rules for each type of node. After these pre-processing steps, the resulting simplied HTML les are more than ten times smaller and have been stripped of a large proportion of generic text (spam, ads, boilerplate template, etc.) and generic images, such as logos, while retaining the relevant content.

#### 3.3 Extracting Multimodal Web Documents

In this step, we transform the simplied HTML les previously obtained into a structured web multimodal web document format. This format consists of interleaved texts and images.

We meticulously preserve the original structure of the web pages from the simplied HTML les by extracting the texts and image links while maintaining their rendering dened by the DOM tree. Given that each HTML tag denotes a distinct separation between the preceding and subsequent nodes, we leverage that information to retain line breaks and line feeds on the original page, preserving the formatting and visual rendering of the content.

We obtain 3.6 billion image links and successfully download 55% of them (approximately 2 billion images).

#### 3.4 Filtering Multimodal Web Documents

The ltering process comprises two distinct steps operating at dierent granularity levels. In the rst step, ltering occurs at the node level for images and the paragraph level for text. This step guarantees that only high-quality and relevant images and paragraphs are retained. Each paragraph or image is evaluated based on specic criteria and may undergo modications or be eliminated if necessary. The second step, conducted at the document level, involves deciding whether to retain or discard the output documents obtained from the

<sup>3</sup>https://commoncrawl.org/

rst step. Most text lters used in both steps are primarily derived from Laurençon et al. (2022).

Node-level image ltering We discard images that are too small, excessively large or have disproportionate dimensions. We observe that these images are often indicative of low-quality or irrelevant content. To eliminate some logos and generic images, we remove images whose URLs contain one of the banned sub-strings, like logo.

Paragraph-level text ltering We apply multiple lters to text paragraphs to remove undesirable content. Specically, paragraphs that contain an insucient number of words are discarded. Additionally, we lter out paragraphs with high repetition ratios, excessive ratios of special characters, low ratios of stop words, low punctuation ratios, high proportions of agged words associated with adult or inappropriate content, or excessively high perplexity scores (as measured by an n-gram language model trained on Wikipedia (Heaeld, 2011)). To identify boilerplate sentences or invitations to share articles on social networks, we create a list of frequently used words associated with these paragraphs and remove paragraphs containing an excessive proportion of words from this list. To further identify machinegenerated content, we extract words from web-crawled documents to form a list of common words and discard documents with a low ratio of common words.

Document-level ltering At the document level, we remove all documents with no or excessively high number of images. For text lters, the same lters used at the paragraph level are applied, with sometimes stricter cuto values.

After these ltering steps, we are left with 365 million web documents and 1.4 billion images. At this step, images can be duplicated across documents.

#### 3.5 Responsible Filtering and Deduplication

We take measures to minimize the amount of inappropriate content in the dataset. In particular, based on manual inspections and tool availability, we implement lters to respect data consent and remove images with pornographic content. Additionally, we also heavily deduplicate content.

Exclusion of opted-out images To respect the preferences of content creators, we remove all images for which creators explicitly opted out of AI model training. We used the Spawning API<sup>4</sup> to verify that the images in the dataset respect the original copyright owners' choices.

Image deduplication based on URL Some images could be present across dierent documents. We observe that it is particularly true for browser-specic icons or common advertisements encountered during the crawling process. To address this issue, we remove all images that appear more than ten times across the entire dataset. We intentionally do not perform strict deduplication, as we notice that when an image is duplicated only a few times across dierent documents, the surrounding text and contextual information tend to be dierent. We also deduplicate images within the same document.

NSFW image ltering To reduce explicit adult content, we use an open-source NSFW classier to remove entire documents containing pornographically classied images. We also lter out images with URLs containing banned sub-strings.

Document deduplication based on URL and set of images We complete the initial deduplication step by forming clusters of documents with the same URLs and retaining the most recent document within each cluster. We repeat this operation by forming clusters of documents containing identical sets of images.

Paragraph deduplication across documents of the same domain names To remove generic spam phrases commonly found at the end of documents, we perform paragraph-level

<sup>4</sup>https://api.spawning.ai/spawning-api exact deduplication within documents sharing the same domain name, resulting in the elimination of approximately 15\% of the text.

Following these filtering and deduplication steps, the final dataset contains 141 million documents and 353 million images, of which 298 million are unique. We observe that using stricter values for the filtering steps yields fewer multimodal documents, although not of higher quality. As such, we invite users who are interested in manipulating a smaller subset of OBELISC to start with a random subset.

#### Analysis of OBELISC

Figure 1 provides an example showcasing an original webpage alongside the resulting multimodal web document. Extracting and filtering the multimodal document is non-trivial as it requires carefully removing undesirable information on the left, top, and bottom of the page, such as menus and navigation bars. We provide other examples at https: //huggingface.co/spaces/Hugging FaceM4/obelisc\_visualization and in Figures 7, 8 and 9.

Given the scale of OBELISC, it would be prohibitive to describe its content exhaustively. Instead, we provide high-level statistics and analyses that shed light on the dataset's properties.

#### 4.1 General Statistics

| Dataset | Images | %<br>unique<br>images | Docs | Tokens | Open | images to a doc $t x$ images | 100 80 |
|----------|--------|-----------------------|------|--------|------|------------------------------|---------------|
| KOSMOS-1 | - | - | 71M | - | X | | 60 |
| MSW | 185M | - | 43M | - | X | % of<br>ging<br>mos | 40 |
| mmc4-ff | 385M | 60.6% | 79M | 34B | ✓ | = <sup>c</sup><br>ong<br>at | $\sim$ mmc4 |
| mmc4 | 585M | - | 103M | 43B | ✓ | y = y belcith $z$ | 20 OBELI |
| OBELISC | 353M | <b>84.3</b> % | 141M | 115B | ✓ | b<br>wit | 0 20 40 60 80 |

Table 1: General statistics of OBELISC and the current largest alternatives.

Figure 3: Distribution of images.

max # of images in doc

Table 1 compares OBELISC against the largest existing alternatives. Our dataset has the highest number of unique documents and total tokens while containing a huge number of images.

It is worth mentioning that we have fewer images than mmc4 (Zhu et al., 2023). This discrepancy can be attributed to two reasons. First, our analysis reveals that mmc4 contains many duplicated images, with only 60.6% being unique compared to 84.3% for OBELISC. We found that images duplicated multiple times often indicate spam or unrelated generic content. Second, mmc4 does not limit the number of images within a document. As a result, the distribution of images across documents is highly uneven, with a substantial portion of them concentrated in documents with excessive image counts (see Figure 3). The images in these documents are often unrelated to each other and exhibit spam or advertisement content. Moreover, these documents often have little text, making them unsuitable for learning the alignment between text and images (see an example in Figure 10).

Figure 5 shows the joint distribution of a number of tokens and a number of images in OBELISC. Although we limit the number of images in a document to 30, we cut the plot at 6 images for clarity. The documents of OBELISC contain a median number of images of 1 and a median number of tokens of 677.

Perplexity analysis To assess the quality of our text in comparison to reference datasets used for training large language models, we leverage an n-gram language model trained on Wikipedia (Heafield, 2011; Laurençon et al., 2022). This allows us to compute perplexity

## ![](_page_6_Figure_0.jpeg)

Figure 4: Kernel density estimations representing the distribution of perplexity scores for OBELISC compared to reference datasets. The lower the perplexity for a document, the more it resembles a Wikipedia article.

## ![](_page_6_Figure_2.jpeg)

Figure 5: Heatmap displaying the joint distribution of the number of tokens and the number of images in OBELISC documents, accompanied by their respective marginal distributions.

scores for 100,000 documents from each dataset. Lower perplexity scores indicate a higher resemblance to Wikipedia documents. Figure 4 displays the distributions of these scores. Our results demonstrate that the texts in OBELISC have a signicantly lower average perplexity compared to the texts in c4 (Rael et al., 2019), mmc4 (Zhu et al., 2023), and OSCAR (Ortiz Suárez et al., 2020). Furthermore, our distribution aligns closely with the one from The Pile (Gao et al., 2020), which was thoughtfully curated from diverse, high-quality sources.

#### 4.2 Topic Modeling

Similar to Zhu et al. (2023), we employ a Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to understand the diversity of the dataset. The LDA gives us insights into the distribution of topics in the dataset, along with estimated proportions and frequently associated words. Table 5 and 6 present the results of the LDA with respectively 20 and 200 topics, oering both a high-level and a more granular analysis of the dataset's content. We observe that the dataset covers topics ranging from Politics to Health by way of Music. Additionally, we compute the most frequent domains and show that news sites are systematically the most represented (Table 4).

#### 4.3 Qualitative Assessment of Dataset Samples

We manually inspect 250 documents from OBELISC to verify the dataset's quality and asses the risks contained in the dataset. We focus on the images' content in relation to the text since it's the core addition compared to a language modeling dataset.

80% of documents have photo images, while 29% have graphic images (drawings, cartoons, etc.). 90% of the documents have all images clearly related to the text content. 30% of documents have images containing at least one written word, and 5% of documents have images that are structured text (slides, tables, scanned documents, etc.), which can help models learn OCR capabilities. 7% of documents have content (images or text) that hasn't been captured by cleaning lters (non-English text, spam or advertisement, etc.). 46% of documents contain images with faces (portraits or group photos). No obvious Personally Identiable Information (PII) texts were found, except for public personalities and people mentioned in news articles. No NSFW images were found. Only 3% of documents contain images with watermarks, and 2% have images with logos.

## 5 Validating the Viability of OBELISC

To conrm the viability of our dataset, we rst show that vision and language models trained on our multimodal web documents outperform the same models trained on image-text pairs on various multimodal benchmarks. Following that, we demonstrate the eectiveness of OBELISC as an alternative to closed datasets by training a model on par with closed-source models.

Model details We follow the Flamingo (Alayrac et al., 2022) architecture closely: we combine two frozen unimodal backbones - LLaMA (Touvron et al., 2023) for the language model, and Open Clip <sup>5</sup> for the vision encoder - add learnable cross-attention Transformer blocks to connect the language and vision blocks. For multimodal web documents, we feed the model sequences corresponding to the succession of text paragraphs and images. For imagetext pairs, we form the training sequences by packing images with their captions. The images are encoded with the vision encoder and vision hidden states are pooled with Transformer Perceiver blocks and then fused into the text sequence through the cross-attention blocks. The training objective is the standard next token prediction. For more details, we refer to the original paper. Following Alayrac et al. (2022), we evaluate our models on a series of multimodal benchmarks spanning visual question answering (VQAv2 (Antol et al., 2015), OKVQA (Marino et al., 2019), TextVQA (Singh et al., 2019)), hateful speech detection (Hateful Meme (Kiela et al., 2020)), image captioning (COCO (Lin et al., 2014), Flickr30k (Young et al., 2014)), and OCR (IIIT5k (Mishra et al., 2012)). Additional details about the architecture, the training, the compute and the evaluation are present in Appendix A.3.

## ![](_page_7_Figure_2.jpeg)

Figure 6: Aggregated 4-shot performance through the training using LAION only, OBELISC only and a mixture of both. The training sequences from multimodal documents and the packed sequences obtained from image-text pairs have dierent numbers of images but the same number of tokens. Thus, we plot the performance over two log x-axes. The initial uptick of the model trained on image-text pairs is attributed to the fact the performance on VQA tasks starts by increasing and then slowly degrades.

Training on dierent mixture of data Figure 6 shows the result of the rst experiment, which consists in training 9B-parameter models on dierent mixture of data. Training on multimodal web documents allows reaching the same performance using an order of magnitude fewer images than training on image-text pairs, even though the images from the two datasets come from Common Crawl. This underlines the benet of having longer text contexts for training multimodal models. Moreover, the model trained on multimodal web documents performs better on average. This is particularly striking on visual questionanswering benchmarks on which the model trained on image-text pairs slowly degrades through the training. We note, however, that the model trained on image-text pairs has a slight advantage performance-wise in captioning, classication, and OCR tasks (see more details in Appendix A.3.5). We hypothesize that this is due to the nature of image-text pairs: captions can be seen as fuzzy class labels. Last, similarly to Alayrac et al. (2022), we observe that combining the two types of datasets leads to increased performance for a given number of images, tokens, or training compute.

Models trained on OBELISC achieve competitive performance at dierent scales Following these insights, we show that OBELISC is a viable open alternative to closed datasets. We train an 80 billion parameters Flamingo-like model on a mixture of image-text pairs from LAION (Schuhmann et al., 2022), openly accessible captioning datasets (Singh et al., 2022), OBELISC and multimodal web documents obtained from Wikipedia using a similar extraction strategy. We compare this model against a reported Flamingo of the same size and trained

<sup>5</sup>https://laion.ai/blog/large-openclip/

| Task<br>Metric<br>Query split | VQAv2<br>VQA acc.<br>testdev | OKVQA<br>VQA acc.<br>val | TextVQA<br>VQA acc.<br>val | Coco<br>CIDEr<br>test | Hateful Memes<br>ROC-AUC<br>test seen | Flickr30k<br>CIDEr<br>test (Karpathy) |
|-------------------------------|------------------------------|--------------------------|----------------------------|-----------------------|--------------------------------------|---------------------------------------|
| Ours<br>4 shots<br>Flamingo | 63.8<br>63.1 | 54.1<br>57.4 | 34.6<br>36.5 | 106.5<br>103.2 | 59.1<br>68.6 | 70.3<br>75.1 |
| Ours<br>8 shots<br>Flamingo | 64.7<br>65.6 | 56.3<br>57.5 | 35.4<br>37.3 | 112.0<br>108.8 | 59.5<br>70 | 72.8<br>78.2 |

Table 2: The model we train on a mixture containing OBELISC is on par with Flamingo 80B. We evaluate our model using in-context learning with 4 and 8 priming shots. Note that Flamingo 80B leverages prompt ensembling to boost performances.

on a similar mixture of multimodal web documents and image-text pairs. Table 2 shows that our model<sup>6</sup> is on par with Flamingo on various multimodal benchmarks. We hypothesize that the minor performance lag is mainly explained by using closed-source image-text pairs, access to closed-source unimodal backbones, and prompt ensembling to boost performance.

Additionally, we note that the model trained in Figure 6 on the mixture of OBELISC and imagetext pairs from LAION is comparable in size (9 billion parameters) to Open Flamingo7, a model trained on mmc4 and the same image-text pairs. Mid-training, after seeing approximately 32M images, they achieved 74.3 CIDEr on COCO in 4 shots, while we reach a score of 83.1. Similarly, they achieved 44.0 VQA accuracy on VQAv2, and we reach a score of 52.3. This highlights the advantages of OBELISC as an open alternative to a multimodal web document dataset.

## 6 Conclusion

With the goal of supporting open-source large multimodal models, we introduce OBELISC, an open web-scale collection of ltered interleaved multimodal web documents based on Common Crawl snapshots. We document a collection and ltering process that balances the scale and removal of undesirable texts and images while addressing some of the well-documented ethical concerns of large-scale multimodal datasets, notably data consent and pornographic content. To demonstrate the usefulness of models trained on multimodal documents, we train a model on OBELISC and show that it is a viable alternative to closed datasets. Open datasets of multimodal documents with scale, quality, and diversity of sources can help support the ability to train competitive open models.

<sup>6</sup>Our model and smaller versions will be released soon when they are fully trained. Thus, the performance of the 80B-parameter model is likely to slightly change, and we will report a complete evaluation of this model and the smaller ones in an updated version of this paper.

<sup>7</sup>https://laion.ai/blog/open-flamingo/

## Acknowledgments and Disclosure of Funding

The authors were granted access to the HPC resources of the Institut du développement et des ressources en informatique scientique (IDRIS) du Centre national de la recherche scientique (CNRS) under the allocation 2022-A0121013450 made by Grand équipement national de calcul intensif (GENCI). The initial development of the dataset was done on Jean-Zay cluster of IDRIS, and we thank the IDRIS team for their responsive support throughout the project, in particular Rémi Lacroix. We thank Guillaume Salou for setting up the virtual machines used to download the images of our dataset, and Sebastian Nagel for his valuable assistance in providing insights on Common Crawl.

## References

- Abbas, A., K. Tirumala, D. Simig, S. Ganguli, and A. S. Morcos (2023). Semdedup: Data-ecient learning at web-scale through semantic deduplication.
- Aghajanyan, A., B. Huang, C. Ross, V. Karpukhin, H. Xu, N. Goyal, D. Okhonko, M. Joshi, G. Ghosh, M. Lewis, and L. Zettlemoyer (2022). Cm3: A causal masked multimodal model of the internet. Ar Xiv abs/2201.07520.
- Alayrac, J.-B., J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. a. Bińkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan (2022). Flamingo: a visual language model for few-shot learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35, pp. 23716–23736. Curran Associates, Inc.
- Antol, S., A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh (2015). VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV).
- Bai, Y., A. Jones, K. Ndousse, A. Askell, A. Chen, N. Das Sarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hateld-Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. Mc Candlish, C. Olah, B. Mann, and J. Kaplan (2022). Training a helpful and harmless assistant with reinforcement learning from human feedback.
- Beaumont, R. (2021). img2dataset: Easily turn large sets of image urls to an image dataset. https://github.com/rom1504/img2dataset.
- Biderman, S., H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Ra, A. Skowron, L. Sutawika, and O. van der Wal (2023). Pythia: A suite for analyzing large language models across training and scaling.
- Blei, D. M., A. Y. Ng, and M. I. Jordan (2003, mar). Latent dirichlet allocation. J. Mach. Learn. Res. 3 (null), 993–1022.
- Broder, A. (1997). On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), pp. 21–29.
- Brown, T., B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. Mc Candlish, A. Radford, I. Sutskever, and D. Amodei (2020). Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural Information Processing Systems, Volume 33, pp. 1877–1901. Curran Associates, Inc.
- Byeon, M., B. Park, H. Kim, S. Lee, W. Baek, and S. Kim (2022). Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset.

- Changpinyo, S., P. Sharma, N. Ding, and R. Soricut (2021). Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR.
- Chowdhery, A., S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel (2022). Palm: Scaling language modeling with pathways.
- Dehghani, M., J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, R. Jenatton, L. Beyer, M. Tschannen, A. Arnab, X. Wang, C. Riquelme, M. Minderer, J. Puigcerver, U. Evci, M. Kumar, S. van Steenkiste, G. F. Elsayed, A. Mahendran, F. Yu, A. Oliver, F. Huot, J. Bastings, M. P. Collier, A. Gritsenko, V. Birodkar, C. Vasconcelos, Y. Tay, T. Mensink, A. Kolesnikov, F. Pavetić, D. Tran, T. Kipf, M. Lučić, X. Zhai, D. Keysers, J. Harmsen, and N. Houlsby (2023). Scaling vision transformers to 22 billion parameters.
- Deng, X., P. Shiralkar, C. Lockard, B. Huang, and H. Sun (2022). Dom-lm: Learning generalizable representations for html documents. Ar Xiv abs/2201.10608.
- Desai, K., G. Kaul, Z. Aysola, and J. Johnson (2021). Redcaps: Web-curated image-text data created by the people, for the people. In J. Vanschoren and S. Yeung (Eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, Volume 1. Curran.
- Gadre, S. Y., G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, E. Orgad, R. Entezari, G. Daras, S. Pratt, V. Ramanujan, Y. Bitton, K. Marathe, S. Mussmann, R. Vencu, M. Cherti, R. Krishna, P. W. Koh, O. Saukh, A. Ratner, S. Song, H. Hajishirzi, A. Farhadi, R. Beaumont, S. Oh, A. Dimakis, J. Jitsev, Y. Carmon, V. Shankar, and L. Schmidt (2023). Datacomp: In search of the next generation of multimodal datasets. ar Xiv preprint ar Xiv:2304.14108 .
- Gao, L., S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy (2020). The Pile: An 800gb dataset of diverse text for language modeling. ar Xiv preprint ar Xiv:2101.00027 .
- Gokaslan, A. and V. Cohen (2019). Openwebtext corpus. http://Skylion007.github.io/Open Web Text Corpus.
- Gu, J., X. Meng, G. Lu, L. Hou, N. Minzhe, X. Liang, L. Yao, R. Huang, W. Zhang, X. Jiang, C. XU, and H. Xu (2022). Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35, pp. 26418–26431. Curran Associates, Inc.
- Heaeld, K. (2011, July). KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, pp. 187– 197. Association for Computational Linguistics.
- Homann, J., S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre (2022). Training compute-optimal large language models.
- Huang, S., L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and F. Wei (2023). Language is not all you need: Aligning perception with language models.

- Jaegle, A., F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira (2021). Perceiver: General perception with iterative attention.
- Jia, C., Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y.-H. Sung, Z. Li, and T. Duerig (2021). Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning.
- Jiang, A. Q., S. Welleck, J. P. Zhou, T. Lacroix, J. Liu, W. Li, M. Jamnik, G. Lample, and Y. Wu (2023). Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. In The Eleventh International Conference on Learning Representations.
- Joulin, A., E. Grave, P. Bojanowski, and T. Mikolov (2017, April). Bag of tricks for ecient text classication. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, Valencia, Spain, pp. 427–431. Association for Computational Linguistics.
- Kaplan, J., S. Mc Candlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei (2020). Scaling laws for neural language models.
- Kiela, D., H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, and D. Testuggine (2020). The hateful memes challenge: Detecting hate speech in multimodal memes. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural Information Processing Systems, Volume 33, pp. 2611–2624. Curran Associates, Inc.
- Koh, J. Y., R. Salakhutdinov, and D. Fried (2023). Grounding language models to images for multimodal generation.
- Laborde, G. Deep nn for nsfw detection.
- Laurençon, H., L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral, T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen, J. Frohberg, M. ŁaŽko, Q. Lhoest, A. Mc Millan-Major, G. Dupont, S. Biderman, A. Rogers, L. Ben allal, F. De Toni, G. Pistilli, O. Nguyen, S. Nikpoor, M. Masoud, P. Colombo, J. de la Rosa, P. Villegas, T. Thrush, S. Longpre, S. Nagel, L. Weber, M. Muijoz, J. Zhu, D. Van Strien, Z. Alyafeai, K. Almubarak, M. C. Vu, I. Gonzalez-Dios, A. Soroa, K. Lo, M. Dey, P. Ortiz Suarez, A. Gokaslan, S. Bose, D. Adelani, L. Phan, H. Tran, I. Yu, S. Pai, J. Chim, V. Lepercq, S. Ilic, M. Mitchell, S. A. Luccioni, and Y. Jernite (2022). The bigscience roots corpus: A 1.6tb composite multilingual dataset. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35, pp. 31809–31826. Curran Associates, Inc.
- Lee, K., D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini (2022). Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.
- Li, J., D. Li, S. Savarese, and S. Hoi (2023). Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models.
- Li, J., D. Li, C. Xiong, and S. Hoi (2022). Blip: Bootstrapping language-image pre-training for unied vision-language understanding and generation. In ICML.
- Li, R., L. B. Allal, Y. Zi, N. Muennigho, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M.-H. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. Murthy, J. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. von Werra, and H. de Vries (2023). Starcoder: may the source be with you!

- Lin, T.-Y., M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollár (2014). Microsoft coco: Common objects in context. cite arxiv:1405.0312Comment: 1) updated annotation pipeline description and gures; 2) added new section describing datasets splits; 3) updated author list.
- Liu, S., L. Fan, E. Johns, Z. Yu, C. Xiao, and A. Anandkumar (2023). Prismer: A vision-language model with an ensemble of experts. ar Xiv preprint ar Xiv:2303.02506 .
- Liu, Y., G. Zhu, B. Zhu, Q. Song, G. Ge, H. Chen, G. Qiao, R. Peng, L. Wu, and J. Wang (2022). Taisu: A 166m large-scale high-quality dataset for chinese vision-language pretraining. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35, pp. 16705–16717. Curran Associates, Inc.
- Loshchilov, I. and F. Hutter (2017). Fixing weight decay regularization in adam. CoRR abs/1711.05101.
- Marino, K., M. Rastegari, A. Farhadi, and R. Mottaghi (2019). Ok-vqa: A visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR).
- Mishra, A., K. Alahari, and C. V. Jawahar (2012). Scene text recognition using higher order language priors. In BMVC.
- Nichol, A., P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mc Grew, I. Sutskever, and M. Chen (2022). Glide: Towards photorealistic image generation and editing with text-guided diusion models.
- Ortiz Suárez, P. J., L. Romary, and B. Sagot (2020, July). A monolingual approach to contextualized word embeddings for mid-resource languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online, pp. 1703–1714. Association for Computational Linguistics.
- Ouyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe (2022). Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35, pp. 27730–27744. Curran Associates, Inc.
- Piktus, A., C. Akiki, P. Villegas, H. Laurençon, G. Dupont, A. S. Luccioni, Y. Jernite, and A. Rogers (2023). The roots search tool: Data transparency for llms.
- Radenovic, F., A. Dubey, A. Kadian, T. Mihaylov, S. Vandenhende, Y. Patel, Y. Wen, V. Ramanathan, and D. Mahajan (2023). Filtering, distillation, and hard negatives for vision-language pre-training.
- Radford, A., J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning.
- Rae, J. W., S. Borgeaud, T. Cai, K. Millican, J. Homann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell,
 - G. van den Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl,
 - S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. Mc Aleese,
- A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan,
- M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya,
- D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz,
- T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d'Autume,
- Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones,
- J. Bradbury, M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart,
- S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis,

- K. Kavukcuoglu, and G. Irving (2022). Scaling language models: Methods, analysis & insights from training gopher.
- Rael, C., N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2019). Exploring the limits of transfer learning with a unied text-to-text transformer. ar Xiv e-prints.
- Ramesh, A., P. Dhariwal, A. Nichol, C. Chu, and M. Chen (2022). Hierarchical textconditional image generation with clip latents.
- Rombach, R., A. Blattmann, D. Lorenz, P. Esser, and B. Ommer (2021). High-resolution image synthesis with latent diusion models.
- Saharia, C., W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi (2022). Photorealistic text-to-image diusion models with deep language understanding.
- Schuhmann, C., R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev (2022). Laion-5b: An open large-scale dataset for training next generation image-text models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35, pp. 25278–25294. Curran Associates, Inc.
- Schuhmann, C., R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki (2021). Laion-400m: Open dataset of clip-ltered 400 million image-text pairs.
- Sharma, P., N. Ding, S. Goodman, and R. Soricut (2018). Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL.
- Singh, A., R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, and D. Kiela (2022). FLAVA: A foundational language and vision alignment model. In CVPR.
- Singh, A., V. Natarjan, M. Shah, Y. Jiang, X. Chen, D. Parikh, and M. Rohrbach (2019). Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8317–8326.
- Sorscher, B., R. Geirhos, S. Shekhar, S. Ganguli, and A. Morcos (2022). Beyond neural scaling laws: beating power law scaling via data pruning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35, pp. 19523–19536. Curran Associates, Inc.
- Srinivasan, K., K. Raman, J. Chen, M. Bendersky, and M. Najork (2021). Wit: Wikipediabased image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21, New York, NY, USA, pp. 2443–2449. Association for Computing Machinery.
- Team, M. N. (2023). Introducing mpt-7b: A new standard for open-source, commercially usable llms.
- Touvron, H., T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample (2023). Llama: Open and ecient foundation language models.
- Wang, P., A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang (2022, 17–23 Jul). OFA: Unifying architectures, tasks, and modalities through a simple sequenceto-sequence learning framework. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato (Eds.), Proceedings of the 39th International Conference on Machine Learning, Volume 162 of Proceedings of Machine Learning Research, pp. 23318–23340. PMLR.

Wang, Q., Y. Fang, A. Ravula, F. Feng, X. Quan, and D. Liu (2022). Webformer: The web-page transformer for structure information extraction.

Wang, W., H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, and F. Wei (2022). Image as a foreign language: Beit pretraining for all vision and vision-language tasks.

Webster, R., J. Rabin, L. Simon, and F. Jurie (2023). On the de-duplication of laion-2b.

Workshop, B., :, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennigho, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. Mc Millan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurençon, Y. Jernite, J. Launay, M. Mitchell, C. Rael, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, D. Radev, E. G. Ponferrada, E. Levkovizh, E. Kim, E. B. Natan, F. D. Toni, G. Dupont, G. Kruszewski, G. Pistilli, H. Elsahar, H. Benyamina, H. Tran, I. Yu, I. Abdulmumin, I. Johnson, I. Gonzalez-Dios, J. de la Rosa, J. Chim, J. Dodge, J. Zhu, J. Chang, J. Frohberg, J. Tobing, J. Bhattacharjee, K. Almubarak, K. Chen, K. Lo, L. V. Werra, L. Weber, L. Phan, L. B. allal, L. Tanguy, M. Dey, M. R. Muijoz, M. Masoud, M. Grandury, M. ŁaŽko, M. Huang, M. Coavoux, M. Singh, M. T.-J. Jiang, M. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner, N. Khamis, O. Nguyen, O. Espejel, O. de Gibert, P. Villegas, P. Henderson, P. Colombo, P. Amuok, Q. Lhoest, R. Harliman, R. Bommasani, R. L. López, R. Ribeiro, S. Osei, S. Pyysalo, S. Nagel, S. Bose, S. H. Muhammad, S. Sharma, S. Longpre, S. Nikpoor, S. Silberberg, S. Pai, S. Zink, T. T. Torrent, T. Schick, T. Thrush, V. Danchev, V. Nikoulina, V. Laippala, V. Lepercq, V. Prabhu, Z. Alyafeai, Z. Talat, A. Raja, B. Heinzerling, C. Si, D. E. Taşar, E. Salesky, S. J. Mielke, W. Y. Lee, A. Sharma, A. Santilli, A. Chan, A. Stiegler, D. Datta, E. Szczechla, G. Chhablani, H. Wang, H. Pandey, H. Strobelt, J. A. Fries, J. Rozen, L. Gao, L. Sutawika, M. S. Bari, M. S. Al-shaibani, M. Manica, N. Nayak, R. Teehan, S. Albanie, S. Shen, S. Ben-David, S. H. Bach, T. Kim, T. Bers, T. Fevry, T. Neeraj, U. Thakker, V. Raunak, X. Tang, Z.-X. Yong, Z. Sun, S. Brody, Y. Uri, H. Tojarieh, A. Roberts, H. W. Chung, J. Tae, J. Phang, O. Press, C. Li, D. Narayanan, H. Bourfoune, J. Casper, J. Rasley, M. Ryabinin, M. Mishra, M. Zhang, M. Shoeybi, M. Peyrounette, N. Patry, N. Tazi, O. Sanseviero, P. von Platen, P. Cornette, P. F. Lavallée, R. Lacroix, S. Rajbhandari, S. Gandhi, S. Smith, S. Requena, S. Patil, T. Dettmers, A. Baruwa, A. Singh, A. Cheveleva, A.-L. Ligozat, A. Subramonian, A. Névéol, C. Lovering, D. Garrette, D. Tunuguntla, E. Reiter, E. Taktasheva, E. Voloshina, E. Bogdanov, G. I. Winata, H. Schoelkopf, J.-C. Kalo, J. Novikova, J. Z. Forde, J. Clive, J. Kasai, K. Kawamura, L. Hazan, M. Carpuat, M. Clinciu, N. Kim, N. Cheng, O. Serikov, O. Antverg, O. van der Wal, R. Zhang, R. Zhang, S. Gehrmann, S. Mirkin, S. Pais, T. Shavrina, T. Scialom, T. Yun, T. Limisiewicz, V. Rieser, V. Protasov, V. Mikhailov, Y. Pruksachatkun, Y. Belinkov, Z. Bamberger, Z. Kasner, A. Rueda, A. Pestana, A. Feizpour, A. Khan, A. Faranak, A. Santos, A. Hevia, A. Unldreaj, A. Aghagol, A. Abdollahi, A. Tammour, A. Haji Hosseini, B. Behroozi, B. Ajibade, B. Saxena, C. M. Ferrandis, D. Contractor, D. Lansky, D. David, D. Kiela, D. A. Nguyen, E. Tan, E. Baylor, E. Ozoani, F. Mirza, F. Ononiwu, H. Rezanejad, H. Jones, I. Bhattacharya, I. Solaiman, I. Sedenko, I. Nejadgholi, J. Passmore, J. Seltzer, J. B. Sanz, L. Dutra, M. Samagaio, M. Elbadri, M. Mieskes, M. Gerchick, M. Akinlolu, M. Mc Kenna, M. Qiu, M. Ghauri, M. Burynok, N. Abrar, N. Rajani, N. Elkott, N. Fahmy, O. Samuel, R. An, R. Kromann, R. Hao, S. Alizadeh, S. Shubber, S. Wang, S. Roy, S. Viguier, T. Le, T. Oyebade, T. Le, Y. Yang, Z. Nguyen, A. R. Kashyap, A. Palasciano, A. Callahan, A. Shukla, A. Miranda-Escalada, A. Singh, B. Beilharz, B. Wang, C. Brito, C. Zhou, C. Jain, C. Xu, C. Fourrier, D. L. Periiján, D. Molano, D. Yu, E. Manjavacas, F. Barth, F. Fuhrimann, G. Altay, G. Bayrak, G. Burns, H. U. Vrabec, I. Bello, I. Dash, J. Kang, J. Giorgi, J. Golde, J. D. Posada, K. R. Sivaraman, L. Bulchandani, L. Liu, L. Shinzato, M. H. de Bykhovetz, M. Takeuchi, M. Pàmies, M. A. Castillo, M. Nezhurina, M. Sänger, M. Samwald, M. Cullan, M. Weinberg, M. D. Wolf, M. Mihaljcic, M. Liu, M. Freidank, M. Kang, N. Seelam, N. Dahlberg, N. M. Broad, N. Muellner, P. Fung, P. Haller, R. Chandrasekhar, R. Eisenberg, R. Martin, R. Canalli, R. Su, R. Su, S. Cahyawijaya, S. Garda, S. S. Deshmukh, S. Mishra, S. Ki-

- blawi, S. Ott, S. Sang-aroonsiri, S. Kumar, S. Schweter, S. Bharati, T. Laud, T. Gigant, T. Kainuma, W. Kusa, Y. Labrak, Y. S. Bajaj, Y. Venkatraman, Y. Xu, Y. Xu, Y. Xu, Z. Tan, Z. Xie, Z. Ye, M. Bras, Y. Belkada, and T. Wolf (2023). Bloom: A 176b-parameter open-access multilingual language model.
- Xie, S. M., H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. Liang, Q. V. Le, T. Ma, and A. W. Yu (2023). Doremi: Optimizing data mixtures speeds up language model pretraining.
- Yang, Z., Z. Gan, J. Wang, X. Hu, Y. Lu, Z. Liu, and L. Wang (2022). An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Articial Intelligence, Volume 36, pp. 3081–3089.
- Young, P., A. Lai, M. Hodosh, and J. Hockenmaier (2014). From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics 2, 67–78.
- Yu, J., Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu (2022). Coca: Contrastive captioners are image-text foundation models. Transactions on Machine Learning Research.
- Yuan, S., S. Zhao, J. Leng, Z. Xue, H. Zhao, P. Liu, Z. Gong, W. X. Zhao, J. Li, and J. Tang (2022). Wudaomm: A large-scale multi-modal dataset for pre-training models.
- Yuksekgonul, M., F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou (2023). When and why visionlanguage models behave like bags-of-words, and what to do about it? In International Conference on Learning Representations.
- Zhang, B. and R. Sennrich (2019). Root Mean Square Layer Normalization. In Advances in Neural Information Processing Systems 32, Vancouver, Canada.
- Zhang, J., Y. Zhao, M. Saleh, and P. J. Liu (2019). Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.
- Zhang, R., J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao (2023). Llama-adapter: Ecient ne-tuning of language models with zero-init attention.
- Zhang, S., S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer (2022). Opt: Open pre-trained transformer language models.
- Zhou, Y., Y. Sheng, N. H. Vo, N. Edmonds, and S. Tata (2021). Simplied dom trees for transferable attribute extraction from the web. Ar Xiv abs/2101.02415.
- Zhu, W., J. Hessel, A. Awadalla, S. Y. Gadre, J. Dodge, A. Fang, Y. Yu, L. Schmidt, W. Y. Wang, and Y. Choi (2023). Multimodal C4: An open, billion-scale corpus of images interleaved with text. ar Xiv preprint ar Xiv:2304.06939 .

## Checklist

- 1. For all authors...
 - (a) Do the main claims made in the abstract and introduction accurately reect the paper's contributions and scope? [Yes]
 - (b) Did you describe the limitations of your work? [Yes] See Section 4.
 - (c) Did you discuss any potential negative societal impacts of your work? [Yes] We think that the release of such a dataset strikes a constructive trade-o between the risks associated with datasets built on top of crawled web pages (for instance, the presence of images with faces, the potential of PII in texts, oensive, insulting or threatening, etc.) with the future works that a dataset of such scale, quality and thoughtful ltering can enable.
 - (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We read the ethics review guidelines and tried our best to match the expectations. Our content is extracted from publicly available websites at the time of the web crawl. Given the size of our dataset, it would be prohibitive to get the explicit consent of the authors of these websites. Instead, we respect the choice of content creators by removing opted-out images. Such a strategy cannot be exhaustive and we remain available for content creators to opt-out of the dataset.
- 2. If you are including theoretical results...
 - (a) Did you state the full set of assumptions of all theoretical results? [N/A]
 - (b) Did you include complete proofs of all theoretical results? [N/A]
- 3. If you ran experiments (e.g. for benchmarks)...
 - (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We will release the code used for the creation of the model and its training, along with the model itself.
 - (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Appendix A.3.
 - (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]
 - (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A.3.
- 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
 - (a) If your work uses existing assets, did you cite the creators? [Yes] We mentioned the libraries we used.
 - (b) Did you mention the license of the assets? [Yes] We only used open-source libraries.
 - (c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
 - (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See the ethics review guidelines part.
 - (e) Did you discuss whether the data you are using/curating contains personally identiable information or oensive content? [Yes] The dataset we are releasing is built from publicly accessible websites. As such, there is no content in our dataset that hasn't been publicly visible on the web at some point. Similarly, the dataset might contain texts or images that can be considered oensive, insulting, or threatening, as such data is prevalent on the web. We took measures to remove pornographic content and low-quality texts as much as possible. We did not take additional intentional measures to remove personal information. A manual inspection of 250 random samples reveals that there isn't obvious personally identiable information (excluding celebrities and people mentioned in news articles), although it is likely that the dataset contains some.

- 5. If you used crowdsourcing or conducted research with human subjects...
 - (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]
 - (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
 - (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]

## A Appendix

### A.1 Creation of the Multimodal Web Document Dataset

### A.1.1 Collecting of a Large Number of HTML Files

Our data collection process begins by considering the 25 most recent Common Crawl<sup>8</sup> dumps available at the time of dataset creation. It contains webpages spanning from February 2020 to January/February 2023. We use a modied version of readability-lxml<sup>9</sup> to extract the main text from the pages, discarding any pages that contain text of excessively high perplexity. This process yields a total of 41.2 billion documents.

Selection of English content To identify non-English content, we apply the Fast Text classier (Joulin et al., 2017) to the extracted text, eectively ltering out 63.6% of the documents.

Early text deduplication Often, a set of URLs is crawled repeatedly across dierent Common Crawl snapshots. However, the content of these websites may vary as web administrators make changes over time. Hence, at this stage, we refrain from deduplicating documents based on their URLs. Instead, we perform Min Hash (Broder, 1997) deduplication with 16 hashes calculated over 5-grams. To further rene the data, we eliminate documents containing substantial proportions of repeated paragraphs and n-grams, employing the methodology described in Massive Text (Rae et al., 2022). (Lee et al., 2022; Abbas et al., 2023) show that crawled data often contains a signicant amount of duplication, and training on deduplicated data can improve performance.

Quality classication We employ a logistic regression classier with hashed token frequencies to only retain pages containing human-written text, similar to Brown et al. (2020). The classier is trained using documents from curated datasets, such as Wikipedia and Open Web Text (Gokaslan and Cohen, 2019), as positive examples, and documents sampled from Common Crawl as negative examples. For simplicity, we use a threshold of 0.5 for the probability that a document comes from a curated corpus, which acts as an indicator that a document is human-written.

Following these steps, we obtain 1.1 billion documents and their HTML sources from the associated Common Crawl WARC les.

#### A.1.2 Simplifying HTML Files

The original HTML content of a document contains a wealth of valuable information that proves highly benecial in the process of ltering out undesirable text and images. Therefore, we prioritize pre-processing the raw HTML into simplied HTML, making the subsequent extraction of textual and visual elements more ecient. For this purpose, we use the library selectolax<sup>10</sup> that facilitates ecient parsing of HTML les and creates corresponding DOM trees.

DOM Tree cleaning strategies To simplify the DOM trees, we employ several cleaning strategies. Firstly, we convert tags that indicate line breaks (such as <br>) into actual line breaks. Multiple consecutive line breaks and spaces are condensed into a single instance. Additionally, HTML comments are removed from the DOM trees. Furthermore, we implement recursive processes to eliminate empty leaves and unnest nodes. When a parent node lacks attached text and has only one child, the child node replaces the parent node in the DOM hierarchy. We repeat these operations after removing some nodes, and describe this process in the following paragraphs.

<sup>8</sup>https://commoncrawl.org/

<sup>9</sup>https://github.com/buriy/python-readability

<sup>10</sup>https://github.com/rushter/selectolax

Tag unwrapping This operation involves removing unnecessary styling applied to displayed text by unwrapping a predened set of tags given below. By applying this procedure, tags such as <i>example</i> are transformed into example, eliminating the associated styling elements.

The following tags are unwrapped during the processing of HTML les: a, abbr, acronym, b, bdi, bdo, big, cite, code, data, dfn, em, font, i, ins, kbd, mark, q, s, samp, shadow, small, span, strike, strong, sub, sup, time, tt, u, var, wbr.

Node removal Following the previous step, we conduct a manual inspection of practical examples encompassing all existing HTML tags. Based on our ndings, we establish a curated list that outlines the tags we intend to retain. Any nodes within the HTML DOM tree with tags not included in this list are subsequently removed. We specically retain tags that dene the document structure (e.g., p or h) and tags associated with media elements (e.g., img). However, we opt to remove tags that typically consist of logos, generic content, or spam (e.g., header), as well as tags that often contain noisy text related to website navigation (e.g., li), or text that poses challenges in terms of linearization (e.g., table).

We retain the following tags during the processing of HTML les, as they dene the document's structure: address, article, aside, blink, blockquote, body, br, caption, center, dd, dl, dt, div, figcaption, h, h1, h2, h3, h4, h5, h6, hgroup, html, legend, main, marquee, ol, p, section, summary, title, ul. Additionally, we also preserve the following tags that dene media elements: audio, embed, figure, iframe, img, object, picture, video. Furthermore, we keep the source tag as it may contain an interesting attribute.

Modication of specic nodes We then specically target some <div> nodes that contain footer, header, navigation, nav, navbar, or menu as ID or date as attribute, as well as CSS rules that possess footer or site-info as class. These nodes typically contain website navigation content or article dates and are therefore removed. Additionally, we observe that the presence of a CSS rule with the class more-link often indicates a distinct shift in topic within the webpage, resembling the start of a new document. To account for this, we replace these nodes with the text END\_OF\_DOCUMENT\_TOKEN\_TO\_BE\_REPLACED, which we replace by an end-of-sentence (EOS) token during training.

With these processing steps, we reduce the size of the HTML les by more than 10 on average while preserving the interesting content.

#### A.1.3 Extracting Multimodal Web Documents

In this section, we begin with the simplied HTML les obtained from the previous section. Our objective is to transform these les into a structured web document format, which is a sequence of interleaved texts and images.

Preservation of the original structure of the web pages During the extraction process, we meticulously preserve the original structure of the web pages from the simplied HTML les. We extract the texts and image links while maintaining their order of appearance in the DOM tree. Each HTML tag denotes a distinct separation between the preceding and subsequent nodes and we retain any line breaks and line feeds that are present in the original page, preserving the formatting and visual rendering of the content.

Image downloading To download the images, we use the img2dataset (Beaumont, 2021) library. We attempt to download a massive collection of 3.6 billion images, of which 55% (approximately 2 billion images) were successfully downloaded. For that, we employ 20 virtual machines. This distributed approach allow us to complete the operation within a few days.

#### A.1.4 Filtering Multimodal Web Documents

The ltering process consists of two steps, targeting dierent levels of granularity. In the rst step, ltering occurs at the node level for images and at the paragraph level (separated by line breaks) for text. We evaluate each paragraph or image and we potentially modify or remove these based on specic criteria. The second step, conducted at the document level, involves deciding whether to retain or discard the output documents from the rst step. The majority of the lters for text we use for both steps were adapted from Laurençon et al. (2022).

Node-level image ltering We discard images with formats other than jpg, png or webp, with a side length below 150 pixels or exceeding 20,000 pixels, as well as those with an aspect ratio greater than 2 or less than 1/2. These criteria help exclude images that are too small, excessively large, or have disproportionate dimensions, which are often indicative of low-quality or irrelevant content. To eliminate some logos and generic images, as in (Zhu et al., 2023), we remove images whose URL contains one of the sub-strings logo, button, icon, plugin or widget.

Paragraph-level text ltering Regarding text paragraphs, we apply a series of lters to remove undesirable or irrelevant content. We discard paragraphs with fewer than 4 words, as they typically contain insucient information to be considered meaningful. Additionally, we remove paragraphs with a high repetition ratio, indicating potential spam content, and those with an excessive ratio of special characters, often associated with irrelevant or low-quality text.

Furthermore, we lter out paragraphs with a low ratio of stop words, as it is often indicative of machine-generated or nonsensical content. Similarly, we exclude paragraphs with a low punctuation ratio, as they typically indicate poor-quality texts. We also consider the agged word ratio, removing paragraphs with a high proportion of agged words associated with adult or inappropriate content. We also use KenLM (Heaeld, 2011) models trained on Wikipedia to lter out paragraphs with excessively high perplexity scores.

To minimize spam, one approach is to identify generic sentences or invitations to share articles on social networks commonly found at the end of documents. We create a list of frequently used words associated with these paragraphs and then lter out paragraphs that contain an excessive proportion of words from this list.

To augment our ability to identify non-human-generated content, we consider a subset of 10 million documents from OSCAR (Ortiz Suárez et al., 2020), a web-crawled corpus. We extract the words from these documents, removed punctuations, converted them to lowercase, and retain only the words occurring at least twice, which we refer to as common words. We lter out paragraphs with a too low common word ratio.

The detail of the cuto values for all text lters at the paragraph level is present in Table 3.

By applying these node-level and paragraph-level lters, we ensure that only high-quality and relevant images and paragraphs are retained for further processing and analysis.

Document-level ltering For document-level ltering, we start by removing all documents with no images or with more than 30 images. We have found that when there are too many images in a document, they are often not related to each other, and are more likely to be considered as spam.

For text lters, we use the same lters as for ltering at paragraph level. Since we are at the document level, the lter metrics are more precise, and we can typically set stricter cuto values while limiting the number of false positives. The cuto values used are also present in Table 3.

After these ltering steps, we obtained 365 million web documents and 1.4 billion images (potentially duplicated in dierent documents at this stage).

#### A.1.5 Additional Filtering and Deduplication Steps

Exclusion of opted-out images To respect the preferences of content creators, we remove all images for which creators explicitly opted out of AI model training. We used the Spawning API<sup>11</sup> to verify that the images in the dataset respect the original copyright owners' choices. This step had a small impact on the overall dataset, by removing only 0.047% of the images.

<sup>11</sup>https://api.spawning.ai/spawning-api

| Metric | Cuto<br>type | Cuto<br>value<br>(paragraph<br>level) | Cuto<br>value<br>(document<br>level) |
|--------------------------------------------|---------------|----------------------------------------|---------------------------------------|
| Number of words | min | 4 | 10 |
| Number of words | max | 1,000 | 2,000 |
| Character repetition ratio | max | 0.1 | 0.1 |
| Word repetition ratio | max | 0.1 | 0.2 |
| Special character ratio | max | 0.3 | 0.275 |
| Stop word ratio | min | 0.3 | 0.35 |
| Flagged word ratio | max | 0.01 | 0.01 |
| Punctuation ratio | min | 0.001 | 0.03 |
| Spam word ratio | max | 0.12 | 0.12 |
| Common word ratio | min | 0.8 | 0.9 |
| Language identication prediction<br>score | min | 0.8 | 0.8 |
## | Perplexity score | max | 1500 | 1500 |

Table 3: Cuto values for text lters at paragraph and document levels. A 'min' (or 'max') cuto indicates that any paragraph or document, depending on the level, with a value for the considered metric strictly below (or above) the cuto value is removed.

Image deduplication based on URL Prior to this step, it is possible for the same image to be present in multiple documents under the same URL. However, we observe that the distribution of image occurrences was highly skewed, with the majority of images appearing only once, while a small subset of images appeared hundreds of thousands of times. Upon closer examination, we notice that these frequently occurring images are predominantly comprised of common advertisements encountered during the crawling process, browserspecic icons, and similar elements. To address this issue, we remove all images that appear more than 10 times across the entire dataset. This approach signicantly reduces the presence of unwanted images. We intentionally do not perform strict deduplication, as we observe that when an image is duplicated only a few times across dierent documents, the surrounding text and contextual information tend to vary. These diverse contexts associated with the duplicated image could be benecial for the training of a model. We also deduplicate images within the same document.

NSFW image removal We use an open-source NSFW classier<sup>12</sup> to reduce the proportion of explicit adult content within our dataset. We carefully choose a cuto that reduces as much as possible the proportion of false positives. Indeed, if favoring precision to recall may seem to be a good idea to remove as much undesirable content as possible, it hurts diversity. An analysis of false positives shows that in many cases, simple portrait photos of women are classied as pornographic, which is not the case for men. People of color are also more often misclassied. We remove the entire document when a pornographically classied image is found in the document. In addition, we also remove all images whose URLs contain the sub-strings porn, sex or xxx. We remove approximately 1% of the documents with this lter. Note that many pornographic documents have been previously removed by the lter on agged words.

Document deduplication based on URL Since we consider many Common Crawl dumps, it is possible that several documents may be associated with the same URL, despite the initial deduplication eorts. Recognizing the inherent similarity among these documents, we opt to retain only the most recent document for each common URL.

Document deduplication based on set of images It is possible that documents with dierent URLs and domain names are very similar and have not been removed by the rst

<sup>12</sup>https://github.com/Gant Man/nsfw\_model deduplication, for instance, news articles copied and pasted multiple times across various sources. To mitigate this, we form groups of documents with an identical set of images, and we keep only the most recent document for each group.

Paragraph deduplication across documents of the same domain names To eliminate generic spam phrases commonly found at the end of documents, such as "Share on Facebook," "Post a comment," or "Accept the cookies," we implement a paragraph-level deduplication process within documents sharing the same domain name. This approach aims to enhance the quality of the text by removing redundant and repetitive content. For each domain name, we identify paragraphs that appear at least three times in an identical manner across associated documents. These repetitive paragraphs are subsequently removed from the documents, resulting in the elimination of approximately 15% of the text present in the web documents.

After all these steps, the nal dataset contains 141 million documents and 353 million images, of which 298 million are unique.

We observe that using stricter values for the ltering steps yields fewer multimodal documents, although not of higher quality. As such, we invite users who are interested in manipulating a smaller subset of OBELISC to start with a random subset.

## A.2 Analysis of OBELISC

#### A.2.1 Examples of Multimodal Web Documents

Figure 7: Example of a document in OBELISC. From http://birdingcraft.com/wordpress/2018/01/23/what-happens-with-birding-in-costa-rica-when-the-rain-stops/

#### Document

## Can I Expect Compensation For My Injuries

The word "compensation" can be a touchy issue when discussing personal injuries and settlement. Even when it is the sole objective of a lawarist or some other legal proceeding, mentioning compensation for my injuries can create false expectations in someone's mininf if not addressed in the proper content. A San Diego lawyer who practices personal injury law, for example, says that it is crucial to ensure that a source source content is a sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole of the sole objective or a sole objective or a sole of the sole objective or a sole objective or a sole objective or a sole objective or a

After suffering injuries in an accident, whether at the workplace or through some other negligent action, seeking damages is understandably a logical thing to do. Such legal action may entail going to court and making your case known to the judge. If there's a large sum of money involved, one should always prepare for a protracted legal battle.

The truth is that both a trial and an outright settlement can have very different variables and outcomes. Choosing to go to trial might seem like a good option. After all, many culpable parties are usually in a more agreeable frame of mind once the threat of a court case looms, making them more likely to offer a settlement.

Such parties usually settle a case and of self-interest. The shrink and financial cost of sustaining an effective legal defense can be raisons. In many cases, though, insurance companies step in to offer compensation, After all, many employers and other parties like vehicle drives tend to have insurance congester exactly boses series of situations. Alter assistance injuries, an amount of money is offered to the scitton in both by them with medical bills and any other expenses they may have incurred due to injuries sustained. Have failed parties and minutene companies usually prefer a quick out-of-court settlement.

As a victim, it is always prudent to remember that a court case could be decided against you, thereby leaving you with no compensation at all. While some cases usually result in higher dollar amounts being dolled out as a settlement because of successful diagration, many victims do not want to take the risk. Such victims are already drowning in medical bills by the time they think of seeking compensation for their injunites. That while your poler as with settlement if given the goldon.

How An Insurance Provider Chooses To Settle A Clair

## ![](_page_24_Picture_8.jpeg)

As mentioned, an insurance provider involved in such cases would rather settle a personal injury case out of court. A jury trial is risky for both the personal injury victim and the insurance provider. The unpredictability of many such cases means that an insurance carrier could find themselves having to fork out significantly higher amounts of money in compensation than if they had chosen a quick, out of court settlement.

An insurance provider is always tooking to minimize its costs while ensuring less risk. As such, they may opt to compensate a personal injury victim while simultaneously seeking reimbursement from the third party that is responsible for your injuries, usually from such a third party's insurance carrier.

It's crucial to remember that, in some jurisdictions, an insurance provider is entitled to a percentage of your compensation if they already settled your medical bills prior to you receiving the settlement. This amount is commenciate with all your medical empands.

There now exist online settlement calculators that purpor to provide a rough estimate of the componation approach linky victim can expect. You put in the various numerical values and factors related to your case, and the site will give you a general idea of what to expect in monetary terms. However, sometimes this information can be mislanding and hence you should never every on it. Even with the best personal injury lawyers handling your case, it is difficult if not impossible to account for all of the numerous variables. Even in cases with admitted liability of a third party, getting a sense of a definitive dollar amount for compensation is still difficult. The extent of the injury suffered, emotional distress and pain, and loss of potential future earnings are things that can prove very tricky to quantify. As such, it is individually not view on online sufference relations for curve thereties.

## ![](_page_24_Picture_13.jpeg)

Medical costs and other expenses related to economic losses due to the injury are factored into calculating the damages awarded to a personal injury victim. Loss of companionablip, deprived enjoyment of life, and emotional distress are some of the issues that determine compensation but may be hard to nail down.

While seemingly straightforward, any compensation awarded to a victim only happens after consideration of all relevant factors. Sometimes, the victim of personal injury is to blame, whether partly or in ful.

This has the potential to negate any compensation or at least diminish it. An experienced personal injury attorney can help such victims to fully understand all the different scenarios involved in such cases.

## Can A Victim Reject A Settlement Offer

## ![](_page_24_Picture_17.jpeg)

A personal injury victim is well within his rights to reject compensation. This could arise when the victim feels that the alleged guilty party has not put forward a dollar amount that is representative of the extent of layer and loss incurred. As wictim, you can sit down with your personal injury attorney to get a sense of how such scenarios generally play out. The accused party may be doing this intentionally, hoping that the victim accepts this offer without much consideration. You can express dissatisfaction with such an offer through a personal injury demand letter, outlining your girevances and why you believe you are entitled to more.

In a nutshell, a victim is entitled to compensation when the accused party is found to be responsible for the accident that caused injury to the victim. With many variables in such cases, there is no minimum amount of money set as the standard for compensation. Each case is examined on the merits of its unique factors, ensuring an equitable settlement for all parties.

Figure 8: Example of a document in OBELISC. From https://www.halt.org/can-i-expect-compensation-for-my-injuries/

## ![](_page_25_Picture_2.jpeg)

## ![](_page_25_Picture_6.jpeg)

## ![](_page_25_Picture_8.jpeg)

## ![](_page_25_Picture_10.jpeg)

Figure 9: Example of a document in OBELISC. From https://www.quirkybyte.com/blog/2021/11/how-dane-whitman-will-become-black-knight-kit-harringtons-character-explained/

## A.2.2 Unwanted Document Containing Many Images

## ![](_page_26_Figure_1.jpeg)

Figure 10: Undesirable document containing many images. Text is only present in small proportions, and the relation between the images is not always clear.

#### A.2.3 Top 100 Domains

| Rank | Domain name | Number of |
|------|-------------------------------|-----------|
| | | documents |
| | | |
| 1 | www.dailymail.co.uk | 434,498 |
| 2 | en.wikipedia.org | 155,258 |
| 3 | nypost.com | 141,494 |
| 4 | www.thestar.com | 138,224 |
| 5 | sputniknews.com | 133,695 |
| 6 | www.redi.com | 133,233 |
| 7 | www.theepochtimes.com | 132,539 |
| 8 | www.fool.com | 125,220 |
| 9 | www.businessinsider.com.au | 123,841 |
| 10 | www.bustle.com | 122,581 |
| 11 | www.dailysabah.com | 120,029 |
| 12 | www.rstpost.com | 119,642 |
| 13 | www.irishtimes.com | 118,329 |
| 14 | theathletic.com | 101,982 |
| 15 | www.news.com.au | 98,339 |
| 16 | www.indiatimes.com | 98,197 |
| 17 | www.theglobeandmail.com | 92,805 |
| 18 | tvtropes.org | 92,104 |
| 19 | www.dailydot.com | 91,034 |
| 20 | mashable.com | 88,310 |
| 21 | observer.com | 87,336 |
| 22 | www.cbsnews.com | 86,759 |
| 23 | www.rappler.com | 86,554 |
| 24 | www.tmz.com | 84,472 |
| 25 | www.salon.com | 84,420 |
| 26 | www.modernghana.com | 83,918 |
| 27 | www.foxnews.com | 83,002 |
| 28 | www.hupost.com | 81,701 |
| 29 | www.ndtv.com | 81,549 |
| 30 | www.thisismoney.co.uk | 80,930 |
| 31 | www.famousbirthdays.com | 78,931 |
| 32 | www.engadget.com | 76,817 |
| 33 | www.rnz.co.nz | 76,327 |
| 34 | www.metro.us | 75,627 |
| 35 | www.patheos.com | 75,003 |
| 36 | www.news24.com | 73,883 |
| 37 | www.thestar.com.my | 73,265 |
| 38 | www.dw.com | 72,774 |
| 39 | www.npr.org | 71,939 |
| 40 | koreajoongangdaily.joins.com | 71,091 |
| | | |
| 41 | peoplesdaily.pdnews.cn | 71,048 |
| 42 | pagesix.com | 70,602 |
| 43 | www.thenigerianvoice.com | 70,470 |
| 44 | wikimili.com | 69,928 |
| 45 | www.indiebound.org | 67,986 |
| 46 | www.cricketcountry.com | 66,605 |
| 47 | expressdigest.com | 64,250 |
| 48 | www.capitalfm.co.ke | 64,163 |
| 49 | www.bizpacreview.com | 64,157 |
| 50 | www.wionews.com | 63,797 |
| 51 | profootballtalk.nbcsports.com | 63,532 |
| 52 | jamaica-gleaner.com | 63,137 |
| 53 | www.rte.ie | 63,074 |

| 54 | www.aspentimes.com | 62,552 |
|-----------|----------------------------------------|------------------|
| 55 | kids.kiddle.co | 62,419 |
| 56 | english.alarabiya.net | 60,368 |
| 57 | www.jellypages.com | 59,381 |
| 58 | people.com | 59,293 |
| 59 | muse.jhu.edu | 59,061 |
| 60 | www.geeky-gadgets.com | 58,975 |
| 61 | www.khaleejtimes.com | 58,851 |
| 62 | www.nbcsports.com | 57,922 |
| 63 | en.topwar.ru | 56,723 |
| 64 | www.thewrap.com | 56,146 |
| 65 | www.outlookindia.com | 55,752 |
| 66 | www.celebdirtylaundry.com | 55,618 |
| 67 | time.com | 55,527 |
| 68 | www.dailystar.co.uk | 55,503 |
| 69 | www.legit.ng | 55,395 |
| 70 | www.thehansindia.com | 55,109 |
| 71 | www.bbc.co.uk | 55,015 |
| 72 | newsinfo.inquirer.net | 54,927 |
| 73 | nesn.com | 54,756 |
| 74 | www.tellerreport.com | 53,939 |
| 75 | www.rawstory.com | 53,676 |
| 76 | www.thestatesman.com | 53,286 |
| 77 | wccftech.com | 52,510 |
| 78 | forward.com | 51,969 |
| 79 | nationalinterest.org | 51,851 |
| 80 | www.pearltrees.com | 50,933 |
| 81 | www.contactmusic.com | 50,284 |
| 82 | www.tweaktown.com | 50,138 |
| 83 | www.destructoid.com | 50,081 |
| 84 | www.publishersweekly.com | 49,735 |
| 85 | www.cbs58.com | 49,680 |
| 86 | www.markedbyteachers.com | 48,994 |
| 87 | www.caughtoside.com | 48,857 |
| 88 | www.islamicinvitationturkey.com | 48,721 |
| 89 | dailyhive.com | 48,447 |
| 90 | www.aljazeera.com | 47,393 |
| 91 | www.bbc.com | 47,349 |
| 92 | worldbulletin.dunyabulteni.net | 47,300 |
| 93 | www.romper.com | 47,115 |
| 94 | www.catchnews.com | 47,025 |
| 95 | www.odt.co.nz | 46,712 |
| 96 | www.jewishpress.com | 46,688 |
| | | |
| 97<br>98 | www.irishcentral.com<br>techcrunch.com | 46,629<br>46,539 |
| | | |
| 99<br>100 | www.nhl.com<br>www.tuko.co.ke | 46,247<br>46,106 |
| | | |
## | | | |

Table 4: Ranking of the 100 domains with the highest number of associated documents in OBELISC.

#### A.2.4 Topic Modeling with 20 Topics

| | Concept | Ratio | Related words | | |
|--|---------|-------|---------------|--|--|
## |--|---------|-------|---------------|--|--|

| Justice | 5.16% | said, police, people, year, according, court, case, told, news,<br>man, two, death, also, one, old, investigation, found, re,<br>ocers |
|--------------|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Politics | 6.35% | said, state, government, would, president, trump, law, court,<br>party, public, new, election, states, political, federal, house,<br>people, also, bill |
| Family | 5.24% | family, one, day, back, life, time, home, would, old, said,<br>years, like, two, love, mother, children, rst, man, went |
| Music | 5.23% | music, album, band, song, new, songs, show, also, rst,<br>sound, rock, one, musical, year, released, live, festival, record,<br>track |
| Climate | 3.46% | water, energy, climate, species, also, earth, space, one, used,<br>gas, use, solar, natural, power, carbon, years, change, system,<br>may |
| Business | 7.12% | year, company, million, market, said, new, business, com<br>panies, per, also, billion, percent, price, nancial, money,<br>industry, years, growth, according |
| Sports | 3.75% | game, season, team, rst, year, two, said, three, play, last,<br>games, one, win, second, points, coach, back, players, four |
| Sports (2nd) | 5.67% | team, rst, year, season, league, last, two, club, world, race,<br>one, game, win, time, back, players, match, second, nal |
| Automotive | 4.18% | new, car, also, design, one, power, cars, two, model, use,<br>used, system, camera, rst, speed, engine, high, vehicle,<br>battery |
| Cinema | 7.36% | lm, story, series, movie, book, new, show, one, also, char<br>acters, character, rst, world, star,<br>lms, love, best, life,<br>man |
| War | 4.26% | war, country, said, military, countries, russia, world, russian,<br>government, united, international, people, states, president,<br>also, security, israel, army, forces |
| Gaming | 5.77% | game, use, also, new, games, data, one, users, app, online,<br>using, video, google, players, play, time, used, information,<br>content |
| Health | 3.0% | health, also, may, medical, patients, disease, study, people,<br>treatment, cancer, body, use, drug, research, risk, brain,<br>care, virus, cases |
| Food | 2.08% | food, also, one, beer, like, eat, made, wine, restaurant, make,<br>coee, meat, well, used, tea, sugar, use, water, taste |
| Urban | 4.62% | city, area, new, park, one, building, town, road, also, north,<br>day, around, river, island, south, place, along, local, two |
| Existence | 5.23% | one, people, god, life, world, women, many, even, human,<br>may, like, way, men, often, would, man, also, social, power,<br>must |
| Asia | 1.61% | india, indian, also, china, said, chinese, government, minister,<br>pakistan, country, delhi, kong, hong, people, singh, two,<br>khan, sri, asia |
| History | 4.24% | book, art, rst, history, years, new, century, work, one, books,<br>also, church, american, world, time, museum, english, known |
| Education | 5.11% | school, said, students, work, university, new, community,<br>also, people, years, year, education, program, women, work<br>ing, support, college, children, project |
| Other | 10.56% | like, one, get, would, time, people, really, know, even, think,<br>much, good, going, way, see, could, make, want, things,<br>something |

Table 5: LDA with 20 topics, trained on 100,000 random web documents. A concept for each topic is derived from the related words.

## A.2.5 Topic Modeling with 200 Topics

| Concept | Ratio | Related words |
|--------------------------------|-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Celebrity Relationships | 0.52% | star, fans, show, love, instagram, couple, together,<br>shared, relationship, revealed, year, kim, charlie,<br>told, actress, pete, new, former, old, lisa |
| Music Industry | 1.47% | band, music, song, album, songs, rock, tour, live,<br>singer, show, record, country, bands, released, stage,<br>one, love, played, pop |
| Racial Diversity | 0.26% | black, white, people, race, african, american, racial,<br>community, racism, gay, racist, americans, diversity,<br>lgbtq, justice, color, lgbt, gender, discrimination,<br>queer |
| Language Usage | 0.17% | language, english, word, words, name, languages,<br>use, used, text, names, letter, letters, meaning,<br>translation,<br>writing,<br>spoken,<br>speech,<br>speaking,<br>speak, term |
| Team Spirit | 0.38% | said, get, team, good, really, going, lot, year, think,<br>got, great, like, last, back, well, play, time, guys,<br>big, hard |
| News Media | 0.28% | news, media, radio, fox, press, magazine, journal<br>ists, television, journalism, story, newspaper, editor,<br>journalist, coverage, times, broadcast, interview,<br>daily, podcast, show |
| European Culture | 0.04% | van, dutch, netherlands, tattoo, amsterdam, bel<br>gium, portugal, belgian, der, tattoos, portuguese,<br>bulgaria, soa, holland, bulgarian, lisbon, santos,<br>europe, tulip, brussels |
| European Nations | 0.19% | european, germany, german, europe, berlin, swe<br>den, poland, greece, also, countries, swedish, polish,<br>czech, denmark, norway, austria, greek, hungary,<br>nland |
| Film Industry | 1.29% | lm, movie,<br>lms, director, movies, best, actor,<br>hollywood, documentary, cinema, role, screen, story,<br>directed, production, actors, also, oscar, award |
| Australian<br>Achieve<br>ments | 0.12% | australia, australian, new, zealand, sydney, award,<br>melbourne,<br>awards,<br>year,<br>victoria,<br>queensland,<br>south, nsw, brisbane, australians, best, won, auck<br>land, prize |
| Culinary Delights | 0.88% | cream,<br>recipe,<br>cheese,<br>make,<br>chocolate,<br>made,<br>bread, add, taste, ice, butter, sauce, cake, sugar,<br>cook, food, salt, milk, sweet |
| Life and Death | 0.4% | death, one, people, life, world, dead, even, lives,<br>many, die, died, lost, killed, still, never, man, end,<br>left, day, hope |
| Spiritual Philosophy | 0.2% | philosophy, spiritual, buddhist, religion, religious,<br>yoga, buddha, meditation, buddhism, tibetan, guru,<br>book, practice, knowledge, thought, mind, life, mod<br>ern, texts, tradition |
| Cultural Histories | 0.13% | jewish, jews, indigenous, native, holocaust, rabbi,<br>tribe, people, indian, community, peoples, tribal,<br>israel, tribes, anti, culture, land, camp, history,<br>torah |
| Personal Development | 0.07% | says, people, explains, like, new, adds, get, work,<br>want, also, tells, lot, say, year, years, really, working,<br>part, wants, help |

| Royal Families | 0.23% | king, prince, royal, queen, princess, charles, henry,<br>elizabeth, duke, harry, palace, meghan, family, |
|-----------------------|-------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Daily News | 0.19% | william, anne, castle, kate, lady, diana, edward<br>said, week, friday, monday, wednesday, according,<br>tuesday, thursday, news, last, day, told, sunday, sat |
| Creative Projects | 0.19% | urday, reported, statement, days, morning, hours<br>project, design, work, working, projects, creative,<br>create, idea, team, process, also, ideas, new, make, |
| Legal Investigations | 0.6% | designer, created, started, concept, worked, wanted<br>investigation, information, former, report, fbi, de<br>partment, oce, according, documents, evidence,<br>public, intelligence, government, claims, allegations,<br>corruption, fraud, alleged, ocials, federal |
| Medical Procedures | 0.19% | surgery, skin, pain, treatment, cancer, procedure,<br>patients, teeth, bone, patient, surgical, injury, eye,<br>hair, tissue, surgeon, tooth, breast, honey, medical |
| Athletic Competitions | 0.46% | olympic,<br>sports,<br>world,<br>athletes,<br>games,<br>sport,<br>olympics, gold, team, medal, NUMm, event, won,<br>year, championships, competition, athlete, time,<br>rst |
| Historical Artifacts | 0.62% | ancient, century, NUMth, history, temple, stone,<br>roman, years, one, city, also, greek, found, known,<br>built, old, site, time, today |
| Literary Works | 0.87% | book, books, read, story, author, novel, writing,<br>reading, series, stories, rst, written,<br>ction, pub<br>lished, readers, characters, world, one, write, new |
| Time Progression | 0.73% | one, year, years, last, still, could, even, time, big,<br>new, two, much, like, back, next, would, since,<br>another, well, already |
| Everyday Life | 0.2% | day, time, sleep, night, home, hours, room, water,<br>house, bed, days, morning, work, get, every, food,<br>hour, two, camp, minutes |
| Colorful Nature | 0.16% | color, tea, dark, white, green, owers, skin, like,<br>black, ower, colors, blue, rose, leaves, light, pink,<br>also, red, used, golden |
| Automotive Industry | 1.21% | car, cars, engine, vehicle, new, vehicles, model,<br>electric, ford, drive, also, wheel, rear, speed, driving,<br>toyota, motor, front, power |
| American Cities | 0.11% | new, york, california, city, san, los, angeles, fran<br>cisco, chicago, jersey, state, times, diego, brooklyn,<br>center, santa, bay, seattle, county |
| Political Movements | 0.57% | political, people, power, party, government, right,<br>america, politics, anti, war, state, world, left, free,<br>nation, democracy, american, country, media, sys<br>tem |
| Mythical Creatures | 0.12% | bear, wolf, dragon, snake, bears, lion, like, tiger,<br>monster, wild, human, wolves, animals, snakes,<br>cave, creatures, giant, humans, hunter, dragons |
| Asian Cultures | 0.09% | north, korea, harry, kim, korean, potter, south, jon,<br>thrones, jong, pyongyang, stewart, nuclear, ron,<br>warner, hogwarts, house, game, colbert, peninsula |
| Data Modeling | 0.31% | data, model, number, value, using, numbers, func<br>tion, used, models, values, two, example, method,<br>gure, one, set, problem, object, line |
| Romantic Stories | 1.34% | story, love, life, girl, one, new, woman, nd, young,<br>man, nds, characters, father, friend, two, charac<br>ter, family, romance, secret, series |

| Medical Research | 0.41% | cancer, cells, cell, dna, disease, gene, human, pa<br>tients, genetic, immune, protein, treatment, genes,<br>bacteria, researchers, diseases, research, proteins, |
|-------------------------------|-------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fitness and Training | 0.21% | study, clinical<br>running, race, run, training, marathon, tness,<br>miles, exercise, bike, mile, runners, NUMk, course,<br>gym, nish, cycling, yoga, half, runner |
| Personal Perspectives | 1.43% | like, people, think, really, would, know, going, get,<br>see, one, lot, things, something, time, want, way,<br>much, thing, say, could |
| Gastronomy Scene | 0.44% | food, restaurant, coee, bar, restaurants, menu,<br>chef, chicken, pizza, meal, kitchen, dishes, dinner,<br>eat, dining, burger, table, meals, served, like |
| Labor Rights | 0.29% | workers, work, employees, job, jobs, union, pay,<br>labor, working, employment, insurance, employers,<br>wage, employee, company, paid, worker, labour,<br>sta, business |
| Competitive Sports | 0.75% | game, second, goal, rst, ball, half, back, minutes,<br>win, lead, two, points, score, minute, nal, match,<br>side, three, time |
| Public Events | 0.71% | year, event, festival, christmas, day, events, NUMth,<br>show, night, tickets, special, holiday, party, live,<br>celebrate, held, also, place, saturday |
| Digital Marketing | 0.37% | digital, content, marketing, media, brand, adver<br>tising, platform, online, campaign, ads, business,<br>industry, social, new, users, platforms, brands, com<br>panies, internet, consumers |
| Public Safety | 0.24% | safety, report, action, letter, statement, said, inci<br>dent, ban, made, public, actions, claims, reported,<br>according, response, taken, complaints, following,<br>take, serious |
| French Heritage | 0.1% | french, france, paris, jean, saint, les, des, pierre,<br>dame, marie, europe, macron, notre, louis, euro<br>pean, michel, jamaica, jacques, emmanuel |
| Eastern European Poli<br>tics | 0.38% | russian, russia, ukraine, ukrainian, moscow, putin,<br>soviet, state, vladimir, war, azerbaijan, country, ar<br>menian, armenia, president, russians, union, sanc<br>tions, region |
| Horror Entertainment | 0.58% | movie, story, horror, characters, character, lm,<br>action, one, plot, ghost, scene, evil, movies, like,<br>series, original, genre, dark, scenes, rst |
| Political Campaigns | 1.25% | trump, president, election, vote, campaign, obama,<br>party, biden, house, donald, political, republican,<br>presidential, voters, democratic, democrats, candi<br>date, clinton, candidates, white |
| Indian Cinema | 0.64% | lm, khan, actor, also, movie, bollywood,<br>lms,<br>kapoor, indian, actress, seen, role, singh, india,<br>release, hindi, kumar, directed, hai, salman |
| Corporate Leadership | 0.82% | years, board, director, president, team, business,<br>leadership, work, executive, also, chief, role, mem<br>ber, management, service, experience, served, sta,<br>working |
| Law Enforcement | 1.94% | police, said, ocers, man, ocer, arrested, year,<br>old, incident, two, found, according, investigation,<br>killed, department, shot, scene, vehicle, suspect |
| Football Clubs | 1.26% | club, league, season, united, premier, players, city,<br>football, chelsea, team, arsenal, player, manchester,<br>liverpool, game, side, back, last, games |

| Essential Skills | 0.84% | get, make, need, one, also, time, best, want, many,<br>use, may, take, nd, like, even, help, way, good, |
|-----------------------|-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Artistic Expression | 0.75% | people, much<br>art, museum, artist, work, artists, exhibition, paint<br>ing, works, gallery, arts, paintings, collection, artis<br>tic, drawing, new, show, contemporary, painted,<br>artwork |
| American Regions | 0.22% | state, county, texas, orida, north, south, michigan,<br>ohio, carolina, states, virginia, west, georgia, center,<br>university, washington, colorado, iowa, arizona |
| Industrial Production | 0.28% | production, company, industry, mining, manufac<br>turing, gold, mine, port, supply, project, companies,<br>factory, industrial, plant, steel, products, equip<br>ment, coal, goods |
| Global Aairs | 0.36% | world, countries, international, united, trade, china,<br>states, global, country, foreign, europe, region, asia,<br>economic, european, nations, south, india, east |
| Government Aairs | 1.26% | minister, government, said, meeting, party, presi<br>dent, prime, would, members, committee, council,<br>parliament, also, general, decision, agreement, po<br>litical, secretary, national, commission |
| Software Development | 0.67% | code, use, le, using, software, version,<br>les, win<br>dows, run, server, application, web, source, open,<br>user, system, new, linux, install |
| UK Happenings | 0.22% | london, british, england, britain, centre, brexit, bbc,<br>wales, labour, west, manchester, johnson, north,<br>programme, south, across, may, year, east |
| Real Estate Market | 0.16% | property, housing, estate, home, real, homes, house,<br>rent, properties, market, land, mortgage, rental,<br>sale, houses, price, owner, buyers, sales, units |
| Fashion Trends | 0.43% | fashion, hair, wearing, dress, wear, look, style, cloth<br>ing, clothes, black, wore, designer, beauty, shirt,<br>women, also, made, show, costume, new |
| Gaming Culture | 0.38% | game, cards, card, games, play, players, poker,<br>player, casino, online, gambling, win, deck, playing,<br>betting, lottery, bet, slot, chess, played |
| Famous Personalities | 0.04% | bond, kelly, martin, daniel, peter, doctor, tony,<br>johnny, parker, sean, evans, frank, andy, ian, lucas,<br>dave, reynolds, spy, emily, amber |
| Wildlife Conservation | 0.61% | species, birds, bird, animals, sh, found, animal,<br>also, wild, wildlife, eggs, habitat, large, food, like,<br>small, humans, insects, many, endangered |
| Pandemic Responses | 0.94% | covid, pandemic, health, people, virus, coronavirus,<br>vaccine, cases, said, spread, outbreak, public, lock<br>down, vaccines, government, new, disease, vaccina<br>tion, deaths |
| Popular Names | 0.11% | john, michael, david, paul, jones, james, johnson,<br>mike, jim, steve, robert, two, bob, davis, moore,<br>allen, brian, mark, one |
| Christian Theology | 0.45% | god, jesus, christ, bible, christian, church, faith,<br>lord, people, gospel, paul, christians, john, prayer,<br>word, biblical, kingdom, pastor, moses |
| Sports | 0.77% | season, team, game, nba, games, basketball, players,<br>player, play, coach, league, hockey, points, teams,<br>nhl, played, rst, star, year |
| Cybersecurity | 0.63% | data, security, network, internet, cloud, informa<br>tion, access, technology, services, service, NUMg,<br>software, computer, systems, networks, cyber, de<br>vices, users, attacks, use |

| Business/Finance | 0.78% | company, business, companies, market, industry, in<br>vestment, investors, capital, tech, rm, ceo, based,<br>technology, billion, businesses, group, million, - |
|------------------------------|-------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Professional Wrestling | 0.18% | nancial, growth<br>wwe, ring, wrestling, match, rick, randy, champion,<br>title, wrestler, vince, show, fans, wrestlers, owens, |
| Japanese Culture/Tech | 0.15% | tag, baker, triple, shane, raw, cody<br>anime, musk, japanese, tesla, manga, series, elon,<br>japan, ninja, episode, samurai, kai, characters, de<br>mon, karate, character, also, dragon, arc, tokyo |
| Scottish Personalities | 0.03% | brown, scotland, scottish, gordon, glasgow, celtic,<br>perry, walker, murray, graham, letter, edinburgh,<br>cover, campbell, watson, thomas, also, well, neil, |
| Streaming Media | 0.12% | henderson<br>video, youtube, videos, live, watch, channel, stream<br>ing,<br>audio,<br>content,<br>stream,<br>channels,<br>footage,<br>shows, online, also, NUMk, recording, watching, |
| Christianity | 0.36% | clip, one<br>church, catholic, pope, religious, christian, churches,<br>bishop, francis, faith, holy, priest, saint, mass, vati<br>can, religion, pastor, christ, parish, christians |
| Smartphone<br>Technol<br>ogy | 0.83% | phone, apple, samsung, iphone, pro, smartphone,<br>device, galaxy, camera, also, display, battery, new,<br>sNUM, screen, NUMgb, phones, NUMg, android |
| Urban Development | 0.78% | city, project, area, council, residents, community,<br>park, town, street, public, local, cities, new, de<br>velopment, mayor, urban, construction, district, |
| Sociocultural Issues | 0.39% | building<br>social, culture, society, cultural, people, political,<br>dierent, moral, identity, important, values, issues,<br>often, public, role, many, way, community, under |
| Common Male Names | 0.03% | standing, view<br>smith, jack, tom, ben, adam, alex, kevin, richard, si<br>mon, holmes, billy, bell, oliver, harvey, jake, collins,<br>burke, baldwin, joel, aaron |
| Combat Sports | 0.49% | ght, title, tennis, champion, ufc, round, world,<br>boxing, ghter, one, win, open, martial,<br>rst, match,<br>mma, ghters,<br>ghting, career |
| Indian Politics | 0.64% | india, indian, state, delhi, government, also, min<br>ister, bjp, said, modi, singh, chief, congress, crore,<br>pradesh, mumbai, gandhi, lakh, hindu |
| Military History | 0.25% | war, world, battle, empire, british, army, history,<br>german, peace, great, military, wars, end, conict,<br>power, two, land, forces, soldiers, ght |
| Internet Cartography | 0.04% | www, map, sri, http, https, maps, lanka, com,<br>atlas, derby, tamil, lankan, html, maria, angelo,<br>tara, colombo, org, mapping, easter |
| European Football | 0.46% | league, champions, team, goals, world, season, foot<br>ball, club, cup, madrid, barcelona, player, real,<br>players, match, messi, ronaldo, liverpool, nal |
| Mobile Applications | 0.73% | app, google, apple, android, users, mobile, apps,<br>phone, new, devices, device, ios, iphone, microsoft,<br>use, also, features, user, screen, windows |
| Korean Entertainment | 0.11% | lee, korean, korea, kim, south, park, seoul, drama,<br>group, bts, jin, jung, rst, also, members, won, woo,<br>hyun, young, min |
| Economics | 1.01% | market, price, prices, markets, growth, ination,<br>economy, stock, economic, rate, rates, investors,<br>higher, year, demand, stocks, trading, dollar, gold |

| Video Games | 0.49% | games, game, xbox, gaming, nintendo, video, play,<br>console, playstation, mario, psNUM, one, sony, |
|--------------------------------|-------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Time Indicators | 0.3% | players, steam, gamers, switch, playing, titles<br>rst, years, since, time, two, NUMth, three, total,<br>day, year, may, second, september, june, january, |
| Science<br>Fic<br>tion/Fantasy | 0.14% | november, four, NUM/NUM, april<br>star, wars, trek, lego, luke, gures, force, series,<br>jedi, kirk, toy, universe, gure, new, ship, galaxy,<br>crew, fans, space, disney |
| Music Production | 1.09% | album, sound, music, band, track, song, guitar,<br>metal, sounds, tracks, songs, record, bass, vocals,<br>new, release, rock, like, released, drums |
| Transportation | 0.42% | document, token, road, end, replaced, bike, traf-<br>c, driving, drivers, bus, train, driver, bridge, car,<br>station, ride, roads, route, transport, rail |
| Personal Life | 1.14% | life, people, love, world, many, time, one, always,<br>years, great, every, like, way, friends, never, day,<br>work, rst, hope, best |
| American History | 0.6% | american, history, NUMs, new, rst, years, century,<br>america, early, states, united, NUMth, became,<br>world, many, one, today, time, war |
| Global Policy | 0.96% | change, climate, development, economic, govern<br>ment, global, policy, need, sector, world, public,<br>new, support, economy, national, social, future, |
| South Asian Aairs | 0.2% | health, impact, crisis<br>pakistan,<br>afghanistan,<br>taliban,<br>kashmir,<br>bangladesh,<br>khan,<br>india,<br>pakistani,<br>afghan,<br>also, nepal, country, indian, kabul, jammu, singh, |
| Sports Scores | 0.83% | islamabad, ali, lahore, karachi<br>game, points, rst, season, two, three, win, second,<br>four, team, lead, run, third, one, ve, scored, home,<br>games, point |
| Travel/Daily Life | 1.03% | day, time, back, get, last, one, got, good, night,<br>next, morning, went, rst, trip, week, see, around,<br>way, little |
| Announcements | 0.83% | new, year, rst, last, time, next, NUMth, month,<br>also, release, announced, two, months, march, since,<br>october, september, week, may |
| Online Dating | 0.13% | dating, gay, online, sites, date, site, tinder, free,<br>men, best, matchmaking, meet, guy, hookup, guys,<br>app, apps, relationship, singles, dates |
| Superhero Comics | 0.42% | comic, marvel, comics, man, batman, spider, super<br>hero, character, avengers, superman, universe, hero,<br>captain, new, heroes, fans, issue, super, characters,<br>also |
| Space Exploration | 0.31% | space, nasa, mission, mars, drone, launch, rocket,<br>satellite, robot, earth, robots, drones, moon, rst,<br>station, orbit, satellites, spacecraft, technology |
| Musical Performance | 0.57% | music, jazz, musical, concert, piano, orchestra, com<br>poser, musicians, classical, symphony, played, per<br>formance, playing, performed, piece, work, instru<br>ments, also, festival, instrument |
| Personal Finance | 0.17% | money, pay, card, credit, bank, cash, vegas, pay<br>ment, paid, account, las, payments, fees, cost, cards,<br>amount, buy, service, fee |
| Television Shows | 0.74% | netix,<br>show,<br>series,<br>season,<br>episode,<br>shows,<br>episodes, television, comedy, watch, cast, fans, also,<br>new, seasons, character, drama, viewers, rst |

| Celebrity Culture | 0.11% | taylor, jackson, justin, swift, star, jennifer, singer,<br>jay, tyler, cohen, nicole, spencer, also, eddie, cole, |
|---------------------------------------|-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Environmental Conser<br>vation | 0.32% | carrie, amy, lopez, bieber, casey<br>water, river, land, environmental, forest, wildlife,<br>conservation, area, natural, lake, areas, project, en<br>vironment, rivers, dam, resources, forests, national,<br>management |
| Physical/Quantum Sci<br>ences | 0.35% | water, air, chemical, used, process, material, sur<br>face, materials, quantum, temperature, high, oxy<br>gen, carbon, radiation, particles, liquid, salt, energy,<br>pollution, chemicals |
| Astronomy | 0.37% | earth, sun, moon, planet, sky, stars, solar, star,<br>space, light, universe, planets, telescope, years, sci<br>entists, system, galaxy, eclipse, dark |
| Islamic/Middle<br>East<br>ern Culture | 0.19% | muslim, saudi, muslims, islam, islamic, arabia,<br>egypt, arab, dubai, allah, uae, ali, middle, abu,<br>prophet, religious, muhammad, mosque, iran, egyp<br>tian |
| Gender Issues | 0.14% | women, men, woman, female, girls, gender, male,<br>abortion, sexual, girl, young, sex, life, equality,<br>feminist, man, violence, ladies, rights, boys |
| Fantasy/Mythology | 0.03% | sam, lewis, max, rings, twin, troy, monkey, toy,<br>stephen, palmer, doll, hobbit, tolkien, zeus, lord,<br>monkeys, seth, horse, toys, witch |
| Video Game Mechanics | 0.36% | attack, damage, enemy, pokemon, use, weapon,<br>enemies, level, also, ght, battle, attacks, players,<br>power, weapons, ability, magic, hero, character,<br>armor |
| MMORPG Gaming | 1.16% | game, games, players, play, new, player, world, play<br>ing, characters, gameplay, mode, character, also,<br>story, battle, fun, experience, free, fantasy |
| Energy<br>and<br>Environ<br>ment | 0.65% | energy, oil, gas, power, carbon, solar, fuel, emis<br>sions, electricity, climate, wind, renewable, coal,<br>natural, green, production, industry, fossil, environ<br>mental |
| Financial Regulations | 0.57% | tax, nancial, bank, government, debt, income,<br>banks, money, taxes, budget, economy, nance,<br>loan, pay, billion, loans, credit, economic, fund |
| US Legislation | 0.75% | state, bill, would, federal, house, senate, congress,<br>law, legislation, act, states, governor, government,<br>passed, public, committee, lawmakers, plan, fund<br>ing |
| Subjective Experience | 0.91% | like, good, really, one, well, much, great, bit, even,<br>little, quite, also, though, still, pretty, lot, see, get,<br>better, would |
| Parenthood | 0.16% | children, child, kids, parents, baby, age, young,<br>birth, parent, pregnancy, pregnant, family, families,<br>babies, adults, mother, old, early, mothers |
| Personal Experiences | 1.93% | like, get, one, know, got, really, good, little, even,<br>think, guy, thing, going, love, pretty, right, let,<br>much, never, back |
| Education | 0.55% | school, students, education, schools, college, stu<br>dent, high, university, class, teachers, year, teacher,<br>campus, program, learning, teaching, classes, chil<br>dren, grade, parents |
| Latin<br>American<br>Cul<br>tures | 0.17% | mexico, spanish, italian, spain, italy, san, mexi<br>can, latin, puerto, del, cuba, rico, colombia, costa,<br>america, cuban, venezuela, juan, country |

| Technological Systems | 0.68% | system, new, technology, systems, development,<br>also, use, time, process, high, based, performance,<br>work, used, well, using, provide, quality, level, de<br>veloped |
|---------------------------------|-------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Social Movements | 0.6% | rights,<br>people,<br>government,<br>human,<br>violence,<br>protest, freedom, police, country, protests, law,<br>civil, political, protesters, movement, state, justice,<br>activists, right, groups |
| Surng/Beach Culture | 0.02% | scott, ryan, wilson, joe, anderson, wave, josh, sarah,<br>phil, surf, jackie, waves, robinson, logan, beach, ken,<br>surng, phoenix, duncan, gibson |
| Brazilian Culture | 0.03% | brazil, brazilian, miller, rio, phillips, paulo, por<br>tuguese, peterson, grande, são, janeiro, ivy, bol<br>sonaro, herman, silva, state, amazon, sao, spike,<br>hernandez |
| Literature/Poetry | 0.32% | poetry, writing, essay, writer, poem, poems, literary,<br>literature, work, poet, book, published, writers,<br>wrote, write, english, works, collection, written, life |
| Family Life | 0.58% | family, years, wife, home, mary, born, school, life,<br>funeral, friends, died, church, death, service, many,<br>member, may, mrs, passed |
| Cricket | 0.47% | cricket, india, test, match, runs, team, england,<br>series, rst, wickets, ipl, overs, game, tNUM, played,<br>indian, ball, innings, captain |
| Canadian/Irish Aairs | 0.09% | canada, canadian, ireland, irish, toronto, ontario,<br>vancouver,<br>dublin,<br>province,<br>alberta,<br>northern,<br>canadians, ottawa, montreal, provincial, centre,<br>quebec, north, trudeau |
| Music Industry | 1.01% | music, album, song, artists, artist, hip, single, hop,<br>released, new, songs, rapper, track, video, rap, pop,<br>release, hit, singer |
| Criminal Justice | 0.6% | prison, crime, criminal, court, charges, sexual, trial,<br>case, jail, years, crimes, guilty, victims, murder,<br>abuse, accused, sentence, justice, convicted |
| Academic Research | 0.66% | university, research, science, professor, institute,<br>studies, college, scientic, school, work, study, en<br>gineering, national, international, department, stu<br>dents, degree, academic, center |
| Names and Dates | 0.02% | williams, hill, ross, carter, kennedy, clark, jan, nel<br>son, jordan, stanley, rated, murphy, arthur, mar<br>shall, hudson, feb, nov, oct, mar |
| Weather Conditions | 0.49% | weather, ice, snow, mountain, winter, north, tem<br>peratures, cold, climate, south, high, lake, rain,<br>temperature, east, west, summer, conditions, ski |
| Health and Medicine | 0.54% | blood, brain, disease, symptoms, may, heart, pa<br>tients, body, treatment, also, cause, risk, pain, con<br>dition, eects, common, severe, doctor, pressure |
| Cryptocurrency | 0.47% | bitcoin, blockchain, crypto, cryptocurrency, digital,<br>mining, ethereum, cryptocurrencies, currency, ex<br>change, btc, market, network, tokens, users, price,<br>nft, trading, transactions, token |
| Diet and Nutrition | 0.38% | food, diet, weight, health, body, fat, eating, foods,<br>eat, sugar, healthy, also, high, diabetes, people,<br>meat, protein, obesity, levels |
| Actions<br>and<br>Move<br>ments | 0.12% | back, get, time, take, right, move, way, next, see,<br>start, around, keep, make, end, away, going, one,<br>left, another, turn |

| Historic Landmarks | 0.36% | NUMth, town, village, name, william, george, cen<br>tury, hall, john, family, built, castle, early, house, |
|-----------------------------------|-------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Electronic Devices | 0.41% | mill, street, history, became, morris<br>power, light, battery, use, control, device, used,<br>system, led, also, using, devices, high, signal, air,<br>electrical, switch, low, sensor |
| Performing Arts | 0.43% | theatre, show, dance, stage, play, theater, perfor<br>mance, production, audience, musical, opera, arts,<br>broadway, dancing, cast, performances, performing, |
| Mental Health | 0.26% | company, ballet, shakespeare<br>mental, people, health, disorder, depression, help,<br>self, anxiety, stress, emotional, person, life, physical, |
| Online Interaction | 0.35% | may, often, brain, also, social, autism, feel<br>post, blog, read, comments, posted, like, would,<br>one, see, com, please, know, article, share, site,<br>email, comment, posts, link, page |
| Substance Usage | 0.27% | drug, drugs, cannabis, marijuana, use, cbd, medical,<br>eects, addiction, fda, used, alcohol, cocaine, sub<br>stance, prescription, heroin, treatment, products,<br>thc, also |
| Outdoor Landscapes | 0.46% | tree, trees, trail, water, road, river, along, forest,<br>area, around, small, park, one, near, old, wood,<br>way, hill, across, ground |
| Colors | 0.06% | red, blue, white, green, black, yellow, color, light,<br>ag, orange, grey, colors, gray, logo, one, pearl, hat, |
| Israel and Fishing | 0.19% | look, colour, two<br>israel, israeli, sh, palestinian, jerusalem,<br>shing,<br>gaza, palestinians, netanyahu, hamas, jewish, bank, |
| Air Travel | 0.4% | west, palestine, state, arab, israelis, trout, salmon<br>airport, ight, aircraft, air, airlines, plane,<br>ights,<br>travel, airline, passengers, aviation, ying,<br>y, inter |
| Waste and Recycling | 0.16% | national, airports, pilot, passenger, boeing, service<br>plastic, waste, made, used, use, bags, make, bag,<br>paper, items, nike, fabric, shoes, cola, using, coca, |
| Philosophical<br>Dis<br>course | 0.34% | trash, recycling, also, shoe<br>would, even, one, could, however, much, fact, yet,<br>rather, far, though, many, well, might, perhaps,<br>less, long, despite, may, time |
| Problems and Issues | 0.16% | could, problem, many, may, problems, due, however,<br>issues, issue, would, even, also, cause, result, still,<br>time, situation, damage, impact, without |
| Firearms and Malaysia | 0.17% | rie,<br>gun,<br>shooting,<br>guns,<br>malaysia,<br>hunting,<br>rearms,<br>shot,<br>deer,<br>weapons,<br>shoot,<br>weapon,<br>malaysian, pistol, rearm, ammunition, rmNUM,<br>hunt, buck |
| Disney and Animation | 0.12% | disney, magic, world, ray, animation, alice, walt,<br>park,<br>animated,<br>fairy,<br>ride,<br>parks,<br>disneyland,<br>theme, magical, pixar, jungle, studios, orlando,<br>characters |
| Middle<br>Eastern<br>Con-<br>ict | 0.81% | syria, turkey, forces, iraq, military, security, attacks,<br>attack, killed, syrian, terrorist, turkish, war, people, |
| Physical Descriptions | 0.48% | state, group, isis, terrorism, terrorists, government<br>eyes, like, face, could, head, hand, back, little,<br>looked, hands, said, around, look, body, would, |
| Architecture | 0.62% | voice, see, away, hair, felt<br>building, house, room, space, built, oor, construc<br>tion, wall, buildings, new, home, design, tower, two,<br>walls, architecture, roof, rooms, designed |

| Travel Destinations | 0.94% | city, hotel, park, one, visit, tour, world, town, place,<br>travel, area, many, also, trip, beautiful, places, visi |
|-------------------------------------|-------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Computer Hardware | 0.41% | tors, located, island<br>intel, performance, computer, memory, amd, core,<br>graphics, usb, windows, laptop, drive, cpu, card, |
| African Nations | 0.17% | power, nvidia, hardware, gpu, processor, gaming<br>africa, south, african, kenya, country, cape, uganda,<br>rNUM,<br>zimbabwe,<br>continent,<br>national,<br>congo,<br>africans, west, tanzania, president, town, johan |
| Military Operations | 0.37% | nesburg, rwanda, nairobi<br>military, army, war, soldiers, forces, troops, general,<br>service, battle, soldier, commander, men, armed,<br>corps, force, command, training, unit, guard, com |
| Tobacco and Cookies | 0.15% | bat<br>cookies, website, smoking, use, tobacco, cigarettes,<br>buy, smoke, experience, cigar, cookie, necessary,<br>used, ivermectin, cigarette, consent, online, may, |
| Nigerian Politics | 0.67% | vaping, also<br>state, nigeria, said, government, nigerian, gover<br>nor, president, ghana, lagos, buhari, also, nNUM,<br>nigerians, country, national, federal, people, apc, |
| Family Dynamics | 0.54% | security, abuja<br>family, father, mother, son, old, daughter, home,<br>children, years, year, parents, wife, young, brother,<br>life, dad, two, house, sister |
| Farming and Agricul<br>ture | 0.4% | plant, farmers, farm, food, plants, agriculture, gar<br>den, soil, agricultural, seeds, grow, growing, seed,<br>crop, crops, production, farming, farms, fruit, har |
| Retail Industry | 0.27% | vest<br>store, market, products, sales, amazon, stores, cus<br>tomers, price, company, business, retail, product,<br>buy, shop, online, consumers, brand, shopping, sell, |
| Online Resources | 0.32% | selling<br>download, information, free, page, available, online,<br>book, edition, website, pdf, article, site, published, |
| Personal Experiences | 2.07% | library, content, please, text, may, read<br>would, time, could, one, didn, rst, back, got, went,<br>years, came, wanted, made, started, took, never, |
| Theology and Morality | 0.45% | day, wasn, thought, even<br>god, man, one, lord, world, life, earth, upon, power,<br>may, spirit, human, evil, love, heaven, gods, soul, |
| Sports and Games | 1.29% | must, every, shall<br>season, game, team, football, n, yards, baseball,<br>games, players, league, coach, eld, play, year, |
| Asia and Pacic | 0.07% | player, bowl, quarterback, teams, rst<br>japan, japanese, tokyo, vietnam, indonesia, pa<br>cic, hawaii, island, vietnamese, indonesian, islands, |
| Healthcare | 0.27% | asian, also, asia, west, rice, jakarta, abe, hawaiian<br>health, care, medical, hospital, patients, doctors,<br>healthcare, patient, treatment, services, medicine,<br>doctor, hospitals, hiv, nursing, nurses, emergency, |
| Commemorations | 0.21% | insurance, nurse, sta<br>day, memorial, anniversary, national, NUMth, cere<br>mony, veterans, ag, honor, statue, cemetery, peo<br>ple, nation, war, country, president, service, years, |
| Collectibles<br>and<br>Auc<br>tions | 0.32% | monument<br>gold, collection, silver, watch, auction, box, original,<br>sold, coin, coins, one, made, sale, watches, design,<br>set, edition, also, rare |

| East Asia | 0.18% | china, chinese, kong, hong, singapore, philippines,<br>beijing, taiwan, thailand, shanghai, asia, also, thai, |
|---------------------------------|-------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Maritime Exploration | 0.4% | province, asian, country, philippine, city, manila<br>sea, island, ship, boat, ocean, water, coast, beach,<br>bay, ships, marine, islands, boats, cruise, port, wa<br>ters, crew, shing, sailing |
| Natural Disasters | 0.39% | re, people, storm, hurricane, disaster, emergency,<br>res, damage,<br>ood, earthquake, rescue, smoke,<br>ooding,<br>reghters, homes, residents, burning, hit,<br>area |
| Legal Matters | 0.69% | court, law, case, judge, legal, supreme, justice, de<br>cision, attorney, led, trial, cases, courts, lawyer,<br>lawyers, lawsuit, appeal, ruling, judges |
| Dimensions and Posi<br>tioning | 0.47% | two, side, one, top, right, back, cut, line, use,<br>small, used, hand, like, left, body, front, size, using,<br>around |
| Relationships and Mar<br>riage | 0.18% | marriage, sex, relationship, married, wedding, love,<br>couple, sexual, divorce, man, husband, wife, cou<br>ples, together, woman, partner, men, one, relation<br>ships, bride |
| Community Projects | 0.84% | community, support, group, people, members, pro<br>gram, help, local, foundation, event, also, work,<br>organization, part, project, together, youth, young,<br>year |
| Photography | 0.26% | image, camera, images, photo, photos, NUMd, pho<br>tography, pictures, cameras, picture, light, lens,<br>photographer, capture, photographs, taken, shot,<br>look, using, shoot |
| Competitive Sports | 0.88% | team, players, teams, cup, tournament, world, foot<br>ball, competition, nal, round, golf, play, club,<br>match, rst, won, league, win, sports |
| Innovation and Science | 0.57% | world, human, new, reality, create, like, time, life,<br>future, nature, work, experience, way, process,<br>space, ideas, dierent, form, idea, science |
| Personal Opinions | 1.87% | people, know, like, think, say, even, want, make,<br>one, something, things, someone, way, doesn, would, |
| Statistics | 0.99% | good, need, person, feel, never<br>percent, per, year, number, according, cent, av<br>erage, report, increase, years, rate, million, data, |
| Personal<br>Communica<br>tion | 0.15% | population, last, people, increased, growth, higher<br>said, would, told, people, added, could, asked, also,<br>going, think, want, year, last, say, saying, one, |
| Animal Companions | 0.3% | interview, make, come, according<br>dog, dogs, cat, animals, animal, cats, horse, pet,<br>breed, horses, pets, also, owner, bull, owners, pig,<br>rescue, puppy, pigs, humans |
| Scientic Research | 0.41% | study, research, data, researchers, found, results,<br>studies, risk, analysis, evidence, group, published,<br>test, ndings, based, university, likely, may, could |
| Mystery<br>and<br>Adven<br>ture | 0.43% | man, back, one, left, door, street, front, around,<br>away, saw, car, went, two, night, told, heard, took, |
| Motor Racing | 0.85% | later, behind, another<br>race, racing, team, season, track, car, races, sec<br>ond, rst, win, championship, lap, two, driver, top, |
| International Politics | 0.56% | series, year, drivers, fNUM<br>united, states, iran, border, trump, nuclear, pres<br>ident, immigration, security, country, administra<br>tion, foreign, american, countries, migrants, policy,<br>refugees, immigrants, government, washington |

| Air Defense | 0.34% | air, aircraft, force, military, navy, defense, defence,<br>wing, ghter, missile,<br>ying, base, naval, command,<br>pilot, pilots, ight, forces, jet |
|---------------------------------|-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Additional Information | 0.62% | within, however, additionally, stated, mentioned,<br>one, extra, password, might, individuals, simply,<br>time, present, actually, get, place, may, together,<br>dierent |
| Financial Performance | 0.62% | million, year, billion, company, quarter, sales, rev<br>enue, per, said, share, total, according, last, rst,<br>NUMm, percent, expected, growth, reported |
| Alcohol and Beverages | 0.38% | beer, wine, drink, alcohol, brewery, drinking, wines,<br>bottle, brewing, beers, craft, taste, brew, drinks,<br>whisky, ale, tasting, bar, whiskey, bottles |
| Celebrity Proles | 0.66% | also, career, born, known, years, worth, age, net,<br>life, famous, american, became, name, rst, million,<br>started, year, appeared, actress |
| Storytelling and Narra<br>tives | 1.26% | like, life, story, world, one, time, sense, way, yet,<br>much, work, makes, narrative, every, often, takes,<br>moments, something, stories, piece |
| Legislation | 0.78% | law, act, rules, may, legal, laws, government, public,<br>must, state, regulations, would, information, rule,<br>commission, states, required, order, authority |
| Social Media | 0.45% | twitter, facebook, social, media, instagram, post,<br>people, account, also, pic, tweet, share, news, online,<br>posted, video, users, page, wrote, shared |
| Comparative Analysis | 0.42% | one, also, however, two, may, dierent, many, used,<br>example, well, often, rst, part, although, another,<br>time, known, fact, various, number |

Table 6: LDA with 200 topics, trained on 100,000 random web documents. A concept for each topic is derived from the related words.

#### A.3 Building the Model

#### A.3.1 Architecture Details

We closely follow the Flamingo architecture introduced in Alayrac et al. (2022). To form the model, we combine a pre-trained image encoder, a pre-trained language model, and add newly initialized parameters of the form of Perceiver blocks (Jaegle et al., 2021) and Transformer-based cross-attentions blocks inserted within the language model every 4 layers.

The pre-trained backbones are frozen during the training, and only the new parameters are updated along with the embeddings of additional tokens.

Following Dehghani et al. (2023), we apply a layer normalization on the projected queries and keys of both the Perceiver and cross-attention blocks, which improved training stability in our early experiments. We use the RMSNorm implementation (Zhang and Sennrich, 2019) for the layer normalization.

| Total | Trainable | Language Model | Vision Model | Perceiver | Cross-Attentions |
|-------|-----------|----------------|--------------|-----------|------------------|
| 9B | 1.5B | 7B | 630M | 126M | 1.4B |
## | 80B | 14B | 65B | 630M | 126M | 13.9B |

Table 7: Breakdown of model parameters. We use LLaMA (Touvron et al., 2023) for the language backbone and OpenCLIP (https://laion.ai/blog/large-openclip/) for the vision backbone.

#### A.3.2 Training Details

We roughly use the same set hyper-parameters for all the runs presented in Figure 6 and Table 2, as detailed in Table 8. The 80B-parameter model training uses a larger batch size and examples of longer sequence length. In all experimental runs, we employ the AdamW optimizer (Loshchilov and Hutter, 2017) and incorporate an auxiliary loss, denoted as $z\_loss = 10^{-3} \times log^2(Z)$ , to encourage the softmax normalizer log(Z) to get closer to 0 (Chowdhery et al., 2022). We use gradient clipping of 1.0.

| Model | Max LR | Adam Betas | Weight Decay | LR Linear Decay | Warmup steps | Batch Size | Seq. Length |
|----------|--------|--------------|--------------|-----------------|-----------------|-------------|-------------|
| Ours 9B | 5e-5 | (0.9, 0.999) | 0.1 | 2.5e-10 | $\frac{2k}{2k}$ | 260K tokens | 512 |
| Ours 80B | 5e-5 | (0.9, 0.999) | 0.1 | 2.5e-10 | | 3.6M tokens | 1024 |

## Table 8: Initial Training Hyper-Parameters

During the training, two models – the 80B-parameter model and the 9B-parameter model trained on LAION + OBELISC – encountered unrecoverable loss spikes. As a remedial measure, we restarted the training from a checkpoint before the spike, shuffled the data and optionally reduced the learning rate. Both models underwent exactly three restarts within the training duration.

The four runs conducted have distinct data mixtures as detailed in Table 9. Each run involves training on a mixture of web documents and image-text pairs. A sampling probability p determines the mixture of these two data sources, which influences the frequency of batches originating from web documents versus those from image-text pairs.

For the 80B-parameter model, the web-document dataset includes both OBELISC and Wikipedia, and the image-text pair dataset included LAION and Public Multimodal Dataset (PMD) (Singh et al., 2022). Given Wikipedia and PMD's higher quality but lower number of examples, we repeat PMD three times and Wikipedia three times.

We used a deduplicated version of LAION (Webster et al., 2023) for all the runs where this dataset was used.

| Model | OBELISC | Wikipedia | LAION | PMD |
|-------------------------------------|---------|-----------|--------|-------|
| 9B-parameter model, OBELISC + LAION | 50% | 0% | 50% | 0% |
| 9B-parameter model, OBELISC only | 100% | 0% | 0% | 0% |
| 9B-parameter model, LAION only | 0% | 0% | 100% | 0% |
| 80B-parameter model | 73.85% | 6.15% | 17.18% | 2.82% |

Table 9: Breakdown of the dataset mixtures used. Percentages correspond to the eective number of tokens seen from each dataset.

#### A.3.3 Compute Details

We train the 9B-parameter models on OBELISC-only and LAION-only on 32 80GB A100 GPUs, and on OBELISC + LAION on 64 80GB A100s, for approximately 6 days. These 3 trainings have the same eective batch size. We train the 80B-parameter model on 512 80GB A100 GPUs for 14 days. The compute infrastructure is hosted on an AWS cluster located in Oregon.

### A.3.4 Evaluation

To ensure fair comparisons against Flamingo (Alayrac et al., 2022), we make sure that we are using the same evaluation splits for each benchmark.

We evaluate the models using an in context learning approach (Brown et al., 2020). For each benchmark, we use the same number of priming examples in the prompt as Flamingo, and to select these examples, we use the Retrieval-based In-Context Example Selection (RICES) method (Yang et al., 2022) with CLIP-ViT-B/32 (Radford et al., 2021) for the image encoder.

We systematically use dierent data splits to select the best-performing prompt (which involves creating validation sets from the training sets, following the methodology proposed by Alayrac et al. (2022)). Table 10 lists the prompts used for each model and task.

For the classication tasks (Hateful Meme (Kiela et al., 2020), IIIT-5k (Mishra et al., 2012)), we use rank classication, i.e. we compute the log probability of the prompt followed by each of the labels individually, and select as the predicted label the one with the highest probability.

For the image captioning (COCO (Lin et al., 2014), Flickr30k (Young et al., 2014)) and visual question answering tasks (VQAv2 (Antol et al., 2015), OKVQA (Marino et al., 2019), TextVQA (Singh et al., 2019)), we report evaluation in the open-ended setup. We use the greedy decoding as we found that it increased the performance. However, we observe that the models tend to generate long answers. To truncate the generated caption or answer, unless specied otherwise, we use a list of manually selected stop words.

The VQA tasks comporting a high proportion of questions with a single-word answer, it was benecial for the 9B-parameter model trained on LAION only to keep the rst word of the generated answer as the prediction to boost its performance.

#### A.3.5 Additional Experimental Results

In Figure 6, we plot the performance per benchmark for the 9B-parameter models trained on LAION only, OBELISC only, and a mixture of OBELISC and LAION. We notice that, even if the training on LAION only is smooth and the loss keeps decreasing (there are no spikes nor instabilities), performance starts to decrease after a certain point on visual question answering benchmarks. We hypothesize that training on image-text pairs can allow a fast association of concepts between images and texts, but fails to teach the model more complex reasoning skills required to solve visual question answering. We tried many dierent prompt candidates in order to boost the performance of the model trained on LAION only for the VQA tasks, without much success.

| Task | Model | Prefx prompt | Example prompt | Stop words |
|---------------------------|---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| VQAv2<br>OKVQA<br>TextVQA | 80B<br>9B LAION only<br>9B OBELISC only<br>9B LAION + OBELISC | {bos_token}Instruction:<br>pro<br>vide an answer to the question.<br>Use the image to answer.\n | Image:{token_around_<br>image}{image_token}{token_<br>around_image}Question:<br>{question} Answer: {answer}\n | "Question",<br>"User",<br>"Image",<br>"task",<br>"What",<br>"Who",<br>"When",<br>"Where", "Why",<br>"How" |
| COCO<br>Flickr30k | 80B<br>9B OBELISC only<br>9B LAION + OBELISC | {bos_token} | Image:{token_around_<br>image}{image_token}{token_<br>around_image}Caption: {cap<br>tion}\n | "Caption",<br>"De<br>scription", "User",<br>"Image", "task" |
| COCO<br>Flickr30k | 9B LAION only | {bos_token}Instruction:<br>pro<br>vide a short caption of the input<br>image.\n | Image:{token_around_<br>image}{image_token}{token_<br>around_image}Image descrip<br>tion: {caption}\n | "Caption",<br>"De<br>scription", "User",<br>"Image", "task" |
| Hateful<br>Memes | 80B<br>9B LAION only<br>9B OBELISC only<br>9B LAION + OBELISC | It's a conversation between a<br>human, the user, and an intel<br>ligent visual AI, Bot. The user<br>sends memes with text written<br>on them, and Bot has to say<br>whether the meme is hateful or<br>not. | {token_around_<br>image}{image_token}{token_<br>around_image}is<br>an<br>image<br>with written "{context}" on<br>it.<br>Is it hateful?<br>Answer:<br>{class_name} | ✗ |
| IIIT5k | 9B LAION only<br>9B OBELISC only<br>9B LAION + OBELISC | ✗ | {token_around_<br>image}{image_token}{token_<br>around_image}"{class_<br>name}"<br>is<br>written<br>on<br>the<br>picture. | ✗ |

Table 10: We select the prompts from a pool of candidates by evaluating 5 intermediate checkpoints on the query and support validation task sets. To form the prompt with N priming examples, we concatenate the prex prompt, followed by N example prompts lled with data from the priming examples, and nally the example prompt lled with data from the example to be evaluated. The data to be replaced is between curly brackets.

On the other hand, we note that training on image-text pairs yield stronger performance on image captioning tasks than on multimodal documents only. This is expected since training and evaluation correspond to the exact same task.

## ![](_page_45_Figure_0.jpeg)

Figure 11: 4-shot performance through the training using LAION only, OBELISC only and a mixture of both. The training sequences from multimodal documents and the packed sequences obtained from image-text pairs have different numbers of images but the same number of tokens. Thus, we plot the performance over two log x-axes.

## A.4 License and Author Statement

We release the dataset under a CC-BY license and Terms of Use that require disclosure of when the dataset is used for the purpose of training models. This license is not intended to replace the licenses of the source content, and any use of content included in the dataset must comply with the original licenses and applicable rights of its data subjects.

The purpose of this statement is to clarify the responsibilities and liabilities associated with the use of this dataset. While we have made every eort to ensure the accuracy and legality of the data contained within this dataset, we cannot guarantee its absolute completeness or correctness.

Therefore, if any rights, legal or otherwise, are violated through this dataset, including but not limited to copyright infringement, privacy violations, or misuse of sensitive information, we, the authors, assume no liability for such violations.

By utilizing this dataset, you agree that any consequences, legal or otherwise, arising from using this dataset will be the user's sole responsibility. You acknowledge that you will exercise due diligence and adhere to all applicable laws, regulations, and ethical guidelines when using the dataset.

By accessing, downloading, or using this dataset, you signify your acceptance of this statement and your commitment to abide by the terms and conditions of the CC-BY license.

If you disagree with the terms of this statement or the CC-BY license, you are not authorized to use this dataset.
