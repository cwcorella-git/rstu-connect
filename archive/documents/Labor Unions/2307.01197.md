---
title: 2307.01197
author: ETH Zürich HKUST EPFL
slug: 2307.01197
reconversion_status: ready_for_reconversion
---
# **Segment Anything Meets Point Tracking**

Frano Rajič<sup>1,3</sup> Lei Ke<sup>1,2</sup> Yu-Wing Tai<sup>2</sup> Chi-Keung Tang<sup>2</sup> Martin Danelljan<sup>1</sup> Fisher Yu<sup>1</sup>

<sup>1</sup>ETH Zürich <sup>2</sup>HKUST <sup>3</sup>EPFL

## ![](_page_0_Figure_3.jpeg)

Figure 1. Segment Anything Meets Point Tracking (SAM-PT). SAM-PT is the first method to utilize sparse point propagation for Video Object Segmentation (VOS). The essence of SAM-PT is to extend SAM [16] with long-term point trackers to effectively operate on videos in a *zero-shot* manner. SAM-PT takes a video as input together with annotations of the target object in the first frame. These annotations are called "query points" and denote either the target object (positive points) or designate non-target segments (negative points). The points are tracked throughout the video using point trackers that propagate the query points to all video frames, producing predicted trajectories and occlusion scores. SAM is subsequently prompted with the non-occluded points in the trajectories as to output a segmentation mask for each video frame independently.

## **Abstract**

The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, employing interactive prompts such as points to generate masks. This paper presents SAM-PT, a method extending SAM's capability to tracking and segmenting anything in dynamic videos. SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation, demonstrating that a SAM-based segmentation tracker can yield strong zero-shot performance across popular video object segmentation benchmarks, including DAVIS, You Tube-VOS, and MOSE. Compared to traditional object-centric mask propagation strategies, we uniquely use point propagation to exploit local structure information that is agnostic to object semantics. We highlight the merits of point-based tracking through direct evaluation on the zeroshot open-world Unidentified Video Objects (UVO) benchmark. To further enhance our approach, we utilize K-

Medoids clustering for point initialization and track both positive and negative points to clearly distinguish the target object. We also employ multiple mask decoding passes for mask refinement and devise a point re-initialization strategy to improve tracking accuracy. Our code integrates different point trackers and video segmentation benchmarks and will be released at https://github.com/SysCV/sampt.

# 1. Introduction

Video segmentation benefits a myriad of applications, including autonomous driving, robotics, and video editing. Despite significant progress made in the past few years with deep neural networks [3, 4, 37, 41], the current methodologies falter when faced with unseen data, particularly in zeroshot settings. These models struggle to maintain consistent performance across diverse scenarios without specific video segmentation data for fine-tuning.

The prevailing methods [3,4] in semi-supervised Video Object Segmentation (VOS) and Video Instance Segmentation (VIS) exhibit performance gaps when dealing with unseen data, particularly in a zero-shot setting, *i.e.*, when these models are transferred to video domains they have not been trained on and encompass object categories that fall outside of the training distribution.

A potential route towards overcoming these challenges lies in adapting successful models in the image segmentation domain for video segmentation tasks. One such promising model is the Segment Anything Model (SAM) [16]. SAM is a powerful foundation model for image segmentation, trained on the large-scale SA-1B dataset, which contains an astounding 11 million images and over 1 billion masks. This extensive training set enables SAM's impressive zero-shot generalization capabilities. The model is highly adaptable, able to produce high-quality masks from single foreground points, and has demonstrated robust performance across a range of downstream tasks under zero-shot transfer protocols. While SAM demonstrates powerful zero-shot capabilities for image segmentation, it is not innately suited for video segmentation tasks.

Recent efforts have been made to adapt SAM for video segmentation. For instance, TAM [40] integrates SAM with the state-of-the-art memory-based mask tracker XMem [4]. Likewise, SAM-Track [6] combines SAM with DeAOT [41]. While these methods mostly recover the performance on in-distribution data, they fall short in preserving the original performance of SAM in more challenging, zero-shot settings. Other methods that do not leverage SAM, such as SegGPT [36], can successfully solve a number of segmentation problems using visual prompting, but still require mask annotation for the first video frame. This problem represents a significant barrier in zero-shot video segmentation, particularly as we seek to develop methods that can easily generalize to unseen scenarios and consistently deliver high-quality segmentation across diverse video domains.

We introduce SAM-PT (Segment Anything Meets Point Tracking), depicted in Fig. 1. This is the first method to utilize sparse point tracking combined with SAM for video segmentation, offering a new perspective on solving the problem. Instead of employing object-centric dense feature matching or mask propagation, we propose a point-driven approach that capitalizes on tracking points using rich local structure information embedded in videos. As a result, it only requires sparse points annotation to denote target object in the first frame and provides better generalization to unseen objects, a strength demonstrated on the open-world UVO [34] benchmark. This approach also helps preserve the inherent flexibility of SAM while extending its capabilities effectively to video segmentation.

## SAM-PT prompts SAM with sparse point trajectories predicted using state-of-the-art point trackers, such as PIPS [11], harnessing their versatility for video segmentation. We identify that initializing points to track using K-Medoids cluster centers from a mask label was the strategy most compatible with prompting SAM. Tracking both positive and negative points enables the clear delineation of target objects from their background. To further refine the output masks, we propose multiple mask decoding passes that integrate both types of points. In addition, we devised a point re-initialization strategy that increases tracking accuracy over time. This approach involves discarding points that have become unreliable or occluded, and adding points from object parts or segments that become visible in later frames, such as when the object rotates.

Notably, our experimental results highlight that SAM-PT competes with existing zero-shot methods [36] or outperforms them [2, 12, 35, 45] on several video segmentation benchmarks. This comes without the need for any video segmentation data during training, underscoring the robustness and adaptability of our approach. SAM-PT holds the potential to enhance progress in video segmentation tasks, particularly in zero-shot scenarios.

#### 2. Related Work

Point Tracking for Video Segmentation. Classical feature extraction and tracking methods such as Lucas-Kanade [23], Tomasi-Kanade [31], Shi-Tomasi [29], SIFT [22], and SURF [1], as well as newer methods such as LIFT [43], Super Point [7], and Super Glue [28], have all demonstrated proficiency in identifying or tracking sparse features and establishing long-range correspondences. However, their effectiveness is confined to a specific set of distinct interest points and they often struggle when applied to non-rigid, dynamic scenes. Flow-based methods, such as RAFT [30], excel in tracking dense points between successive frames. However, they stumble with deriving accurate long-range point trajectories. When chaining flow predictions over time, errors tend to accumulate and lead to drift, while occlusions result in tracking failures.

Significant strides have recently been made in long-term point tracking across video frames, as evinced by methods such as Tap Net [9] and PIPS [11], as well as the concurrent and state-of-the-art Omni Motion [32] and TAPIR [10] techniques. These approaches optimize long-range point trajectories across an entire video, navigating mostly well through periods of occlusion.

Our work stands apart as the first to integrate these successful long-term point tracking methods, utilizing them to guide a promptable foundation model for image segmentation toward performing video segmentation tasks.

**Segment and Track Anything models.** SAM [16] is an innovative image segmentation model for promptable im-

age segmentation, trained on over 1 billion segmentation masks. It showcases remarkable zero-shot generalization abilities and can produce high-quality masks from a single foreground point. To further improve the quality of the masks, especially when segmenting objects with intricate structures, HQ-SAM [15] extends SAM with a learnable high-quality output token which proves efcient in diverse segmentation domains. However, SAM and HQ-SAM cannot be directly used to solve video segmentation tasks.

A few concurrent works extend SAM, for example, TAM [40] and SAM-Track [6] combine SAM with state-ofthe-art mask trackers (such as XMem [4] and DeAOT [42]) to perform interactive video object segmentation. These methods employ SAM for mask initialization or correction and XMem/DeAOT for mask tracking and prediction. Using the pre-trained mask trackers recovers the indistribution performance, but hinders the performance in zero-shot settings. PerSAM [45] also demonstrates the ability to track multiple reference objects in a video. Instead of building an interactive tracking pipeline or SAM netuning, we focus on learning robust associations for diverse objects in zero-shot scenarios.

Zero-shot VOS / VIS. Among the non-SAM-based methods, Painter [35] and its SegGPT [36] extension are another sort of generalist models for solving a variety of image and segmentation tasks. These methods likewise use visual prompting techniques but are inherently different frameworks from SAM. Despite its wide applicability, Painter shows lacking performance in video segmentation tasks. Conversely, SegGPT successfully uses in-context prompting to achieve one-shot video object segmentation performance comparable to ours, also without training on any video data. The training domains, however, notably differ between SegGPT and our method.

STC [12] and DINO [2] also do not use any video segmentation data during training. In the semi-supervised video object segmentation, they take a reference mask as input and perform frame-by-frame feature matching, which propagates the reference mask across the entirety of the video. Our SAM-PT, on the other hand, diverges substantially from these methodologies by adopting point tracking, eschewing the process of frame-by-frame feature matching. Additionally, our method requires only sparse points to represent the target object, rather than a full reference mask, and yields superior performance on conventional semi-supervised video object segmentation benchmarks.

# 3. Method

We propose SAM-PT to adapt SAM, a foundation model for image segmentation, for addressing video segmentation tasks in a zero-shot setting. SAM-PT combines the strengths of existing prominent point trackers, such as PIPS [11] and Tap Net [9], with the powerful image segmentation of SAM to enable tracking of anything in videos. First, Sec. 3.1 briey describes SAM. Sec. 3.2 then introduces our SAM-PT method with its four constituent steps. Finally, Sec. 3.3 analyzes and highlights the method's novelty as the rst point-driven video segmentation method compared to existing works.

# 3.1. Preliminaries: SAM

The Segment Anything Model (SAM) [16] is a novel vision foundation model designed for promptable image segmentation. SAM is trained on the large-scale SA-1B dataset, which contains 11 million images and over 1 billion masks. SA-1B has 400 times more masks than any existing segmentation dataset. This extensive training set facilitates SAM's impressive zero-shot generalization capabilities to new data. SAM has showcased its ability to produce high-quality masks from a single foreground point, and has demonstrated robust generalization capacity on a variety of downstream tasks under a zero-shot transfer protocol using prompt engineering. These tasks include, but are not limited to, edge detection, object proposal generation, and instance segmentation.

SAM comprises of three main components: an image encoder, a exible prompt encoder, and a fast mask decoder. The image encoder is a Vision Transformer (ViT) backbone and processes high-resolution 1024 × 1024 images to generate an image embedding of 64 × 64 spatial size. The prompt encoder takes sparse prompts as input, including points, boxes, and text, or dense prompts such as masks, and translates these prompts into c-dimensional tokens. The lightweight mask decoder then integrates the image and prompt embeddings to predict segmentation masks in real-time, allowing SAM to adapt to diverse prompts with minimal computational overhead.

# 3.2. Ours: SAM-PT

While SAM shows impressive capabilities in image segmentation, it is inherently limited in handling video segmentation tasks. Our Segment Anything Meets Point Tracking (SAM-PT) approach effectively extends SAM to videos, offering robust video segmentation without requiring training on any video segmentation data.

SAM-PT is illustrated in Fig. 2 and is primarily composed of four steps: 1) selecting query points for the rst frame; 2) propagating these points to all video frames using point trackers; 3) using SAM to generate per-frame segmentation masks based on the propagated points; 4) optionally reinitializing the process by sampling query points from the predicted masks. We next elaborate on these four steps.

1) Query Points Selection. The process begins with dening query points in the rst video frame, which either

## ![](_page_3_Figure_0.jpeg)

Figure 2. **Segment Anything Meets Point Tracking (SAM-PT) overview.** The essence of SAM-PT is to extend image segmentation foundation model to effectively operate on videos. SAM-PT has four steps: **1) Query Points Selection.** It starts with first-frame query points which denote the target object (positive points) or designate non-target segments (negative points). These points are provided by the user or derived from a ground truth mask. **2) Point Tracking.** Initiated with the query points, our approach leverages point trackers to propagate the points across video frames, predicting point trajectories and occlusion scores. **3) Segmentation.** The trajectories are then used to prompt the Segment Anything Model (SAM) and output per-frame mask predictions. **4) Point Tracking Reinitialization.** Optionally, the predicted masks are used to reinitialize the query points and restart the process when reaching a prediction horizon h. Re-initialization helps by getting rid of unreliable and occluded points, and adds points from object parts or segments that become visible in later frames, such as when the object rotates.

denote the target object (positive points) or designate the background and non-target objects (negative points). Users can manually and interactively provide query points, or they may be derived from a ground truth mask. For example, in the case of semi-supervised video object segmentation, the ground truth mask is provided for the first frame where the object appears. We derive the query points from ground truth masks using different point sampling techniques by considering their geometrical locations or feature dissimilarities, as depicted in Fig. 3. These sampling techniques are:

- Random Sampling: An intuitive approach where query points are randomly selected from the ground truth mask.
- **K-Medoids Sampling:** This technique takes the cluster centers of K-Medoids clustering [26] as query points to ensure good coverage of different parts of the object and robustness to noise and outliers.
- **Shi-Tomasi Sampling:** This method extracts Shi-Tomasi corner points from the image under the mask as they have been shown to be good features to track [29].
- **Mixed Sampling:** A hybrid method combining the above techniques since it might benefit from the unique strengths of each.

While each method contributes distinct characteristics that influence the model's performance, our ablation study reveals that K-Medoids sampling yields the best results with a good full coverage of various segments of the complete object. Shi-Tomasi sampling follows closely, indicating their respective strengths in this context. The selection and arrangement of these points considerably affect the overall video segmentation performance, thus determining the optimal method is crucial.

- 2) Point Tracking. Initiated with the query points, we employ robust point trackers to propagate the points across all frames in the video, resulting in point trajectories and occlusion scores. We adopt the state-of-the-art point tracker PIPS [11] to propagate the points as PIPS shows moderate robustness toward long-term tracking challenges such as object occlusion and re-appearance. This is also shown more effective than methods such as chained optical flow propagation or first-frame correspondences in our experiment section.
- **3) Segmentation.** In the predicted trajectories, the non-occluded points serve as indicators of where the target object is throughout the video. This allows us to use the non-occluded points to prompt SAM, as illustrated in Fig. 4, and leverage its inherent generalization ability to output per-frame segmentation mask predictions. Unlike conventional tracking methods that require training or fine-tuning on video segmentation data, our approach excels in zero-shot video segmentation tasks.

We combine positive and negative points by calling SAM in two passes. In the initial pass, we prompt SAM exclu-

## ![](_page_4_Figure_0.jpeg)

Figure 3. **Positive Point Sampling.** For an image paired with either a ground truth or predicted segmentation mask, positive points are sampled from within the mask area using one of the following point sampling methods: Random, K-Medoids [26], Shi-Tomasi [29], or Mixed. Notably, Random Sampling and K-Medoids Sampling only require the segmentation mask for input, not the corresponding input image. For negative points, we always use Mixed Sampling on the target object's background mask.

## ![](_page_4_Figure_2.jpeg)

Figure 4. **Interacting with SAM in SAM-PT.** In the first pass, SAM is prompted exclusively with positive points to define the object's initial localization. In the second pass, both positive and negative points along with the previous mask prediction are fed to the same mask decoder for further mask refinement. The negative points remove segments from the background and neighboring objects and notably help in cases when the point tracker mistakenly predicts positive points off the target object. The second pass is repeated iteratively to get a refined segmentation mask.

sively with positive points to define the object's initial localization. Subsequently, in the second pass, we prompt SAM with both positive and negative points along with the previous mask prediction. Negative points provide a more nuanced distinction between the object and the background and help by removing wrongly segmented areas.

Lastly, we execute a variable number of mask refinement iterations by repeating the second pass. This utilizes SAM's capacity to refine vague masks into more precise ones. Based on our ablation study, this step notably improves video object segmentation performance.

4) Point Tracking Reinitialization. We optionally execute a reinitialization of the query points using the predicted masks once a prediction horizon of h = 8 frames is reached, and denote the variant as SAM-PT-reinit. Upon reaching this horizon, we have h predicted masks and will take the last predicted mask to sample new points. At this stage, all previous points are discarded and substituted with the newly sampled points. Following this, steps 1) through 4) are repeated with the new points, starting from the horizon timestep where reinitialization occurs. The steps are iteratively executed until the entire video is processed. The reinitialization process serves to enhance tracking accuracy over time by discarding points that have become unreliable or occluded, while incorporating points from object segments that become visible later in the video. Other reinitialization variants are discussed in Appendix A and included in the ablation study in Sec. 4.3.

# 3.3. SAM-PT vs. Object-centric Mask Propagation

With sparse point tracking combined with prompting SAM, SAM-PT distinguishes itself from traditional video segmentation methods that depend on dense object mask propagation, as noted in Tab. 1. To propagate the first-frame GT label to the remaining video frames, traditional techniques commonly use feature matching with masks cached to a mask memory [4,6,40,41], frame-by-frame feature matching [2,12], feature matching with the first-frame mask [45], optical flow [39], and, recently, in-context visual prompting [35,36]. In contrast, SAM-PT introduces a unique approach to video object segmentation, employing the robust combination of point tracking with SAM, which is inherently designed to operate on sparse point prompts.

The point propagation strategy of SAM-PT offers several advantages over traditional object-centric tracking methods. First, point propagation exploits local structure context that is agnostic to global object semantics. This enhances our model's capability for zero-shot generalization, an advantage that, coupled with SAM's inherent generalization power, allows for tracking diverse objects in diverse environments, such as on the UVO benchmark. Second, SAM-PT allows for a more compact object representation with sparse points, capturing enough information to characterize the object's segments/parts effectively. Finally, the use of points is naturally compatible with SAM, an image segmentation foundation model trained to operate on sparse point prompts, offering an integrated solution that aligns well with the intrinsic capacities of the underlying model.

Comparing SAM-PT with conventional methods in Tab. 1, SAM-PT emerges as superior or comparable to methods that refrain from utilizing video segmentation data during training. However, there is a performance gap that exists between such methods and those that leverage video segmentation training data in the same domain, such as

Table 1. Comparison of semi-supervised Video Object Segmentation (VOS) methods with respect to mask annotation requirements and propagation techniques. Our method, SAM-PT, is the first method for VOS that uses sparse point propagation. With such compact mask representation, we achieve the highest $\mathcal{J}\&\mathcal{F}$ scores on the DAVIS 2016 and 2017 validation subsets among methods that do not utilize any video segmentation data during training. The methods are compared based on their use of video mask data during training, whether they are evaluated zeroshot on DAVIS, what first-frame labels they require, and what label propagation technique they employ.

| Method | Video<br>Mask | Zero-<br>Shot | Frame<br>Init. | Propagation | DAVIS<br>2016 | DAVIS<br>2017 |
|---------------|---------------|---------------|----------------|---------------------|---------------|---------------|
| Siam Mask [33] | / | X | Box | Feature Correlation | 69.8 | 56.4 |
| QMRA [19] | / | X | Box | Feature Correlation | 85.9 | 71.9 |
| TAM [40] | / | X | Points | Feature Matching | 88.4 | - |
| SAM-Track [6] | / | X | Points | Feature Matching | 92.0 | - |
| XMem [4] | ✓ | X | Mask | Feature Matching | 92.0 | 87.7 |
| DeAOT [41] | ✓ | × | Mask | Feature Matching | 92.9 | 86.2 |
| Painter [35] | Х | 1 | Mask | Mask Prompting | - | 34.6 |
| STC [12] | X | 1 | Mask | Feature Matching | - | 67.6 |
| DINO [2] | X | 1 | Mask | Feature Matching | - | 71.4 |
| PerSAM-F [45] | X | / | Mask | Feature Matching | - | 71.9 |
| SegGPT [36] | X | 1 | Mask | Mask Prompting | 82.3 | 75.6 |
| SAM-PT (ours) | Х | 1 | Points | Points Prompting | 83.1 | 76.6 |

XMem [4] or DeAOT [41]. Further, the potential of our model extends beyond video object segmentation to other tasks, such as Video Instance Segmentation (VIS), thanks to the inherent flexibility of our point propagation strategy.

In summary, SAM-PT is the first method that introduces sparse point propagation combined with prompting a image segmentation foundation model to perform zero-shot video object segmentation. It provides a fresh perspective and adds a new dimension to the study of video object segmentation.

# 4. Experiments

#### 4.1. Datasets

In the following subsections, we present an overview of the datasets used in our study. Section 4.1.1 provides a brief introduction to the Video Object Segmentation task and outlines the specific datasets we utilize for this task. Similarly, Section 4.1.2 discusses the Video Instance Segmentation task and the dataset associated with it.

#### 4.1.1 Video Object Segmentation

Video Object Segmentation (VOS) refers to the process of segmenting a specific object across an entire video sequence. Semi-supervised VOS (also known as one-shot VOS or semi-automatic VOS) is the primary setting for VOS on which we evaluate our method. In this setting, the ground truth object mask of the first frame is provided, and the task is to predict the masks for subsequent frames. Alternatively, the first frame label can be a bounding box in-

stead of a segmentation mask, or a set of points as is the case for our method. We evaluate our method on four VOS datasets: DAVIS 2016, DAVIS 2017 [27], You Tube-VOS 2018 [38], and MOSE 2023 [8].

**DAVIS 2016 [27].** DAVIS 2016 is a single-object VOS benchmark, consisting of 20 highly diverse video sequences, each of which possesses well-annotated segmentation masks.

**DAVIS 2017 [27].** A multi-object extension of its 2016 version, DAVIS 2017 includes 60 videos in the training set and 30 videos in the validation set, comprising a total of 197 different objects. The video scenarios within this dataset are small but diverse.

**You Tube-VOS 2018 [38].** You Tube-VOS 2018 is a large-scale dataset collected from You Tube, comprising 3471 training videos encompassing 65 categories and 474 validation videos with an additional 26 unseen categories. The diversity in categories and the inclusion of seen and unseen classes allow for a comprehensive evaluation of a given model's generalization capability.

MOSE 2023 [8]. MOSE 2023 is a recently introduced dataset designed for multiple object segmentation and tracking in complex scenes. This dataset is replete with challenges such as the transient visibility of objects, the presence of minute or less noticeable entities, extensive occlusions, and scenes with a high object density. By design, each video in this dataset must contain multiple objects so that occlusions must be present, and objects must show sufficient motion, as opposed to being stationary or showing little movement.

**Metrics.** We report the standard evaluation metrics for video object segmentation [8,27,38], including region similarity $\mathcal{J}$ , contour accuracy $\mathcal{F}$ , and their average, $\mathcal{J}\&\mathcal{F}$ .

# 4.1.2 Video Instance Segmentation

Video Instance Segmentation (VIS) is a task that combines object detection, instance segmentation, and object tracking across video frames, which aims to identify and segment each object instance over the whole video sequence. This is a much less explored task compared to VOS but has been gaining interest. We evaluate our method on the densevideo task of the UVO v1.0 [34] dataset.

**UVO v1.0.** The Unidentified Video Objects (UVO) dataset is designed to recognize and segment all objects regardless of the categories, even those unseen during training, thereby focusing on VIS in the open world. Each video in UVO features on average 12.3 object annotations, a considerable increase from previous datasets having only 2 or 3 objects per video on average. UVO sources its videos from the Kinetics-400 [14] dataset and contains three different splits: Frame Set, Video Sparse Set, and Video Dense Set. The Video Dense Set consists of 3-second clips annotated densely at 30fps and tracked over time. The primary goal of Video Dense Set is to study video open-world segmentation. Objects identifiable under COCO categories carry their respective COCO labels, while ambiguous objects or those outside the COCO taxonomy are labeled as "other". This meticulous and exhaustive annotation structure makes the Video Dense Set ideal for research areas that require an understanding of videos in a dense and comprehensive manner, such as robotics, autonomous driving, and augmented-reality applications.

Metrics. We evaluate our method using standard evaluation metrics in image instance segmentation, adapted for video instance segmentation [38]. These include Average Precision (AP) and Average Recall (AR) IoU-based metrics. Given that each instance in a *video* comprises a sequence of masks, unlike image instance segmentation, IoU computation is carried out not only in the spatial dimensions but also in the temporal dimension. This implies that the sum of intersections at every single frame is divided by the sum of unions at every single frame. These metrics are generally computed on a per-category basis and subsequently averaged across all categories. However, we work with the class-agnostic version of UVO.

#### **4.2. Implementation Details**

**Training Data.** For our experiments, we use pre-trained checkpoints provided by the respective authors for both PIPS [11] and SAM. PIPS is trained exclusively on a synthetic dataset, Flying Things++ [11], derived from the Flying Things [24] optical flow dataset. This dataset includes multi-frame amodal trajectories with synthetic occlusions caused by moving objects. SAM, on the other hand, has been trained on the large-scale SA-1B dataset, the largest image segmentation dataset to date, with over 1 billion masks on 11M licensed and privacy-respecting images. It is noteworthy that neither of these datasets includes video segmentation data, and they do not overlap with any of our evaluation data. This effectively positions our model in a zero-shot video segmentation setting.

**Model Variations.** Our experiments led to two optimal model hyperparameters, distinguished as SAM-PT (without reinitialization) and SAM-PT-reinit (with reinitialization). These configurations were derived from our ablation study in Sec. 4.3. However, we found that using iterative refinement negatively impacted both SAM-PT and

SAM-PT-reinit on the MOSE dataset, and likewise hindered SAM-PT-reinit on the You Tube-VOS dataset. Consequently, iterative refinement was deactivated for these specific datasets. For DAVIS, we additionally report results for replacing SAM with HQ-SAM [15] and denote the model variants as HQ-SAM-PT and HQ-SAM-PT-reinit. The HQ-SAM variants use 3 iterative refinement iterations instead of 12 iterations.

**VOS Evaluation.** When evaluating on VOS, we use the provided ground truth mask for the first frame to sample the query points required by our method. Then, we give only the sampled points as input to our method, not the mask. For all datasets, we use the full-resolution data and resize it to the longest side of 1024 to match SAM's input resolution.

VIS Evaluation. For evaluating our method on the VIS task, we leverage SAM's automatic mask generation capacity to generate up to 100 mask proposals for the initial frame. We then propagate these proposed masks throughout the entire video sequence using our method. We evaluate TAM [40], a concurrent method we compare against, in the same manner. Our mask proposal generation process is currently simplistic and does not create any proposals for subsequent video frames. Consequently, it cannot identify objects that emerge in later frames, placing it at a disadvantage compared to VIS methods that are capable of doing so. Despite this limitation, our approach provides a consistent platform for comparing zero-shot methods in terms of how effectively they propagate diverse mask proposals from the first frame.

### 4.3. Ablation Study

We conducted detailed ablation experiments on the DAVIS 2017 validation subset to validate various components and designs of SAM-PT. We employed SAM's ViT-H as the backbone, for all tests. Each aspect was examined sequentially, integrating the optimal settings obtained from prior experiments. To ensure statistical soundness, multiple iterations of each experiment were carried out (between 4 and 12 runs per setup), with findings represented as mean and standard deviation across these runs.

While these results provide insight, there may be a risk of overfitting due to our limited validation dataset. While we endeavored to maintain a consistent evaluation protocol, future research should aim for a larger validation set, possibly derived from the You Tube VOS 2018 train dataset, to mitigate this concern.

**Query Point Sampling** Fig. 5 illustrates that the number of positive points and the choice of point selection methods significantly influence performance. Using 8 points

## ![](_page_7_Figure_0.jpeg)

Figure 5. Query Point Sampling Ablation Study. We report mean $\mathcal{J}\&\mathcal{F}$ scores along with the standard deviation on the validation subset of DAVIS 2017 for different number of positive points per mask and different point selection methods. Using 8 positive points per mask leads to a 40-point performance boost compared to using a single point. Given 8 positive points, K-Medoids and Shi-Tomasi perform comparably well as point selection methods.

per mask showed a remarkable 40-point performance enhancement compared to a single point. This substantiates the argument that a single positive point is inadequate for prompting SAM as it often results in the segmentation of partial objects only. Among the point selection methods, K-Medoids and Shi-Tomasi produced comparable results, with a slight preference towards K-Medoids owing to its marginally higher mean score and resilience to the number of positive points per mask.

Point Tracking. Tab. 2a shows that PIPS [11] demonstrated superior performance over Tap Net [9], Super-Glue [28], and RAFT [30]. Tap Net's limitations stem from its lack of effective time consistency and its training on 256x256 images, which hampered its performance with higher-resolution images. Super Glue, while proficient in matching sparse features across rigid scenes, grapples with effectively matching points from the reference frame in dynamic scenes, particularly under object deformations. RAFT, being an optical flow model, faced difficulties handling occlusions. Although PIPS's prior use in our experiments may have offered some hyperparameter advantages, its superior performance is primarily attributable to its more robust design that emphasizes trajectory modeling over eight subsequent frames. This approach fosters the generation of coherent point trajectories and enhances occlusion detection.

**Negative Points.** Tab. 2b highlights that incorporating negative points had a favorable impact, particularly in reducing segmentation errors when points deviated from the target object. The addition of negative points empowered SAM to better handle the point trackers' failure cases, leading to improved segmentation and a 1.8-point enhancement over the non-use of negative points. Note that throughout all experiments, we always used the mixed point sampling

Table 2. **Point Tracker and SAM Configuration Ablation Study Results.** Using the best parameters from the ablation study in Fig. 5, we report the mean performance (with standard deviation) on the validation subset of DAVIS 2017 to study the impact of (a) different point trackers, (b) the number of negative points per mask, (c) the use of iterative refinement and (d) patch similarity filtering. We find that the best configuration uses PIPS as the point tracker, 1 negative point per mask, 12 refinement iterations, and no patch similarity filtering. PT: point tracker. NP: negative points per mask. IRI: iterative refinement iterations. PS: point similarity filtering threshold.

| SAM-PT ( | Config | uratio | n | DAVIS 2017 Validation [27] | | | | |
|-------------------------------------|----------|--------|-------|----------------------------|----------------|----------------|------|--|
| PT | NP | IRI | PS | $\mathcal{J}\&\mathcal{F}$ | $\mathcal{J}$ | F | Gain | |
| (a) point tracker | | | | | | | | |
| RAFT [30] | 0 | 0 | Х | $63.0 \pm 0.6$ | $60.7 \pm 0.6$ | $65.4 \pm 0.5$ | | |
| Super Glue [28] | 0 | 0 | Х | $21.7 \pm 2.8$ | $19.6 \pm 2.1$ | $23.8 \pm 3.4$ | | |
| Super Glue [28] | 0 | 3 | Х | $28.4 \pm 3.1$ | $24.7 \pm 2.4$ | $32.0 \pm 3.8$ | | |
| Tap Net [9] | 0 | 0 | X | $60.9 \pm 0.2$ | $58.2 \pm 0.3$ | $63.5 \pm 0.2$ | | |
| PIPS [11] | 0 | 0 | X | $72.3 \pm 1.2$ | $70.4 \pm 1.3$ | $74.3 \pm 1.1$ | +9.3 | |
| (b) negative points p | per ma | sk | | | | | | |
| PIPS | 0 | 0 | Х | $72.3 \pm 1.2$ | $70.4 \pm 1.3$ | $74.3 \pm 1.1$ | | |
| PIPS | 1 | 0 | Х | $74.1 \pm 0.7$ | $72.1 \pm 0.6$ | $76.1 \pm 0.7$ | +1.8 | |
| PIPS | 8 | 0 | X | $74.0 \pm 0.8$ | $71.9 \pm 0.8$ | $76.0 \pm 0.9$ | | |
| PIPS | 16 | 0 | X | $73.4 \pm 0.6$ | $71.4 \pm 0.6$ | $75.3 \pm 0.6$ | | |
| PIPS | 72 | 0 | X | $72.2 \pm 0.4$ | $70.3 \pm 0.4$ | $74.0 \pm 0.4$ | | |
| (c) iterative refinement iterations | | | | | | | | |
| PIPS | 1 | 0 | Х | $74.1 \pm 0.7$ | $72.1 \pm 0.6$ | $76.1 \pm 0.7$ | | |
| PIPS | 1 | 1 | X | $75.7 \pm 0.7$ | $73.4 \pm 0.7$ | $78.1 \pm 0.6$ | | |
| PIPS | 1 | 3 | X | $76.0 \pm 0.6$ | $73.4 \pm 0.7$ | $78.6 \pm 0.7$ | | |
| PIPS | 1 | 12 | X | $76.3 \pm 0.6$ | $73.6 \pm 0.6$ | $78.9 \pm 0.6$ | +2.2 | |
| (d) patch similarity | filterir | ıg | | | | | | |
| PIPS | 1 | 12 | Х | $76.3 \pm 0.6$ | $73.6 \pm 0.6$ | $78.9 \pm 0.6$ | none | |
| PIPS | 1 | 12 | 0.002 | $72.7 \pm 2.0$ | $70.2 \pm 1.8$ | $75.2 \pm 2.1$ | | |
| PIPS | 1 | 12 | 0.01 | $70.7 \pm 2.0$ | $68.3 \pm 1.8$ | $73.2 \pm 2.1$ | | |

method for sampling negative points which amounts to using random sampling when there is only one negative point per mask.

**Iterative Refinement.** The iterative refinement approach contributed to higher-quality masks and mitigated the impact of artifacts in SAM's output. Tab. 2c displays that this yielded an improvement of 2.2 points over the non-refinement approach.

**Patch Similarity.** Our initial findings in Tab. 2d suggest that using patch similarity to filter unreliable tracking points was overly restrictive in our context, leading to substantial deletion of points. Although it did not prove beneficial in our current setup, this aspect certainly warrants further exploration, particularly in scenarios involving point re-initialization.

**Reinitialization.** Fig. 6 presents the performance of different reinitialization variants. In Tab. 5 and Tab. 6, we also show it brings 2.5 and 2.0 points improvements on MOSE and UVO benchmarks respectively. The re-initialization process enhanced robustness against points falling off ob-

## ![](_page_8_Figure_0.jpeg)

Figure 6. **Reinitialization Ablation Study.** Mean $\mathcal{J}\&\mathcal{F}$ scores (with std. dev.) on the validation subset of DAVIS 2017 for different reinitialization variants and configurations. The best result is achieved by reinitialization variant A, 12 refinement iterations, and 72 negative points per mask. The reinitialization variants differ in how the timestep at which the points get reinitialized is chosen, see Appendix A for more details. Although using reinitialization improves the performance only marginally on the validation subset, this strategy demonstrates substantial improvement on the MOSE 2023 (Tab. 5) and UVO (Tab. 6) datasets.

jects. By reinitializing all points based on the current mask prediction, we account for errors in point tracker outputs by discarding incorrect points and starting fresh from the current mask prediction. However, this assumes that we trust the currently outputted mask, which may not always be the case and sometimes leads to failures.

In summary, our best-performing SAM-PT model employs K-Medoids for point selection with 8 points per mask, PIPS for point tracking, a single negative point per mask, and employs 12 iterations for iterative refinement without patch similarity filtering. Meanwhile, using reinitialization achieved optimum performance with 12 refinement iterations and 72 negative points per mask.

# 4.4. Comparison with State-of-the-art Methods

All reported results were computed with official tools or official evaluation servers. Sec. 4.4.1 reports Video Object Segmentation results, including qualitative results on unseen web videos. Sec. 4.4.2 reports Video Instance Segmentation results.

#### 4.4.1 Video Object Segmentation

**Performance Overview.** Our proposed method outperforms others that have not been trained on any video object segmentation data on the DAVIS 2017 dataset, as reflected in Tab. 3. A mean $\mathcal{J}\&\mathcal{F}$ score of 76.6 points exceeds the PerSAM-F by 4.7 points and the SegGPT generalist model by a single point. The experiments were repeated 8 times for statistical robustness, and we report the mean and standard deviation of our method's performance.

Table 3. Quantitative results in semi-supervised VOS on the validation subset of DAVIS 2017.

| | DAVIS 2017 Validation [27] | | | | | |
|---------------------------------------|----------------------------|----------------|----------------|--|--|--|
| Method | $\mathcal{J}\&\mathcal{F}$ | $\mathcal J$ | $\mathcal{F}$ | | | |
| (a) trained on video segmentation d | ata | | | | | |
| AGSS [20] | 67.4 | 64.9 | 69.9 | | | |
| AGAME [13] | 70.0 | 67.2 | 72.7 | | | |
| AFB-URR [18] | 74.6 | 73.0 | 76.1 | | | |
| STM [25] | 81.8 | 79.2 | 84.3 | | | |
| SWEM [21] | 84.3 | 81.2 | 87.4 | | | |
| RDE [17] | 86.1 | 82.1 | 90.0 | | | |
| SwinB-DeAOT-L [41] | 86.2 | 83.1 | 89.2 | | | |
| XMem [4] | 87.7 | 84.0 | 91.4 | | | |
| (b) not trained on video segmentation | on data (zero-s | shot) | | | | |
| Painter [35] | 34.6 | 28.5 | 40.8 | | | |
| DINO [2] | 71.4 | 67.9 | 74.9 | | | |
| SegGPT [36] | 75.6 | 72.5 | 78.6 | | | |
| PerSAM-F [45] | 71.9 | 69.0 | 74.8 | | | |
| SAM-PT (ours) | $76.3 \pm 0.6$ | $73.6 \pm 0.6$ | $78.9 \pm 0.6$ | | | |
| SAM-PT-reinit (ours) | $76.6 \pm 0.7$ | $74.4 \pm 0.8$ | $78.9 \pm 0.6$ | | | |
| HQ-SAM-PT [15] (ours) | $77.2 \pm 0.5$ | $74.7 \pm 0.5$ | $79.8 \pm 0.4$ | | | |
| HQ-SAM-PT-reinit [15] (ours) | $77.0 \pm 0.7$ | $74.8 \pm 0.8$ | $79.2 \pm 0.6$ | | | |

We also outperform PerSAM-F on the You Tube-VOS 2018 and MOSE 2023 datasets, achieving mean scores of 67.0 and 41.0 as shown in Tabs. 4 and 5. However, with different mask training data, our performance falls short when compared to SegGPT on the two datasets.

Qualitative Analysis. Visualizations of successful video segmentation on DAVIS 2017 for SAM-PT and SAM-PT-reinit can be seen in Fig. 7a and Fig. 7b respectively. Notably, Fig. 8 presents successful video segmentation on unseen web videos – clips from the "Avatar: The Last Airbender" anime-influenced animated television series, demonstrating the zero-shot capabilities of our method.

Limitations and Challenges. Despite the competitive zero-shot performance, certain limitations persist, primarily due to the limitations of our point tracker in handling occlusion, small objects, motion blur, and re-identification. In such scenarios, the point tracker's errors propagate into future video frames. Fig. 7c illustrates these problematic instances on DAVIS 2017, while Fig. 9 presents additional cases on "Avatar: The Last Airbender" clips. Although using point re-initialization and negative points somewhat alleviates the failures of the point tracker, they still prevent the performance from being on par with methods trained on video data.

#### 4.4.2 Video Instance Segmentation

**Results and Analysis.** Given the same mask proposals, SAM-PT outperforms TAM [40] significantly even though SAM-PT was not trained on any video segmentation

## ![](_page_9_Figure_0.jpeg)

Figure 7. Visualization of SAM-PT on the DAVIS 2017 Validation [27] dataset. Given the rst-frame masks, we sample 8 positive points and either 1 (SAM-PT) or 72 (SAM-PT-reinit) negative points per object to initialize SAM-PT and SAM-PT-reinit. Circles denote positive points, crosses denote negative points, and red symbols (circle and cross) denote that the point was predicted to be occluded.

## ![](_page_10_Figure_0.jpeg)

Figure 8. Successful segmentation using SAM-PT on short clips from "Avatar: The Last Airbender". Although our method has never seen data from Avatar, an anime-inuenced animated television series, it segments and tracks various objects in short clips.

## ![](_page_10_Figure_2.jpeg)

Figure 9. Challenging scenarios for SAM-PT on short clips from "Avatar: The Last Airbender". These cases illustrate instances where our model struggles when faced with point tracking failures that are the result of incorrectly predicting the point at a similar-looking segment or when faced with object occlusions and disappearing objects.

data. TAM is a concurrent approach combining SAM and XMem [4], where XMem was pre-trained on BL30K [5] and trained on DAVIS and You Tube-VOS, but not on UVO. On the other hand, SAM-PT combines SAM with the PIPS point tracking method, both of which have not been trained on video segmentation tasks.

Table 4. Quantitative results in semi-supervised VOS on the validation subset of You Tube-VOS 2018. Metrics are reported separately for "seen" and "unseen" classes, with $\mathcal G$ being the overall average score over the metrics. Note that SegGPT and SAM-PT adopt completely different training data. $\clubsuit$ : our reproduced result using the official code of [45].

| | You Tube-VOS 2018 Validation [38] | | | | | | | |
|------------------------------|----------------------------------|-----------------|-----------------|-----------------|-----------------|--|--|--|
| Method | G | $\mathcal{J}_s$ | $\mathcal{F}_s$ | $\mathcal{J}_u$ | $\mathcal{F}_u$ | | | |
| (a) trained on video segme | ntation data | | | | | | | |
| AGAME [13] | 66.0 | 66.9 | - | 61.2 | - | | | |
| AGSS [20] | 71.3 | 71.3 | 65.5 | 75.2 | 73.1 | | | |
| STM [25] | 79.4 | 79.7 | 84.2 | 72.8 | 80.9 | | | |
| AFB-URR [18] | 79.6 | 78.8 | 83.1 | 74.1 | 82.6 | | | |
| RDE [17] | 83.3 | 81.9 | 86.3 | 78.0 | 86.9 | | | |
| SWEM [21] | 82.8 | 82.4 | 86.9 | 77.1 | 85.0 | | | |
| XMem [4] | 86.1 | 85.1 | 89.8 | 80.3 | 89.2 | | | |
| SwinB-DeAOT-L [41] | 86.2 | 85.6 | 90.6 | 80.0 | 88.4 | | | |
| (b) not trained on video seg | gmentation da | ta (zero-shot) | | | | | | |
| Painter [35] | 24.1 | 27.6 | 35.8 | 14.3 | 18.7 | | | |
| SegGPT [36] | 74.7 | 75.1 | 80.2 | 67.4 | 75.9 | | | |
| PerSAM-F <b>♠</b> [45] | 54.4 | 53.9 | 56.4 | 50.7 | 56.6 | | | |
| SAM-PT (ours) | $67.0 \pm 0.3$ | $68.6 \pm 0.2$ | $71.2 \pm 0.1$ | $61.0 \pm 0.5$ | $67.4 \pm 0.4$ | | | |
| SAM-PT-reinit (ours) | $67.5 \pm 0.2$ | $69.0 \pm 0.4$ | $69.9 \pm 0.3$ | $63.2 \pm 0.4$ | $67.8 \pm 0.5$ | | | |

Table 5. Quantitative results in semi-supervised VOS on the validation subset of MOSE 2023 [8]. ♠: our reproduced result using the official code of [45].

| | MOSE 2023 Validation [8] | | | | | |
|-----------------------------|----------------------------|----------------|----------------|--|--|--|
| Method | $\mathcal{J}\&\mathcal{F}$ | $\mathcal J$ | $\mathcal{F}$ | | | |
| (a) trained on video segme | entation data | | | | | |
| RDE [17] | 48.8 | 44.6 | 52.9 | | | |
| SWEM [21] | 50.9 | 46.8 | 54.9 | | | |
| XMem [4] | 57.6 | 53.3 | 62.0 | | | |
| DeAOT [41] | 59.4 | 55.1 | 63.8 | | | |
| (b) not trained on video se | gmentation d | ata (zero-shot | ) | | | |
| Painter [35] | 14.5 | 10.4 | 18.5 | | | |
| SegGPT [36] | 45.1 | 42.2 | 48.0 | | | |
| PerSAM-F <b>♠</b> [45] | 23.3 | 19.8 | 26.8 | | | |
| SAM-PT (ours) | $38.5 \pm 0.2$ | $34.9 \pm 0.3$ | $42.1 \pm 0.2$ | | | |
| SAM-PT-reinit (ours) | $41.0 \pm 0.5$ | $38.5 \pm 0.5$ | $43.5 \pm 0.5$ | | | |

### 5. Conclusion

We present SAM-PT, an innovative solution that extends SAM's segmentation ability from static images to dynamic videos. Integrated with long-term point trackers, our approach demonstrates strong performance across several benchmarks including DAVIS, You Tube-VOS, MOSE, and UVO. While our method has limitations such as difficulty handling occlusions, small objects, and motion blur, and inconsistencies in mask predictions, it contributes a simple and effective new point-based perspective to video object segmentation research. By illustrating a promising way to extend foundational models like SAM into the video domain, our research provides a potential pathway for advancements in diverse applications from autonomous driv-

Table 6. **Results on the validation split of UVO [34] Video-Dense Set v1.0.** SAM-PT outperforms TAM [40] even though the former was not trained on any video segmentation data. TAM is a concurrent approach combining SAM [16] and XMem [4], where XMem was pre-trained on BL30K [5] and trained on DAVIS [27] and You Tube-VOS [38], but not on UVO. On the other hand, SAM-PT combines SAM with the PIPS point tracking method, both of which have not been trained on any video segmentation tasks.

| Method | Propagation | AR100 | ARs | ARm | ARl | AP |
|------------------------------|------------------------|----------------|----------------|----------------|----------------|---------------|
| (a) trained on video segmen | tation data, including | g UVO's train | ing subset | | | |
| Mask2Former VIS [44] | N/A | 35.4 | - | - | - | 27.3 |
| ROVIS [44] | N/A | 41.2 | _ | _ | - | 32.7 |
| (b) trained on video segmen | tation data | | | | | |
| TAM [40] | Feature Matching | 24.1 | 21.1 | 32.9 | 31.1 | 1.7 |
| (c) not trained on video seg | mentation data (zero- | shot) | | | | |
| SAM-PT (ours) | Points Prompting | $28.8 \pm 0.1$ | $23.3 \pm 0.1$ | $40.8 \pm 0.2$ | $48.3 \pm 0.6$ | $6.7 \pm 0.2$ |
| SAM-PT-reinit (ours) | Points Prompting | 30.8 | 25.1 | 44.1 | 49.2 | 6.5 |

ing to video labeling. Furthermore, the future incorporation of more advanced point trackers can enhance the performance of SAM-PT.

#### References

- [1] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. Speeded-up robust features (surf). *Computer vision and image understanding*, 110(3):346–359, 2008. 2
- [2] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In *ICCV*, 2021. 2, 3, 5, 6, 9
- [3] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar, and Alexander G. Schwing. Mask2former for video instance segmentation. ar Xiv preprint ar Xiv: 2112.10764, 2021. 1, 2
- [4] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model. In *ECCV*, 2022. 1, 2, 3, 5, 6, 9, 11, 12
- [5] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modular interactive video object segmentation: Interaction-to-mask, propagation and difference-aware fusion. In CVPR, 2021. 11, 12
- [6] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. ar Xiv preprint ar Xiv:2305.06558, 2023. 2, 3, 5, 6
- [7] Daniel De Tone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In CVPRW, 2018. 2
- [8] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip H. S. Torr, and Song Bai. Mose: A new dataset for video object segmentation in complex scenes. ar Xiv preprint ar Xiv: 2302.01872, 2023. 6, 12
- [9] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: A benchmark for tracking any point in a video. In *NeurIPS*, 2022. 2, 3, 8

- [10] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal renement. ar Xiv preprint ar Xiv: 2306.08637, 2023. 2
- [11] Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In ECCV, 2022. 2, 3, 4, 7, 8
- [12] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as a contrastive random walk. In NeurIPS, 2020. 2, 3, 5, 6
- [13] Joakim Johnander, Martin Danelljan, Emil Brissman, Fahad Shahbaz Khan, and Michael Felsberg. A generative appearance model for end-to-end video object segmentation. In CVPR, 2019. 9, 12
- [14] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. ar Xiv preprint ar Xiv: 1705.06950, 2017. 7
- [15] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. ar Xiv preprint ar Xiv: 2306.01567, 2023. 3, 7, 9
- [16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. ar Xiv preprint ar Xiv:2304.02643, 2023. 1, 2, 3, 12
- [17] Mingxing Li, Li Hu, Zhiwei Xiong, Bang Zhang, Pan Pan, and Dong Liu. Recurrent dynamic embedding for video object segmentation. In CVPR, 2022. 9, 12
- [18] Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region renement. NeurIPS, 2020. 9, 12
- [19] Fanchao Lin, Hongtao Xie, Yan Li, and Yongdong Zhang. Query-memory re-aggregation for weakly-supervised video object segmentation. In AAAI, 2021. 6
- [20] Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object segmentation. In ICCV, 2019. 9, 12
- [21] Zhihui Lin, Tianyu Yang, Maomao Li, Ziyu Wang, Chun Yuan, Wenhao Jiang, and Wei Liu. Swem: Towards realtime video object segmentation with sequential weighted expectation-maximization. In CVPR, 2022. 9, 12
- [22] David G Lowe. Distinctive image features from scaleinvariant keypoints. IJCV, 60:91–110, 2004. 2
- [23] Bruce D Lucas and Takeo Kanade. An iterative image registration technique with an application to stereo vision. In IJCAI, 1981. 2
- [24] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical ow, and scene ow estimation. In CVPR, 2016. 7
- [25] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In ICCV, 2019. 9, 12

- [26] Hae-Sang Park and Chi-Hyuck Jun. A simple and fast algorithm for k-medoids clustering. Expert Systems with Applications, 36(2, Part 2):3336–3341, 2009. 4, 5
- [27] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. ´ The 2017 davis challenge on video object segmentation. ar Xiv:1704.00675, 2017. 6, 8, 9, 10, 12, 14
- [28] Paul-Edouard Sarlin, Daniel De Tone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In CVPR, 2020. 2, 8
- [29] Jianbo Shi and Tomasi. Good features to track. In CVPR, 1994. 2, 4, 5
- [30] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs eld transforms for optical ow. In ECCV, 2020. 2, 8
- [31] Carlo Tomasi and Takeo Kanade. Detection and tracking of point. IJCV, 9:137–154, 1991. 2
- [32] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. ar Xiv:2306.05422, 2023. 2
- [33] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and Philip HS Torr. Fast online object tracking and segmentation: A unifying approach. In CVPR, 2019. 6
- [34] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran. Unidentied video objects: A benchmark for dense, openworld segmentation. In ICCV, 2021. 2, 6, 12
- [35] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. In CVPR, 2023. 2, 3, 5, 6, 9, 12
- [36] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. ar Xiv preprint ar Xiv:2304.03284, 2023. 2, 3, 5, 6, 9, 12, 14
- [37] Junfeng Wu, Yi Jiang, Wenqing Zhang, Xiang Bai, and Song Bai. Seqformer: a frustratingly simple model for video instance segmentation. In ECCV, 2022. 1
- [38] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: A large-scale video object segmentation benchmark, 2018. 6, 7, 12
- [39] Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, and Weidi Xie. Self-supervised video object segmentation by motion grouping. In ICCV, 2021. 5
- [40] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. ar Xiv preprint ar Xiv:2304.11968, 2023. 2, 3, 5, 6, 7, 9, 12
- [41] Zongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object segmentation. In NeurIPS, 2022. 1, 2, 5, 6, 9, 12
- [42] Zongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object segmentation. In NeurIPS, 2022. 3
- [43] K. M. Yi, Eduard Trulls, Vincent Lepetit, and P. Fua. Lift: Learned invariant feature transform. ECCV, 2016. 2

- [44] Zitong Zhan, Daniel Mc Kee, and Svetlana Lazebnik. Robust online video instance segmentation with track queries. *ar Xiv* preprint ar Xiv: 2211.09108, 2022. 12
- [45] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, and Hongsheng Li. Personalize segment anything model with one shot. *ar Xiv preprint ar Xiv:2305.03048*, 2023. 2, 3, 5, 6, 9, 12, 14

# A. Point Tracking Reinitialization

In our SAM-PT-reinit method, we introduce a reinitialization strategy. Here, the point tracker begins anew after every h frames, where h represents a pre-set tracking horizon (e.g., 8 frames), or is dynamically determined based on SAM's mask predictions for each timestep within the horizon (e.g., using most-similar-mask-area heuristics). Upon reaching this horizon, the query points given to the tracker are reinitialized according to the mask prediction SAM outputted at the horizon frame. While this method may increase the computational load (especially if some of SAM's computed masks are disregarded), it demonstrates substantial performance improvement in demanding video sequences, such as those in the MOSE dataset.

We explored four reinitialization methods, each varying in how they compute the value of h:

- (A) Reinit-on-Horizon-and-Sync-Masks: This straightforward variant reinitializes points after a fixed number of frames (e.g., every 8 frames). However, it may stumble if the mask is absent at the reinitialization timestep. Despite this potential pitfall, it operates at the same speed as methods that do not employ reinitialization.
- (B) Reinit-at-Median-of-Area-Diff: In this variant, the tracker outputs trajectory points for each frame within the horizon, and SAM predicts masks based on these trajectories. Reinitialization happens at the frame within the horizon that has the mean mask area among the non-empty masks predicted by SAM. Notably, this approach may be significantly slower than methods without reinitialization, as it may reject several SAM masks (e.g., out of 8 computed masks, reinitialization might occur on the second one, necessitating recomputation of the remaining 6 masks in the next step).
- (C) Reinit-on-Similar-Mask-Area: This method triggers reinitialization when the mask area is similar to the initial mask area, causing it to be several times slower than methods without reinitialization.

#### (D) Reinit-on-Similar-Mask-Area-and-Sync-Masks:

This variant reinitializes when the mask area for all masks in the batch is similar to the initial mask areas, synchronizing the masks to be tracked from the same timestep. This synchronization allows for the use of negative points from other masks when querying SAM, but it also runs several times slower

Table 7. Quantitative results in semi-supervised video object segmentation on additional subsets of DAVIS. ♠: our reproduced result using the official code of [45].

| | DAVIS 2016 Validation [27] | | | DAVIS 2017 Test-dev [27] | | | |
|-----------------|----------------------------|----------------|----------------|--------------------------|----------------|----------------|--|
| | J&F | $\mathcal{J}$ | $\mathcal{F}$ | J&F | $\mathcal{J}$ | F | |
| PerSAM-F ♠ [45] | 74.8 | 74.5 | 75.0 | 47.6 | 45.5 | 49.7 | |
| SegGPT [36] | 82.3 | 81.8 | 82.8 | _ | - | - | |
| SAM-PT | $83.1 \pm 1.5$ | $83.0 \pm 0.8$ | $83.0 \pm 1.1$ | $62.7 \pm 0.5$ | $59.4 \pm 0.6$ | $66.1 \pm 0.4$ | |
| SAM-PT-reinit | $80.2 \pm 0.6$ | $80.3 \pm 0.6$ | $80.0 \pm 0.6$ | $61.5 \pm 1.1$ | $59.3 \pm 1.0$ | $63.8 \pm 1.2$ | |

than methods without reinitialization.

From our investigations, we found the **(A) Reinit-on-Horizon-and-Sync-Masks** strategy to be the most effective, as indicated by its superior performance on the DAVIS 2017 validation subset. The choice of reinitialization method may depend on the specific validation subset and the degree of hyperparameter tuning involved. Note that we have always used reinitialization along with negative points.

#### A.1. Computational Cost and Speed Optimization

The introduction of reinitialization in SAM-PT-reinit comes with a trade-off: it slows down the inference speed by a factor of 2 to 8, depending on the reinitialization method and parameters used. The major bottleneck is the invocation of SAM's backbone for each video frame. We propose caching the backbone outputs for unprocessed video frames as a possible solution to mitigate this slow-down. This strategy requires storing embeddings for all video frames in the working memory but offers the potential for significant speedup, particularly useful for applications requiring faster inference.

#### **B. More DAVIS Subsets**

We report results on DAVIS 2016 Validation and DAVIS 2017 Test-dev in Tab. 7.

# C. Per-sequence DAVIS 2017 Validation Results

See figure Fig. 10 for per-sequence DAVIS 2017 Validation results. For exact numbers and tables, check our Git Hub experiment summaries or the Wandb project.

## ![](_page_14_Figure_0.jpeg)

## ![](_page_14_Figure_1.jpeg)

## ![](_page_14_Figure_2.jpeg)

$\begin{array}{c} \text{sequence} \\ \text{(b) point selection methods} \end{array}$ 

## ![](_page_15_Figure_0.jpeg)

## ![](_page_15_Figure_1.jpeg)

$\label{eq:sequence} \mbox{sequence}$ (d) number of negative points per mask

## ![](_page_16_Figure_0.jpeg)

Figure 10. **Per-sequence Ablation Results.** The charts plot the median J-Mean result of different ablation result experiments along with 95% bootstrapped confidence intervals, based on 1000 bootstrap samples.
