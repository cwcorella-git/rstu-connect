---
date: 2022
title: "ADOP Approximate Differentiable One-Pixel Point RenderingADOP Approximate Differentiable One-Pixel Point Rendering - 2110.06635.pdf"
---

# ADOP Approximate Differentiable One-Pixel Point RenderingADOP Approximate Differentiable One-Pixel Point Rendering - 2110.06635.pdf

![Figure from page 1](images/page_001_content/img-000.png) ![Figure from page 1](images/page_001_content/img-001.png)
                                        ADOP: Approximate Dierentiable One-Pixel Point Rendering
                                        DARIUS RÜCKERT, Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany
                                        LINUS FRANKE and MARC STAMMINGER, Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany
arXiv:2110.06635v3 [cs.CV] 3 May 2022
                                        Fig. 1. Given a set of RGB images and an initial 3D reconstruction (le), our inverse rendering approach is able to synthesize novel frames and optimize the
                                        scene’s parameters (right), for instance point position and camera pose as well as image seings such as exposure time and white balance. In combination
                                        with a fast point-based renderer, we achieve real-time render times even for complex environments.
                                        In this paper we present ADOP, a novel point-based, dierentiable neural 1 INTRODUCTION
                                        rendering pipeline. Like other neural renderers, our system takes as input With neural rendering it is possible to generate stunning free-
                                        calibrated camera images and a proxy geometry of the scene, in our case
                                                                                                                           viewpoint reconstructions of real world scenes. The eld has ad-
                                        a point cloud. To generate a novel view, the point cloud is rasterized with
                                        learned feature vectors as colors and a deep neural network lls the remain-
                                                                                                                           vanced enormously in the last years with two major streams emerg-
                                        ing holes and shades each output pixel. The rasterizer renders points as ing [Tewari et al. 2020]: Proxy-based approaches that use some
                                        one-pixel splats, which makes it very fast and allows us to compute gradi- 3D proxy to carry reconstructed scene information (e.g., triangle
                                        ents with respect to all relevant input parameters eciently. Furthermore, meshes [Thies et al. 2019] or point clouds [Aliev et al. 2020]), and
                                        our pipeline contains a fully dierentiable physically-based photometric direct approaches that encode light or radiance elds (e.g., NeRFs
                                        camera model, including exposure, white balance, and a camera response [Mildenhall et al. 2020]).
                                        function. Following the idea of inverse rendering, we use our renderer to Approaches with geometric proxies extend the idea of previous
                                        rene its input in order to reduce inconsistencies and optimize the quality image based rendering methods [Chaurasia et al. 2013], with the
                                        of its output. In particular, we can optimize structural parameters like the dierence that neural networks are used to interpolate and blend
                                        camera pose, lens distortions, point positions and features, and a neural
                                                                                                                           the input images on the geometric proxy. The rationale is that neu-
                                        environment map, but also photometric parameters like camera response
                                        function, vignetting, and per-image exposure and white balance. Because
                                                                                                                           ral networks can achieve a better interpolation of view-dependent
                                        our pipeline includes photometric parameters, e.g. exposure and camera re- eects as well as handle imperfections of the geometric proxy and
                                        sponse function, our system can smoothly handle input images with varying the resulting reprojection errors. Although this holds true to some
                                        exposure and white balance, and generates high-dynamic range output. We degree, inconsistencies in the input still have signicant impact
                                        show that due to the improved input, we can achieve high render quality, on the quality of the result. As such, slight miscalibration of in-
                                        also for dicult input, e.g. with imperfect camera calibrations, inaccurate trinsic and extrinsic camera parameters can lead to blurred results.
                                        proxy geometry, or varying exposure. As a result, a simpler and thus faster Even worse, varying camera exposure in the input images cannot be
                                        deep neural network is sucient for reconstruction. In combination with compensated well and leads to patches of inconsistent brightness.
                                        the fast point rasterization, ADOP achieves real-time rendering rates even In this paper, we present a novel neural rendering system that di-
                                        for models with well over 100M points.
                                                                                                                           rectly accounts for such inconsistencies and renders very eciently.
                                                             https://github.com/darglein/ADOP As input we assume a number of images of a scene. A standard
                                                                                                                           3D reconstruction pipeline estimates intrinsic and extrinsic camera
                                                                                                                           parameters and reconstructs a point cloud of the scene geometry.
                                        CCS Concepts: • Computing methodologies → Reconstruction; Ren- Following prior point-based neural rendering work [Aliev et al.
                                        dering; Image-based rendering; Neural networks.
                                                                                                                           2020], we learn feature vectors for all points and generate novel
                                                                                                                           views by rendering the point cloud to an image pyramid, from which
                                        Additional Key Words and Phrases: Image-based Rendering, Novel View
                                                                                                                           a deep neural network generates the nal image.
                                        Synthesis, Neural Rendering, Machine Learning, Inverse Rendering
                                                                                                                              However, our renderer is dierentiable with respect to structural
                                                                                                                           input parameters, such as intrinsic and extrinsic camera parameters,
                                                                                                                           camera exposure, and even point cloud positions. The core idea is an
                                        Authors’ addresses: Darius Rückert, darius.rueckert@fau.de, Friedrich-Alexander- approximation of the spatial gradients of one-pixel point splatting,
                                        Universität Erlangen-Nürnberg, Erlangen, Germany; Linus Franke, linus.franke@ an otherwise non-dierentiable operation. We thus call our method
                                        fau.de; Marc Stamminger, marc.stamminger@fau.de, Friedrich-Alexander-Universität
                                        Erlangen-Nürnberg, Erlangen, Germany. ADOP: Approximate dierentiable one-pixel point rendering. Using
``` 99:2 • Rückert et al. this technique, we can apply the idea of inverse rendering and use images are then used to reconstruct the nal output image by a deep the renderer itself to improve its output by optimizing the input neural network. NPBG is very exible as it doesn’t require a textured (see Fig. 1). In this paper, we show that this self-calibration reduces triangle mesh as proxy and shows impressive results. Similarly, Dai a large variety of sources of inconsistencies in the input, resulting et al. [2020] show that rendering multiple depth layers of a point in high-quality novel view synthesis, less constraints on the input, cloud can also be used to synthesize novel views. and faster rendering times. In detail, ADOP makes the following contributions:
       • We present a point-based rendering pipeline, that can self-
                                                                          2.2 Inverse Rendering
         optimize all structural input parameters obtained from the
         reconstruction, namely camera poses, lens distortions, and Inverse rendering and dierentiable rendering have been a topic
         even point cloud positions, which leads to signicantly im- of research for some time. However, major breakthroughs have
         proved render quality. only been made in recent years due to improved hardware and
       • We show how to optimize for camera response and vignetting, advancements in deep learning [Kato et al. 2020]. Its application
         as well as per-image exposure and white balancing to support can be challenging though: Traditional triangle rasterization with
         input images with varying exposure, as it is almost unavoid- depth-testing has no analytically correct spatial derivative [Kato
         able for outdoor scenes. As a side eect, the learned features et al. 2020; Loper and Black 2014]. Available systems therefore either
         encode the scene texture in high-dynamic range, and it is approximate the gradient [Genova et al. 2018; Kato and Harada 2019;
         possible to apply arbitrary tone mapping operators to the Kato et al. 2018; Loper and Black 2014] or approximate the rendering
         synthesized views. itself using alpha blending along edges [Chen et al. 2019; Liu et al.
       • We present a highly ecient rasterizer and rendering opti- 2019a; Rhodin et al. 2015]. Volume raycasting on the other hand is
         mizations with which our system achieves real-time render- dierentiable by accumulating all voxel values along the ray [Kato
         ing rates. We show that due to the signicantly improved et al. 2020]. This has been used by multiple authors to build volu-
         input, smaller networks are sucient to achieve good render- metric reconstruction systems [Henzler et al. 2019; Lombardi et al.
         ing quality. Together with a highly optimized point renderer, 2019; Tulsiani et al. 2017] or predict the signed distance eld of an
         we can render photo realistic full-HD images in 30FPS on object [Jiang et al. 2020]. Instead of a voxel grid, an implicit function
         o-the-shelf hardware. can also be used inside a dierentiable volumetric raycasting frame-
                                                                          work [Liu et al. 2019b, 2020; Niemeyer et al. 2020; Zakharov et al.
2 RELATED WORK 2020]. Mildenhall et al. [2020] show with Neural Radiance Fields
                                                                          (NeRF) that a deep neural network can be trained by volumetric ray-
2.1 Novel View Synthesis casting to store the view-dependent radiance of an object. Due to the
Traditional novel view synthesis, which is closely related to image- impressive results of NeRF, multiple extensions and improvements based rendering (IBR), relies on the basic principle of warping colors have been published in the following years [Martin-Brualla et al. from one frame to another. One approach is to use a triangle-mesh 2021; Sitzmann et al. 2020; Yu et al. 2021; Zhang et al. 2020]. Inverse proxy to directly warp the image colors to a novel view [Chaurasia rendering has also been proposed for point cloud rasterization [Kato et al. 2013; Debevec et al. 1998; Shum and Kang 2000]. However, et al. 2020; Lin et al. 2018]. The spatial gradients of the points can be imperfect geometry leads to ghosting artifacts around silhouettes. approximated in dierent ways. This includes gradient computation This can be improved by replacing hand-crafted heuristics in the along silhouettes [Han et al. 2020], gradients for Gaussian splat- classic IBR pipeline with deep neural networks [Hedman et al. 2018; ting [Kopanas et al. 2021; Wiles et al. 2020; Yifan et al. 2019], and Riegler and Koltun 2020, 2021; Thies et al. 2020]. If no proxy geom- gradient approximation using a temporary 3D volume [Insafutdinov etry is available, learning-based approaches have been developed and Dosovitskiy 2018]. A dierent approach is taken by Lassner that create a multi plane image representation [Mildenhall et al. and Zollhöfer [2021] who render the points as small spheres instead 2019; Srinivasan et al. 2019; Tucker and Snavely 2020; Zhou et al. of splats. Our dierentiable point rendering approach is similar to 2018] or directly estimate the required warp-eld [Flynn et al. 2016; established dierentiable splatting techniques [Kopanas et al. 2021; Ganin et al. 2016; Zhou et al. 2016]. Novel view synthesis can also be Yifan et al. 2019], however we render only one pixel per point, which performed by reconstructing a 3D model of the scene and rendering allows our approach to be multiple magnitudes more ecient than it from novel view points. Thies et al. [2019] learn a neural texture competing methods [Lassner and Zollhöfer 2021]. A nice property on a triangle mesh which can be rendered using traditional raster- of a dierentiable rendering pipeline is that the camera parameters ization. The rasterized image is then converted to RGB by a deep can be optimized during rendering. Lin et al. [2021] and Jeong et al. neural network. Other approaches use ray-casting to automatically [2021] show that the camera model and camera pose can be opti- learn a voxel grid [Nguyen-Phuoc et al. 2018; Sitzmann et al. 2019; mized in the NeRF pipeline by providing a dierentiable projection Zhu et al. 2018] or an implicit function [Mescheder et al. 2019; Park module. This allows to correct small errors in the initial calibra- et al. 2019]. It has also been shown that point clouds are suitable tion or even estimate the scene’s geometry from scratch. Similar geometric proxies for novel view synthesis [Meshry et al. 2019; work focuses on synthesizing photometric correct views by storing Pittaluga et al. 2019]. Neural Point-based Graphics (NPBG) [Aliev linear HDR radiance values inside the NeRF. A xed tonemapper et al. 2020], which is closely related to our method, renders a point [Mildenhall et al. 2021] or neural tonemapper [Huang et al. 2021] cloud with learned neural descriptors in multiple resolutions. These then converts the integrated HDR intensities to the LDR color space. ![Figure from page 3](images/page_003_content/img-000.png) ![Figure from page 3](images/page_003_content/img-001.png)
                                                                                             ADOP: Approximate Dierentiable One-Pixel Point Rendering • 99:3
Fig. 2. Overview of our point-based HDR neural rendering pipeline. The scene, consisting of a textured point cloud and an environment map, is rasterized into a set of sparse neural images in multiple resolutions. A deep neural network reconstructs an HDR image, which is then converted to LDR by a dierentiable physically-based tonemapper. All parameters in the rectangular boxes, as well as the neural network can be optimized simultaneously during training. Concerning input renement, this paper is inspired by these ap- 3 NEURAL RENDERING PIPELINE proaches and similar to concurrent work of Tancik et al. [2022]. An overview of our end-to-end trainable neural rendering pipeline However, we use an improved dierentiable tonemapper module is shown in Fig. 2. As input, we use a point cloud, which can come and apply the structural optimization on a point cloud instead of from common Multiview Stereo or LiDAR and SLAM systems, and NeRFs. the novel frame’s camera parameters. Learned neural descriptors
                                                                                  are assigned to the point cloud.
                                                                                      The rst step in the pipeline is a dierentiable rasterizer (Fig. 2
                                                                                  left), that renders each point as a one-pixel-sized splat by projecting
                                                                                  it to image space using the camera parameters. Second, a deep neural
2.3 Point-Based Rendering renderer network is used to ll in holes and shade the image. Finally,
Point-based rendering has been a topic of interest in computer graph- a learnable tone mapping operator with a camera sensor model is ics research for some time [Levoy and Whitted 1985]. Over the past applied, which converts the rendered high-dynamic range (HDR) decades two orthogonal approaches have been developed [Kobbelt image to the displayable low dynamic range (LDR). For optimization, and Botsch 2004]. The rst major approach is the rendering of points input views are re-rendered, compared to the ground truth, and the as oriented discs, which are usually called splats or surfels [?], with loss is backpropagated through our rendering pipeline. Since all the radius of each disc being precomputed from the point cloud den- steps are dierentiable, we can simultaneously optimize all param- sity. To get rid of visible artifacts between neighboring splats, the eters, in particular the scene structure, photometric model, point discs are rendered with a Gaussian alpha-mask and then combined features, and network parameters, which improves consistency of by a normalizing blend function [Alexa et al. 2004; Pster et al. 2000; the input and adds robustness from a possible imperfect initial re- Zwicker et al. 2001]. Further extensions to surfel rendering exist, construction or calibration. In the following section, we detail on for example, Auto Splats, a technique to automatically compute these three steps. oriented and sized splats [Preiner et al. 2012]. In recent years, deep learning-based approaches have been presented that improve image 3.1 Dierentiable One-Pixel Point Rasterization quality of surfel rendering, especially along sharp edges [Bui et al.
                                                                                  Forward Pass. Our dierentiable rasterizer renders multiple resolu-
2018; Yang et al. 2020].
                                                                                  tions of a textured point cloud using one-pixel-sized splats. Formally
The second major approach to point-based graphics is point sam-
                                                                                  speaking, the resolution layer  ∈ {0, 1 . . . ,  −1} of the neural image
ple rendering [Grossman and Dally 1998], where points are rendered
                                                                                   is the output of the render function Φ
as one-pixel splats generating a sparse image of the scene. Using iterative [Rosenthal and Linsen 2008] or pyramid-based [Grossman I = Φ (, , i, t, , , ), (1) and Dally 1998; Marroquim et al. 2007; Pintus et al. 2011] hole-lling approaches, the nal image is reconstructed as a post processing where  is the camera model, (, i) the camera pose, t the point step. To reduce aliasing in moving scenes, points with a similar position,  the point normal,  the environment map, and  the depth value can be blended during rendering [Botsch et al. 2005; neural texture. Schütz et al. 2021]. It has been shown that software implementa- The rasterizer performs three steps, which are projection, occlu- tions [Günther et al. 2013; Schütz et al. 2021] outperform hardware sion check, and blending. The rst step is the projection of each accelerated rendering through GL_POINTS [Shreiner et al. 2009]. world point t into the image-space of layer . Using the camera Recently developed approaches replace traditional hole-lling tech- model  and the rigid transformation from world to camera-space niques with deep neural networks to reduce blurriness and better (, i), we dene this projection as: complete large holes [Le et al. 2020; Meshry et al. 2019; Pittaluga 1 et al. 2019; Song et al. 2020].  (, , i, t) =  (t + i) (2)
                                                                                                                            2
![Figure from page 4](images/page_004_content/img-000.png) ![Figure from page 4](images/page_004_content/img-001.png) ![Figure from page 4](images/page_004_content/img-002.png) ![Figure from page 4](images/page_004_content/img-003.png) 99:4 • Rückert et al. Fig. 3. One-pixel point rendering with fuzzy depth testing and threshold
                                                                            Fig. 4. Pixel lookup for the spatial gradient computation of the center (dark
 = 0 on the le and  = 0.01 on the right.
                                                                            blue) point. The white pixels are from the background and the teal pixels
                                                                            are other rasterized points.
The real valued result of  is then converted to pixel coordinates by rounding it to the nearest integer. A world point t ∈ R3 is therefore Backward Pass. The backward pass of our point rasterizer rst com- projected to pixel coordinates  ∈ Z2 by putes the partial derivatives of the render function (1) w.r.t. its
                              ⌊ ⌉
                                                                            parameters.
                         =  (, , i, t ) . (3)
                                                                                                           Φ Φ
                                                                                                                                                (8)
Note that the rounding operation makes the projection discrete,
                                                                                                               , ,...
                                                                                                            
which requires us to approximate its derivative (see later). After all Using the chain rule, we can then compute the parameter’s gradient points have been projected to image space, we discard them if they w.r.t. the loss and pass it to the optimizer. The dicult part of this fail at least one of the following conditions: calculation are the partial derivatives of the structural parameters
                                                                            , , i and t. This will be explained in the following section. The
              Bounds Test:  ∈ [0,  [ ∧  ∈ [0, ℎ[ (4)
                                                                            derivatives of the texture  and environment map  are straightfor-
                                       t + i ward and will not be detailed in this work.
            Normal Culling: ( ) · >0 (5)
                                      |t + i | The problem of deriving Φ w.r.t. the structural parameters is the
               Depth Test:  ≤ (1 + )mini ( ) (6) rounding operation Eq. (3) of our one-pixel point rasterizer. Due to
                                                                            the blending operation (7), it is not possible to compute the change of
The last condition (6) is the fuzzy depth-test as described in Schütz
                                                                            a pixel’s color by moving a point out of this pixel, without storing a
et al. [2021]. A point passes the fuzzy depth test if its -value is
                                                                            list of points with each pixel and reevaluating the blending function.
smaller than or equal to the scaled minimum depth value at that
                                                                            So instead, we use an approximation that works well in practice.
pixel. If the camera model does not provide a valid -value, we
                                                                            The principle is shown in Fig. 4. We virtually shift the projected
instead use the distance between the camera and the 3D point. A
                                                                            point at  = (, ) one pixel in each direction and compute the
large value of  in Eq. (6) increases the number of points that pass
                                                                            change of intensity at the target pixels, which is possible if we know
the depth test. This leads to a smooth image but also can introduce
                                                                            the depth value at the target pixel. The induced intensity change of
artifacts when background points are merged into the foreground.
                                                                            these shifts are:
In practice we use  = 0.01 as suggested by Schütz et al. [2021].   After the bounds check (4), normal culling (5), and fuzzy depth- Δ  Δ 
                                                                                                                                                (9)
                                                                                              Δ  Δ 
                                                                                                            , , ...
test (6), for each pixel (, , ) we obtain a list of points Λ,, . If =(+1,) =(−1,) a pixel is not hit by any point, we sample the output color from the environment map  using the inverse camera projection  −1 . from which we can approximate the gradient for  (equally for ) at Otherwise, we sample the texture  of every point and write the the desired location with mean into the output image. The blending function  can therefore  (   )
                                                                                        1 Δ  Δ 
be written as: ≈ + (10)
                                                                                       =(,) 2 Δ =(−1,) Δ =(+1,)
                                                                                                                                    .
                          {
                             (  −1 (, )) Λ,, = ∅
            (, , Λ) = 1  (7) To compute each induced change of (9), multiple cases have to
                            |Λ |  ∈Λ,,  () else be considered. As shown in Fig. 4, neighboring elements of  can
                            ,,
Fig. 3 shows two color images that have been rendered using the either show a color from the environment map (background) or one-pixel point rasterization technique described in this section. We from other rasterized points. In the rst case, the point simply can see that with  = 0.01 (right image) the aliasing is signicantly overwrites the background, otherwise, three dierent cases have to reduced and for example the letters are easier to read. be considered. Firstly, the virtually shifted point can be completely
                                                                            behind the neighbor in which case no intensity change would be
Environment Map. To represent the environment, we place a spheri- induced. Secondly, the shifted point can be in front of the neighbor cal environment map of resolution 10242 around the scene. Analog replacing the old color with the point’s texture. Lastly, the neighbor to the scene points, the environment map contains neural features, point can have a similar depth value according to the fuzzy depth which are rendered as background. Both, the features of the point test (6), which causes us to have to compute the change of the cloud and those of the environment map are learned using deriva- blend function (7) if  is added to the neighboring Λ. In summary, tives computed in the following backward pass. the four cases to compute local spatial gradients are (with (, ) =
                                                                                        ADOP: Approximate Dierentiable One-Pixel Point Rendering • 99:5
( ± 1,  ± 1)): of a four layer fully convolutional U-Net with skip-connections,
             where the lower resolution input images are concatenated to the
            
             (, ) −  (, ) Λ,, = ∅
            
             intermediate feature tensors. Downsampling is performed using
      
              > (1 + )mini (, )
Δ  0
             average-pooling and the images are upsampled by bilinear inter-
      =  (, ) −  (, )  (1 + ) < mini (, ) polation. As convolution primitives, we use gated convolutions [Yu
Δ =(,) 
             
            
             et al. 2019] which have been initially developed for hole-lling tasks
             |Λ, | (,)+ (,)
            
             1+ |Λ, | −  (, ) else and are therefore well suited for sparse point input. Overall the
            
                                                                       (11) network architecture is similar to Aliev et al. [2020] with one less
Stochastic Point Discarding. Especially at small resolution layers, layer (see Sec. 4.6) and a few modications to enable HDR imaging. hundreds of points can pass the fuzzy depth-test of a single pixel, First, we remove the batch-normalization layers, as they normalize resulting in dramatic overdraw. To reduce this number, we apply the mean and standard deviation of intermediate images to xed a stochastic point-discarding technique similar to Enderton et al. values. The total sensor irradiance is therefore lost and cannot be [2010]. In a preprocess, we compute the world-space radius   propagated from a 3D point to the nal image. Usually, we store the as the distance to the 4iℎ-nearest point. This radius is projected neural point descriptors linearly, but if the scene’s radiance range is into screen-space during rendering, giving an approximation of sizable (larger than 1 : 400) we use a logarithmic scaling, otherwise the circular splat-size  s . Based on this size we then discard convergence issues with the optimizer can occur. For logarithmic points stochastically to obtain a desired number of points per pixel descriptors, we convert them to linear space during rasterization so by assigning each point a uniform random value  ∈ [0, 1] and that the convolution operations only use linear brightness values. rendering it only if the following condition is met:
                                                                              3.3 Dierentiable Tone Mapping
                                  s 1
                                 √ > (12) The HDR output image of the neural renderer is converted to LDR
                                    1−  by a learnable tone mapping operator (Fig. 2 right), that mimics the
The parameter  roughly controls how many points are rendered physical lens and sensor properties of digital photo cameras. The per pixel. In our experiments, we use  = 1.5 for all datasets and rst tone mapping stage applies brightness correction to the HDR scenes, see also Fig. 9. image   using the estimated exposure value EV of image .
                                                                                                                 
Implementation Details. To implement the forward pass eciently,  = EV (14) we mainly follow the work of Schütz et al. [2021]. They propose a 2  three-pass point rasterizer that outperforms traditional point ren- If image meta information is available, we initialize EV using dering with GL_POINTS. After a depth-only render pass, the color Eq. (15), where  is the f-number of the optical system, i the expo- is accumulated for all points that pass the fuzzy depth-test. During sure time in seconds,  the ISO arithmetic speed rating, and EV the accumulation, we track the number of points per pixel so we can mean exposure value of all images. Otherwise, we initialize EV to compute Eq. (7) without writing the set Λ, to memory. To fur- zero for all images. ther improve eciency, we adopted the proposed blocked-morton-
                                                                                                          ( )  
                                                                                                           2 
shue on the point cloud and use optimized local reductions inside EV = log2  + log2 − EV (15)
                                                                                                            i 100
the rasterization kernels. A side-eect of not explicitly storing Λ, is that during backpropagation all points have to be rendered again. After brightness correction, we compensate for a changing white However, this re-rendering only marginally impacts performance as balance by estimating the white point ( ,  ,  ) for each image. the total backpropagation time is dominated by the neural networks. During optimization, we keep  constant so that the white point To further improve the pose estimation of our inverse render- cannot change the total image brightness. ing pipeline, we follow state-of-the-art SLAM systems [Engel et al. As a next step, we model the vignette eect of digital cameras, 2017; Mur-Artal and Tardós 2017; Rückert and Stamminger 2021] which is a radial intensity fallo due to various optical and sensor- that optimize the rigid transformation (, i) ∈  (3) in tangent- specic eects. The model we use [Goldman 2010] is a polynomial space. Linearized pose increments are expressed as the Lie-algebra of the distance  of a pixel  to the vignette center   : elements t ∈ (3) and applied to a transformation by   =   · (1 +  2 2 +  4 4 +  6 6 ) (16)
                           ′ ′
                        ( , i ) = exp(t) · (, i), (13) For better stability, we normalize  and   to be in the range [0, 1].
where exp(t) is the exponential map from (3) to  (3). To imple- The coecients  are initialized to zero and   is initialized to the ment tangent-space optimization in libTorch [Jia et al. 2014][Paszke image center. et al. 2019], we create a tangent tensor for the pose which is up- The last stage in the tone mapping operator maps the linear RGB dated by the optimizer. After each step, the tangent is applied to the values to non-linear image intensities by applying the camera re- original pose (13) and reset to zero. sponse function (CRF) [Grossberg and Nayar 2003] and an optional
                                                                              color space conversion from RGB to sRGB. We combine both oper-
3.2 Neural Renderer ations into a single function , which is implemented using a 1D
The neural renderer step (Fig. 2 center) uses the multi-resolution texture for every color channel. neural images to produce a single HDR output image. It consists  = (  ) (17) 99:6 • Rückert et al.
            Scene #Points #Images Resolution Exposure White Balance Camera Model Initial Reconstruction
            M60 9,713,277 313 2048 × 1080 Constant Constant Pinhole + Dist. COLMAP
            Train 11,818,812 301 1920 × 1080 Auto Constant Pinhole + Dist. COLMAP
            Playground 12,521,941 307 1920 × 1080 Auto Constant Pinhole + Dist. COLMAP
            Lighthouse 12,313,620 309 2048 × 1080 Auto Constant Pinhole + Dist. COLMAP
            Kemenate 34,144,401 405 2880 × 1920 Constant Constant Pinhole + Dist. Metashape
            Boat 53,036,216 742 2880 × 1920 Auto Constant Pinhole + Dist. COLMAP
            Oce 72,916,173 688 5472 × 3648 Auto Auto Fisheye LiDAR + SLAM
Table 1. Overview of our evaluation scenes. M60, Train, Playground, and Lighthouse are from the Tanks and Temples dataset. Kemenate and Boat were captured by the authors, Oice was provided by (anonymous). To guide the optimization towards a plausible response function, we Inference (RTX 3080) Training (A100) initialize (t) to t 0.45 and add the following additional constraints Nerf++ ∼ 183,000 ms ∼ 24 h based on Debevec and Malik [2008]: (0) = 0, (1) = 1,  ′′ (t) = 0. SVS ∼ 2,400 ms > 48 h These constraints ensure that the whole intensity range is covered NPBG 50 ms ∼3h and the response function is smooth. Overexposed and underex- ADOP (ours) 27 ms ∼4h posed pixels are clamped to the maximum and minimum output
                                                                                Table 2. Timings for novel view synthesis averaged over the four tanks and
values of 1 and 0. However, depending on the random network ini-
                                                                                temples scenes, and approximate training times. For SVS and Nerf++ we
tialization, it is possible that the whole image is over/under exposed used the provided standard parameters for training. generating a zero gradient over the complete image. Inspired by the LeakyReLU activation function [Xu et al. 2015], we dene a separate response function  during training that leaks small values instead of clamping them.
                                                                                only in Boat the exact exposure value is stored in the meta data. In
                              
                               t t <0
                              
                              
                               the other cases, we let ADOP estimate the real exposure value for
                       (t) = (t) 0≤t ≤1 (18) each image.
                              
                               − +  + 1 1 < t
                              √ For a quantitative evaluation we follow the standard approach
                               
                                                                                in our community. All input images are divided into a training set
The last term of Eq. (18) asserts that the maximum leaked value is and a test set. The training set is used to optimize all parameters (1+). This is important in HDR scenes, because an overexposure of of our rendering pipeline over multiple epochs. We then let our multiple magnitudes should not create a large gradient. In practice system synthesize each test image and compare it to the real image we use  = 0.01, which results in a maximum image intensity of using dierent metrics. Since the test images are also prone to  (∞) = 1.01. miscalibration, we do a realignment and exposure estimation of the
                                                                                test images after training has been completed. For a fair comparison,
4 EXPERIMENTS we show the results with and without test renement during our
4.1 Datasets and Evaluation Methodology comparison to other approaches (see Tab. 4).
We have collected seven datasets with dierences in size, modality, and camera settings (see Tab. 1). The rst four scenes, i.e. M60, 4.2 Training Details and Timings Train, Playground, and Lighthouse, are from the popular tanks and Our inverse rendering pipeline (see Figure 2) is optimized end-to- temples dataset [Knapitsch et al. 2017]. They consist of around 310 end using the C++ front-end of libTorch. The neural network and the images in Full-HD resolution captured by a high-end video camera. 4-element point descriptors are initialized randomly and optimized We use COLMAP [Schönberger and Frahm 2016] to reconstruct using ADAM [Kingma and Ba 2015] with an initial learning rate the initial dense point-cloud as well as the camera extrinsics and of 0.0002 and 0.08, respectively. If the radiance variation is high intrinsics. The three remaining scenes, Kemenate, Boat, and Oce, (see Sec. 4.11), the descriptors are stored logarithmically and their are added to show how ADOP can handle dierent capture setups learning rate is reduced by a factor of 10. The remaining parameters, as well as to evaluate some specic aspects of our pipeline. The for example, camera pose and exposure time are updated using Boat scene was captured outdoors with a large variance in exposure the SGD optimizer. Over time, each learning rate is reduced when time. The Oce dataset consists of very high resolution sheye a plateau is detected. For the exact values the reader is referred input images and its point cloud was produced by a LiDAR scanner to the conguration les provided in the source code. To further instead of a multi-view stereo approach. For Kemenate, the initial improve the robustness of our pipeline, we delay the optimization reconstruction was processed by Metashape [Agisoft 2021]. Please of structural parameters and photometric parameters for 25 epochs. see also the accompanying video for results of all scenes. As shown in At that point, the rendered image is a blurry version of the target Table 1, several scenes were captures with varying exposure, though image and spatial gradients contain reasonable values. ![Figure from page 7](images/page_007_content/img-000.png) ![Figure from page 7](images/page_007_content/img-001.png) ![Figure from page 7](images/page_007_content/img-002.png) ![Figure from page 7](images/page_007_content/img-003.png)
                                                                                         ADOP: Approximate Dierentiable One-Pixel Point Rendering • 99:7
                             Train Playground M60 Lighthouse Kemenate Boat Oce
                          VGG ↓ Di VGG ↓ Di VGG ↓ Di VGG ↓ Di VGG ↓ Di VGG ↓ Di VGG ↓ Di
Baseline 514.3 374.8 369.5 508.5 204.7 652.9 186.8 + Env. 490.8 -5% 374.8 0% 364.1 -2% 440.1 -14% 202.5 -1% 646.1 -1% 185.5 -1% + Env. + TM 412.0 -19% 352.2 -6% 360.3 -3% 381.1 -25% 189.3 -8% 555.7 -15% 145.4 -22% + Env. + TM + SO 368.1 -28% 318.7 -15% 317.7 -14% 350.0 -31% 180.2 -12% 535.6 -18% 131.7 -30% Table 3. Qantitative ablation study on the major contributions of ADOP. The improvements due to the enviroment map (Env), the tone mapper (TM), and the structure optimization (SO) diers between the scenes. For example, the M60 scene was captures with fixed exposure hence the tone mapper only has a small impact. The structure optimization is beneficial in all cases. A qualitative evaluation for the Train scene is shown in Figure 6.
                  L1 ↓ MSE ↓ VGG ↓ PSNR ↑ LPIPS ↓ SSIM ↑
Train w. L1 0.029 0.0033 487.43 25.10 0.264 0.846 Train w. MSE 0.034 0.0037 558.84 24.67 0.322 0.822 Train w. VGG 0.032 0.0035 357.08 24.79 0.145 0.804 Fig. 5. Eect of training ADOP with dierent loss functions. Using the VGG loss for training improves the image quality significantly even though some
                                                                                 Fig. 6. Qalitative ablation study of the results presented in Table 3.
metrics favor a training with L1 loss.

                                                                               4.4 Ablation Study
                                                                               Compared to previous work of neural point-based rendering, we
                                                                               have proposed several ideas to improve the quality of novel view
The training for all scenes was done on an NVidia A100 with
                                                                               synthesis. In Tab. 3 and Fig. 6, we analyze how much each mod-
40GB video ram. The batch size was set to 16 with a crop size of
                                                                               ule contributes to the nal result. The rst row shows a baseline
512 × 512 pixels. The four tanks and temples scenes were trained for
                                                                               experiment, which is similar to NPBG [Aliev et al. 2020]. After
400 epochs, which takes around 3.5 hours per scene. The remaining
                                                                               that we enable one by one, the environment map (Env), the tone
scenes were trained for 800 epochs. Table 2 shows a inference and
                                                                               mapper (TM), and the structure optimization (SO). In the latter, we
training time comparison of ADOP to Nerf++, SVS and NPBG. Our
                                                                               initialize the scene using the COLMAP reconstruction and then
method trains slightly slower than NPBG, mostly due to our more
                                                                               optimize the camera pose, camera model, and point cloud during
complex backpropagation, but still substantially faster than Nerf++
                                                                               the training stage. All together, a signicant improvement in im-
and SVS. At inference time, our method is the fastest and achieves
                                                                               age quality is achieved with a VGG loss reduction between 12%
real-time performance (<33ms) on an NVidia RTX3080.
                                                                               and 30%. The impact of the individual modules diers between the
                                                                               scenes. The indoor scenes (M60, Kemenate, and Oce) are not im-
4.3 Loss Function proved by the environment map. The tonemapper shows the largest
We have trained ADOP on the M60 scene using typical loss functions impact on scenes with a high variance in exposure times (Train, also used in prior work: L1, MSE, and the VGG perceptual loss Lighthouse, Boat, Oce). However, structure optimization yields a [Johnson et al. 2016]. For evaluation we use the LPIPS loss [Zhang positive change on all datasets. et al. 2018], the peak signal-to-noise ratio (PSNR), and the structured similarity index (SSIM). The results are shown in Fig. 5. Using the 4.5 Robust Pose Correction VGG loss, the most details are visible and the overall sharpness is To further prove the robustness of our approximate gradient compu- signicantly improved compared to the other losses. All further tation (see Sec. 3.1), we add Gaussian noise to the camera position experiments thus use VGG as loss function. and rotation. As shown in Fig. 7, ADOP is robust to a bad initial ![Figure from page 8](images/page_008_content/img-000.png) ![Figure from page 8](images/page_008_content/img-001.png) ![Figure from page 8](images/page_008_content/img-002.png) ![Figure from page 8](images/page_008_content/img-003.png) ![Figure from page 8](images/page_008_content/img-004.png) ![Figure from page 8](images/page_008_content/img-005.png) 99:8 • Rückert et al.
                                                                                                             Train Loss Test Loss Time (ms)
                                                                                                #Params VGG ↓ VGG ↓ LPIPS ↓ FP32 FP16
                                                        COLMAP 3 Layers 136,387 302.5 330.9 0.1364 34.264 22.604
                         600
                                                     COLMAP + Noise 4 Layers 574,651 262.8 317.0 0.1191 41.343 26.665
                                                   COLMAP + Noise + SO 5 Layers 2,335,923 245.7 325.8 0.1239 48.106 30.798
           VGG (Train)
                                                                                     Fig. 8. Qality comparison on the number of Unet-layers inside the neural
                         400 renderer. More layers improves the hole-filling capabilities (see images) but
                                                                                     also reduces rendering eiciency. On some scenes, a 5 layer network is also
                                                                                     prone to overfit on the training images (low train loss, but high test loss).
                               0 100 200 300 400
                                           Epoch
Fig. 7. Without structure optimization (SO), neural point-based rendering is sensitive to a bad initialization resulting in severe image artifacts (top right). However, our SO technique (boom le) can recover from the erroneous input and outperforms the COLMAP initialization aer 400 training epochs. calibration and can also be used for image to point cloud alignment (see the supplemental material).

4.6 U-Net Layers
Due to the improved consistency of the input, we can aord to use a smaller and faster neural network for nal reconstruction. While other methods often use a ve layer network [Aliev et al. 2020; Thies et al. 2019], we show in Fig. 8 that also fewer layers Fig. 9. Stochastic discarding roughly halves the number of blended points,
                                                                                     which increaes rasterization eiciency by 15-25%, without impact on quality.
can produce good results. Note that with the number of layers in particular the hole-lling capabilities decrease, so that the chosen number of layers should be selected according to the point densities. see Tab. 1. The methods we compare against are SynSin [Wiles et al. In our experiments, we found that a four layer network improves 2020], Pulsar [Lassner and Zollhöfer 2021], Neural Rendering in the eciency and is less prone to overtting, all shown results thus Wild (NRW) [Meshry et al. 2019], NeRF++ [Zhang et al. 2020], Stable were generated with four layers. View Synthesis (SVS) [Riegler and Koltun 2021], and Neural Point
                                                                                     Based Graphics (NPBG) [Aliev et al. 2020]. The rst two are general
4.7 Stochastic Point Discarding dierentiable rendering front-ends that have been adapted by us
In Sec. 3.1 we proposed a stochastic point discarding technique to to the novel view synthesis problem. They can be seen as similar reduce the pixel overdraw. To examine a possible quality loss, we to our approach and NPBG with the dierence of using sphere and train ADOP with and without point discarding on the Kemenate splat-based rendering instead of multi-resolution one-pixel point dataset. We measure the number of blending operations, render rendering. We did not compare against all related inverse rendering time, and image quality, the result is presented in Fig. 9. Visually, no systems, because some are not applicable to our data [Loper and dierence is noticeable, which is backed by the perceptual rendering Black 2014] and others are outperformed by Pulsar and Synsin [Kato loss. However, on average only 50% of the points are blended, which et al. 2020; Lin et al. 2018; Liu et al. 2019a] . reduces rendering time by 20%. Pulsar, Synsin, Nerf++ and SVS only support pinhole cameras.
                                                                                     Therefore we train them on the undistorted images and camera
4.8 Comparison With Prior Work parameters provided by COLMAP. During evaluation the synthe-
We evaluate our system with state-of-the-art prior work on the sized undistorted images are distorted again and compared to the scenes from the tanks and temples dataset [Knapitsch et al. 2017], ground truth. NPBG, NRW, and our approach use one-pixel point ![Figure from page 9](images/page_009_content/img-000.png) ![Figure from page 9](images/page_009_content/img-001.png)
                                                                                             ADOP: Approximate Dierentiable One-Pixel Point Rendering • 99:9
Fig. 10. Comparative results of novel-view synthesis on the Train, Playground, and M60 scene. The table below provides a quantitative comparision, in this figure we show images for the methods that perform best. Note that due to memory constraints SVS has been evaluated in half-resolution.
                                    Train Playground M60 Lighthouse
        Method VGG ↓ LPIPS ↓ PSNR ↑ VGG ↓ LPIPS ↓ PSNR ↑ VGG ↓ LPIPS ↓ PSNR ↑ VGG ↓ LPIPS ↓ PSNR ↑
        Synsin + Unet 706.0 0.3853 16.97 521.3 0.3198 21.81 564.3 0.2904 20.16 679.7 0.3655 15.41
        Pulsar + Unet 677.9 0.3418 17.78 661.2 0.3849 20.00 508.2 0.2403 21.42 587.1 0.3112 17.82
        NRW 817.5 0.4552 14.44 632.8 0.4110 19.47 741.2 0.4476 16.96 709.5 0.3835 14.88
        NeRF++ 857.5 0.5168 18.04 696.8 0.5292 22.24 700.5 0.4378 23.06 741.2 0.4609 20.06
        SVS (half res.) 633.1 0.323 17.43 516.6 0.3462 22.11 461.1 0.2440 23.74 606.5 0.3232 17.12
        NPBG 521.9 0.2094 17.66 389.9 0.1816 23.31 380.69 0.1494 24.15 500.0 0.2140 17.54
        ADOP 422.6 0.1603 21.62 345.9 0.1594 25.00 342.8 0.1344 25.07 375.1 0.1536 22.44
        ADOP w. TR 368.1 0.1438 23.19 318.7 0.1464 25.78 317.7 0.1189 25.84 350.0 0.1439 23.04
Table 4. Qantitative evaluation of novel view synthesis on the four scenes of the tanks and temples dataset. The values in this table represent the mean loss over all test images. ADOP outperforms the other approach with and without test refinement (TR). In the laer, we estimate the expsoure value and refine the camera pose of the test images. ![Figure from page 10](images/page_010_content/img-000.png) ![Figure from page 10](images/page_010_content/img-001.png) ![Figure from page 10](images/page_010_content/img-002.png) ![Figure from page 10](images/page_010_content/img-003.png) ![Figure from page 10](images/page_010_content/img-004.png) ![Figure from page 10](images/page_010_content/img-005.png) ![Figure from page 10](images/page_010_content/img-006.png) ![Figure from page 10](images/page_010_content/img-007.png) 99:10 • Rückert et al. Fig. 11. ADOP is able to render fisheye camera systems at high quality. Fig. 12. View extrapolation with a large distance to the closest training
                                                                           image.
rendering, which natively supports the camera models, so they have been trained on the original distorted images. The results (see Fig. 10) show that ADOP is able to produce con- vincing results on all tested scenes. Compared to the other methods, our renderings show less artifacts as well as preserve color, bright-
                                                                                    20
ness, and sharpness better. For example, in the close-up view of the playground scene, the metal bars of the swing are most detailed in 15
                                                                               EV
the ADOP rendering. In the M60 dataset, NPBG has similar visual
                                                                                    10
results, but the quantitative evaluation (in Tab. 4) shows that our
                                                                                         0 100 200 300 400 500 600 700
method still has the edge. In the last row of Tab. 4 we show the quantitative results of ADOP with test renement (TR) enabled. TR
                                                                           Fig. 13. Sample images of the boat dataset, taken with auto exposure. The
compensates errors of the initial reconstruction by estimating the
                                                                           exposure value (EV) for each image in the dataset is ploed below the
exposure value and camera pose of the test frames once training has images. been completed. This further improves the loss however note that even without TR our approach outperforms the state-of-the-art.

4.9 Fisheye Camera
Our system is able to learn with images originating from sheye cameras without needing to undistort the projection, resulting in high quality neural sheye renders (see Fig. 11). The oce scene uses a LiDAR and SLAM system for poses and the point cloud, which can be noisy. As presented before, ADOP is able to correct this eciently.

4.10 View Extrapolation
In Fig. 12 we show reconstructions from a view that is far from the input views, comparing the results of our system with SVS. In general, ADOP generates relatively good results in such cases compared to established methods, although this setup is generally dicult.
                                                                           Fig. 14. Novel views synthesized on the boat dataset. Le: no tone mapping
4.11 HDR Neural Rendering and exposure correction results in splotchy artifacts, Center: HDR recon-
When capturing outdoor scenes, it is almost unavoidable to use struction with tone mapping and same exposure as reference photograph. varying exposure to be able to adapt to the huge range of brightness Right: ground truth. values. To test the performance of our pipeline on such HDR scenes, we apply it to our Boat dataset. The camera was set to auto expo- sure, all other image related settings, such as aperture, ISO-Speed, Novel view synthesis results of the Boat scene are shown in Fig. 14. and white balance were set constant for all frames. In Fig. 13, four For the images in the left column, uniform exposure was assumed, in samples from the dataset are shown, underneath the exposure value the middle column the exposure value used by the tonemapper has is plotted for every frame in the dataset. The dierence between been set to the real EV from the EXIF meta data and kept constant the smallest and largest EV is 8.7, which corresponds to a factor of during training. Our system is then able to handle the high dynamic
28.7 = 426.67. range and avoids splotchy artifacts.

![Figure from page 11](images/page_011_content/img-000.png) ![Figure from page 11](images/page_011_content/img-001.png)
                                                                                        ADOP: Approximate Dierentiable One-Pixel Point Rendering • 99:11
                                                                             an estimated point cloud of the scene as input. All stages are dif-
                                                                             ferentiable, so we can rene all input parameters to generate an
                                                                             improved output. By also including a photometric sensor model, we
                                                                             can handle input images with varying exposure and generate high
                                                                             dynamic range output.
                                                                                Our experiments show that, due to the optimized input, we achieve
                                                                             superior rendering quality, especially for novel view synthesis. At
                                                                             the same time, the one-pixel point renderer is very fast. In combina-
                                                                             tion with the fact that the improved consistency in the input allows
                                                                             us to use a simpler and thus faster neural network for reconstruc-
                                                                             tion, we achieve real-time frame rates on a standard GPU, even for
                                                                             complex scenes.
Fig. 15. At inference time, we can replace the learned tone mapper (TM) by https://github.com/darglein/ADOP a filmic TM, which renders the images in a more natural look.
                                                                             ACKNOWLEDGMENTS
                                                                             Linus Franke was supported by the Bayerische Forschungsstiftung
Additionally, we can replace the learned TM at inference time, for (Bavarian Research Foundation) AZ-1422-20. instance by a lmic tone mapper [Hable 2010], that better resembles human perception. The result is shown in Fig. 15. With the lmic REFERENCES tone mapper, the dark areas have signicantly more contrast without Agisoft. 2021. Metashape. https://www.agisoft.com/
                                                                             Marc Alexa, Markus Gross, Mark Pauly, Hanspeter Pster, Marc Stamminger, and
overexposing the bright wood inside the boat. The lmic TM also Matthias Zwicker. 2004. Point-based Computer Graphics. In ACM SIGGRAPH 2004 slightly reduces color saturation, as such colors look more natural. Course Notes. 7–es.
                                                                             Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lem-
                                                                                pitsky. 2020. Neural Point-Based Graphics. In Computer Vision–ECCV 2020: 16th
5 LIMITATIONS European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII 16.
                                                                                Springer, 696–712.
Due to the vast amount of dierent parameters, the search of suitable Mario Botsch, Alexander Hornung, Matthias Zwicker, and Leif Kobbelt. 2005. High- hyper parameters is non-trivial. We have to balance learning rates of quality surface splatting on today’s GPUs. In Proceedings Eurographics/IEEE VGTC the texture color, structural parameters, tone mapping settings, and Symposium Point-Based Graphics, 2005. IEEE, 17–141.
                                                                             Giang Bui, Truc Le, Brittany Morago, and Ye Duan. 2018. Point-Based Rendering
neural network weights. An extensive grid-search was necessary to Enhancement via Deep Learning. The Visual Computer 34, 6 (2018), 829–841. nd viable settings that work well for all of our scenes. Gaurav Chaurasia, Sylvain Duchene, Olga Sorkine-Hornung, and George Drettakis.
                                                                                2013. Depth synthesis and local warps for plausible image-based navigation. ACM
Furthermore, the optimization of point position is not stable for Transactions on Graphics (TOG) 32, 3 (2013), 1–12. moderate to large learning rates. Our pipeline therefore requires a Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec Jacobson, reasonable initial point cloud, for example, by a multi view stereo and Sanja Fidler. 2019. Learning to Predict 3D Objects With an Interpolation-Based
                                                                                Dierentiable Renderer. Advances in Neural Information Processing Systems 32 (2019),
system or a LiDAR scanner. We believe that this problem is caused 9609–9619. by the gradient approximation during rasterization. It works well Peng Dai, Yinda Zhang, Zhuwen Li, Shuaicheng Liu, and Bing Zeng. 2020. Neural for camera model and pose optimization because the spatial gradi- Point Cloud Rendering via Multi-Plane Projection. In Proceedings of the IEEE/CVF
                                                                                Conference on Computer Vision and Pattern Recognition (CVPR).
ent of thousands of points are averaged in one optimizer step. For Paul Debevec, Yizhou Yu, and George Boshokov. 1998. Ecient view-dependent IBR the positional point-gradients however, only a single approximate with projective texture-mapping. In EG Rendering Workshop, Vol. 4.
                                                                             Paul E Debevec and Jitendra Malik. 2008. Recovering High Dynamic Range Radiance
gradient is used to update its coordinates. A very low learning rate Maps from Photographs. In ACM SIGGRAPH 2008 classes. 1–10. is therefore required to average the point gradient over time. Eric Enderton, Erik Sintorn, Peter Shirley, and David Luebke. 2010. Stochastic Trans- Finally, due to the one-pixel point rendering, holes appear when parency. IEEE transactions on visualization and computer graphics 17, 8 (2010),
                                                                                1036–1047.
the camera is moved too close to an object or the point cloud is very Jakob Engel, Vladlen Koltun, and Daniel Cremers. 2017. Direct Sparse Odometry. IEEE sparse, because the neural network architecture can only ll holes up transactions on pattern analysis and machine intelligence 40, 3 (2017), 611–625. to a certain size threshold. When the camera moves, also ickering John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. 2016. DeepStereo:
                                                                                Learning to Predict New Views from the World’s Imagery. In Proceedings of the IEEE
becomes noticeable in such situations–the eect is visible in parts conference on computer vision and pattern recognition. 5515–5524. of the accompanying video. Using a deeper neural renderer network Yaroslav Ganin, Daniil Kononenko, Diana Sungatullina, and Victor Lempitsky. 2016.
                                                                                DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation. In European
helps in these cases, at the price of reduced rendering performance. conference on computer vision. Springer, 311–326. Future work should be conducted here, for example one could try Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, and to dynamically generate new points during magnication that have William T Freeman. 2018. Unsupervised Training for 3D Morphable Model Re-
                                                                                gression. In Proceedings of the IEEE Conference on Computer Vision and Pattern
interpolated neural descriptors or add a temporal component to the Recognition. 8377–8386. neural rendering. Daniel B Goldman. 2010. Vignette and Exposure Calibration and Compensation. IEEE
                                                                                transactions on pattern analysis and machine intelligence 32, 12 (2010), 2276–2288.
                                                                             Michael D Grossberg and Shree K Nayar. 2003. Determining the Camera Response
6 CONCLUSION from Images: What is Knowable? IEEE Transactions on pattern analysis and machine
                                                                                intelligence 25, 11 (2003), 1455–1467.
We have presented ADOP, a novel fully dierentiable neural point- Jerey P Grossman and William J Dally. 1998. Point Sample Rendering. In Eurographics based rendering pipeline that uses a set of calibrated images and Workshop on Rendering Techniques. Springer, 181–192. 99:12 • Rückert et al. Christian Günther, Thomas Kanzok, Lars Linsen, and Paul Rosenthal. 2013. A GPGPU- Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, based pipeline for accelerated rendering of point clouds. (2013). and Yaser Sheikh. 2019. Neural Volumes: Learning Dynamic Renderable Volumes John Hable. 2010. Filmic Tonemapping for Real-time Rendering. In Siggraph 2010 Color From Images. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1–14. Course. Matthew M Loper and Michael J Black. 2014. OpenDR: An approximate dierentiable Zhizhong Han, Chao Chen, Yu-Shen Liu, and Matthias Zwicker. 2020. DRWR: A renderer. In European Conference on Computer Vision. Springer, 154–169. Dierentiable Renderer without Rendering for Unsupervised 3D Structure Learning Ricardo Marroquim, Martin Kraus, and Paulo Roma Cavalcanti. 2007. Ecient Point- from Silhouette Images. In International Conference on Machine Learning 2020. Based Rendering Using Image Reconstruction.. In PBG@ Eurographics. 101–108. Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Gabriel Brostow. 2018. Deep Blending for Free-Viewpoint Image-Based Rendering. Dosovitskiy, and Daniel Duckworth. 2021. NeRF in the Wild: Neural Radiance Fields ACM Transactions on Graphics (TOG) 37, 6 (2018), 1–15. for Unconstrained Photo Collections. In Proceedings of the IEEE/CVF Conference on Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. 2019. Escaping Plato’s Cave: 3D Computer Vision and Pattern Recognition. 7210–7219. Shape From Adversarial Rendering. In Proceedings of the IEEE/CVF International Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Conference on Computer Vision. 9984–9993. Geiger. 2019. Occupancy Networks: Learning 3D Reconstruction in Function Space. Xin Huang, Qi Zhang, Feng Ying, Hongdong Li, Xuan Wang, and Qing Wang. 2021. HDR- In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. NeRF: High Dynamic Range Neural Radiance Fields. arXiv preprint arXiv:2111.14451 4460–4470. (2021). Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Eldar Insafutdinov and Alexey Dosovitskiy. 2018. Unsupervised learning of shape Noah Snavely, and Ricardo Martin-Brualla. 2019. Neural rerendering in the wild. In and pose with dierentiable point clouds. In Proceedings of the 32nd International Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Conference on Neural Information Processing Systems. 2807–2817. 6878–6887. Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, and and Jaesik Park. 2021. Self-Calibrating Neural Radiance Fields. In Proceedings of the Jonathan T Barron. 2021. NeRF in the Dark: High Dynamic Range View Synthesis IEEE/CVF International Conference on Computer Vision. 5846–5854. from Noisy Raw Images. arXiv preprint arXiv:2111.13679 (2021). Yangqing Jia, Evan Shelhamer, Je Donahue, Sergey Karayev, Jonathan Long, Ross Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Cae: Convolutional ar- Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local Light Field Fusion: chitecture for fast feature embedding. In Proceedings of the 22nd ACM international Practical View Synthesis With Prescriptive Sampling Guidelines. ACM Transactions conference on Multimedia. 675–678. on Graphics (TOG) 38, 4 (2019), 1–14. Yue Jiang, Dantong Ji, Zhizhong Han, and Matthias Zwicker. 2020. SDFDi: Dieren- Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra- tiable Rendering of Signed Distance Fields for 3D Shape Optimization. In Proceedings mamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1251–1261. for View Synthesis. In European conference on computer vision. Springer, 405–421. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual Losses for Real-Time Raul Mur-Artal and Juan D Tardós. 2017. ORB-SLAM2: An open-source slam system Style Transfer and Super-Resolution. In European conference on computer vision. for monocular, stereo, and rgb-d cameras. IEEE transactions on robotics 33, 5 (2017), Springer, 694–711. 1255–1262. Hiroharu Kato, Deniz Beker, Mihai Morariu, Takahiro Ando, Toru Matsuoka, Wadim Thu Nguyen-Phuoc, Chuan Li, Stephen Balaban, and Yong-Liang Yang. 2018. RenderNet: Kehl, and Adrien Gaidon. 2020. Dierentiable Rendering: A Survey. arXiv preprint A deep convolutional network for dierentiable rendering from 3D shapes. In arXiv:2006.12057 (2020). Proceedings of the 32nd International Conference on Neural Information Processing Hiroharu Kato and Tatsuya Harada. 2019. Learning View Priors for Single-View 3D Systems. 7902–7912. Reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2020. Dif- Pattern Recognition. 9778–9787. ferentiable Volumetric Rendering: Learning Implicit 3D Representations without Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. 2018. Neural 3D Mesh Renderer. 3D Supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and In Proceedings of the IEEE conference on computer vision and pattern recognition. Pattern Recognition. 3504–3515. 3907–3916. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love- Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. grove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, Representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann Pattern Recognition. 165–174. LeCun (Eds.). http://arxiv.org/abs/1412.6980 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Tanks and Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Temples: Benchmarking Large-Scale Scene Reconstruction. ACM Transactions on PyTorch: An imperative style, high-performance deep learning library. Advances in Graphics (ToG) 36, 4 (2017), 1–13. neural information processing systems 32 (2019), 8026–8037. Leif Kobbelt and Mario Botsch. 2004. A Survey of Point-Based Techniques in Computer Hanspeter Pster, Matthias Zwicker, Jeroen Van Baar, and Markus Gross. 2000. Surfels: Graphics. Computers & Graphics 28, 6 (2004), 801–814. Surface elements as rendering primitives. In Proceedings of the 27th annual conference Georgios Kopanas, Julien Philip, Thomas Leimkühler, and George Drettakis. 2021. Point- on Computer graphics and interactive techniques. 335–342. Based Neural Rendering with Per-View Optimization. In Computer Graphics Forum, Ruggero Pintus, Enrico Gobbetti, and Marco Agus. 2011. Real-time rendering of massive Vol. 40. Wiley Online Library, 29–43. unstructured raw point clouds using screen-space operators. In Proceedings of the Christoph Lassner and Michael Zollhöfer. 2021. Pulsar: Ecient Sphere-Based Neural 12th International conference on Virtual Reality, Archaeology and Cultural Heritage. Rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 105–112. Recognition (CVPR). 1440–1449. Francesco Pittaluga, Sanjeev J. Koppal, Sing Bing Kang, and Sudipta N. Sinha. 2019. Re- Hoang-An Le, Thomas Mensink, Partha Das, and Theo Gevers. 2020. Novel View vealing Scenes by Inverting Structure From Motion Reconstructions. In Proceedings Synthesis from Single Images via Point Cloud Transformation. arXiv preprint of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). arXiv:2009.08321 (2020). Reinhold Preiner, Stefan Jeschke, and Michael Wimmer. 2012. Auto Splats: Dynamic Marc Levoy and Turner Whitted. 1985. The Use of Points as a Display Primitive. Citeseer. Point Cloud Visualization on the GPU.. In EGPGV@ Eurographics. 139–148. Chen-Hsuan Lin, Chen Kong, and Simon Lucey. 2018. Learning Ecient Point Cloud Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Generation for Dense 3D Object Reconstruction. In proceedings of the AAAI Confer- Johnson, and Georgia Gkioxari. 2020. Accelerating 3D Deep Learning with Py- ence on Articial Intelligence, Vol. 32. Torch3D. arXiv preprint arXiv:2007.08501 (2020). Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. 2021. BARF: Helge Rhodin, Nadia Robertini, Christian Richardt, Hans-Peter Seidel, and Christian Bundle-Adjusting Neural Radiance Fields. In Proceedings of the IEEE/CVF Interna- Theobalt. 2015. A Versatile Scene Model With Dierentiable Visibility Applied to tional Conference on Computer Vision. 5741–5751. Generative Pose Estimation. In Proceedings of the IEEE International Conference on Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. 2019a. Soft Rasterizer: A Dierentiable Computer Vision. 765–773. Renderer for Image-based 3D Reasoning. In Proceedings of the IEEE/CVF International Gernot Riegler and Vladlen Koltun. 2020. Free View Synthesis. In European Conference Conference on Computer Vision. 7708–7717. on Computer Vision. Springer, 623–640. Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li. 2019b. Learning to Infer Implicit Gernot Riegler and Vladlen Koltun. 2021. Stable View Synthesis. In Proceedings of the Surfaces without 3D Supervision. Advances in Neural Information Processing Systems IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12216–12225. 32 (2019), 8295–8306. Paul Rosenthal and Lars Linsen. 2008. Image-space point cloud rendering. In Proceedings Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, and Zhaopeng Cui. of Computer Graphics International. Citeseer, 136–143.
   2020. DIST: Rendering Deep Implicit Signed Distance Function with Dierentiable Darius Rückert and Marc Stamminger. 2021. Snake-SLAM: Ecient Global Visual Iner-
Sphere Tracing. In Proceedings of the IEEE/CVF Conference on Computer Vision and tial SLAM using Decoupled Nonlinear Optimization. In 2021 International Conference Pattern Recognition. 2019–2028. on Unmanned Aircraft Systems (ICUAS). IEEE, 219–228.
                                                                                                        ADOP: Approximate Dierentiable One-Pixel Point Rendering • 99:13
Johannes L Schönberger and Jan-Michael Frahm. 2016. Structure-from-motion revisited. Springer, 286–301. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4104– Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Josh
   4113. Tenenbaum, and Bill Freeman. 2018. Visual Object Networks: Image Generation
Markus Schütz, Bernhard Kerbl, and Michael Wimmer. 2021. Rendering Point Clouds with Disentangled 3D Representations. Advances in Neural Information Processing with Compute Shaders and Vertex Order Optimization. Computer Graphics Forum Systems 31 (2018), 118–129. (2021). https://doi.org/10.1111/cgf.14345 Matthias Zwicker, Hanspeter Pster, Jeroen Van Baar, and Markus Gross. 2001. Surface Dave Shreiner, Bill The Khronos OpenGL ARB Working Group, et al. 2009. OpenGL Splatting. In Proceedings of the 28th annual conference on Computer graphics and programming guide: the ocial guide to learning OpenGL, versions 3.0 and 3.1. Pearson interactive techniques. 371–378. Education. Harry Shum and Sing Bing Kang. 2000. Review of image-based rendering techniques. In Visual Communications and Image Processing 2000, Vol. 4067. International Society A IMAGE TO POINT CLOUD ALIGNMENT for Optics and Photonics, 2–13. Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Another application of our dierentiable rendering pipeline is the Wetzstein. 2020. Implicit neural representations with periodic activation functions. alignment of camera images to point-clouds that were reconstructed Advances in Neural Information Processing Systems 33 (2020). Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nieûner, Gordon Wetzstein, and by external devices. (Anonymous) provided us a dataset captured Michael Zollhofer. 2019. DeepVoxels: Learning Persistent 3D Feature Embeddings. In by the (Anonymous) mobile scanning platform. This platform con- Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2437–2446.
                                                                                             sists of a high performance LiDAR scanner and four 5472 × 3648
Zhenbo Song, Wayne Chen, Dylan Campbell, and Hongdong Li. 2020. Deep Novel pixel sheye cameras. Their reconstruction software is able to com- View Synthesis from Colored 3D Point Clouds. In European Conference on Computer bine multiple laser scans into a consistent point cloud and provides Vision. Springer, 1–17. Pratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng, and camera pose estimates for panorama generation and point cloud Noah Snavely. 2019. Pushing the Boundaries of View Extrapolation With Multiplane coloring. However, the image to point cloud registration is not per- Images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern fect. Small errors during the SLAM-based tracking and vibrations Recognition. 175–184. Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P due to the hand-held operation result in pose errors in the scale of Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. 2022. Block-NeRF: Scalable millimeters. If we then use these poses for precise operation, such Large Scene Neural View Synthesis. arXiv preprint arXiv:2202.05263 (2022). Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan
                                                                                             as reprojecting the color of neighboring source images into a target
Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nieûner, view, small ghosting artifacts can be observed (see Figure 16 center et al. 2020. State of the Art on Neural Rendering. In Computer Graphics Forum, row). We train our neural rendering pipeline on this oce dataset, Vol. 39. Wiley Online Library, 701–727. Justus Thies, Michael Zollhöfer, and Matthias Nieûner. 2019. Deferred Neural Rendering: which is composed of 688 images and 73M points, to synthesize all Image Synthesis using Neural Texturess. ACM Transactions on Graphics (TOG) 38, 4 captured views. During training, our system also optimizes the cam- (2019), 1–12. era pose of each frame. The rened poses are then used to reproject Justus Thies, Michael Zollhöfer, Christian Theobalt, Marc Stamminger, and Matthias Nieûner. 2020. Image-guided neural object rendering. In 8th International Conference the colors into the same target frame as before (see Figure 16 bottom on Learning Representations. OpenReview. net. row). It can be seen that the ghosting artifacts are mostly eliminated Richard Tucker and Noah Snavely. 2020. Single-View View Synthesis With Multiplane Images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                                                                                             and the synthesized image is a lot sharper. This experiment shows
Recognition. 551–560. that the proposed method is able to perform pixel-perfect alignment Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Jitendra Malik. 2017. Multi-View of sheye camera images to a point cloud of a LiDAR scanner. To Supervision for Single-View Reconstruction via Dierentiable Ray Consistency. In Proceedings of the IEEE conference on computer vision and pattern recognition. our knowledge, no other available dierentiable renderer can fulll 2626–2634. this task, as they assume a pinhole camera model or are not able to Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. 2020. SynSin: handle a point cloud with 73M points. End-to-end View Synthesis from a Single Image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7467–7477. Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. 2015. Empirical Evaluation of Rectied B RUNTIME PERFORMANCE Activations in Convolutional Network. arXiv preprint arXiv:1505.00853 (2015). Zhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou, Pei Sun, Dumitru Erhan, Runtime performance has been a limiting factor for dierentiable Sean Raerty, and Henrik Kretzschmar. 2020. SurfelGAN: Synthesizing Realistic rendering systems in the past [Kato et al. 2020]. Most software Sensor Data for Autonomous Driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). rasterization techniques exceed the 100 ms barrier even for small Wang Yifan, Felice Serena, Shihao Wu, Cengiz Öztireli, and Olga Sorkine-Hornung. scenes and render resolutions [Lassner and Zollhöfer 2021]. This
   2019. Dierentiable Surface Splatting for Point-based Geometry Processing. ACM limits their usefulness in real-world applications such as 3D re-
Transactions on Graphics (TOG) 38, 6 (2019), 1–14. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021. pixelNeRF: Neural construction from high-resolution photographs. Currently, the two Radiance Fields From One or Few Images. In Proceedings of the IEEE/CVF Conference most performant dierentiable rendering methods that are able to on Computer Vision and Pattern Recognition. 4578–4587. Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. 2019.
                                                                                             process point-cloud data are Synsin [Wiles et al. 2020] and Pul-
Free-Form Image Inpainting with Gated Convolution. In Proceedings of the IEEE/CVF sar [Lassner and Zollhöfer 2021]. Synsin, which is the default point International Conference on Computer Vision. 4471–4480. render engine of PyTorch3D [Ravi et al. 2020], splats each point to Sergey Zakharov, Wadim Kehl, Arjun Bhargava, and Adrien Gaidon. 2020. Autolabeling 3D Objects with Dierentiable Rendering of SDF Shape Priors. In Proceedings of the a disk and blends the  nearest points of each pixel into the output IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12224–12233. image. Pulsar converts each point to sphere and blends them with a Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. NeRF++: Analyzing similar approach as Soft Rasterizer [Liu et al. 2019a]. Both methods and Improving Neural Radiance Fields. arXiv preprint arXiv:2010.07492 (2020). Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. are fully dierentiable, meaning that the point position and color The Unreasonable Eectiveness of Deep Features as a Perceptual Metric. In CVPR. can be optimized during rendering. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fye, and Noah Snavely. 2018. Stereo Magnication: Learning View Synthesis Using Multiplane Images. ACM
                                                                                                Table 5 shows the measured GPU frame-time for Synsin, Pulsar,
Transactions on Graphics (TOG) 37, 4 (2018), 1–12. our approach, and OpenGL’s default point rendering with GL_POINTS. Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A Efros. 2016. These timings only include the rasterization itself without the neu- View Synthesis by Appearance Flow. In European conference on computer vision.
                                                                                             ral network and tonemapper described in the previous sections. For
![Figure from page 14](images/page_014_content/img-000.png) ![Figure from page 14](images/page_014_content/img-001.png) 99:14 • Rückert et al.
                           Geometry # Layers Forward Backward Forward Backward Forward Backward Forward Backward
    Synsin Disc Splats 1 856.63 14.14 1692.63 17.15 3859.1 31.16 7342.05 25.73
    Pulsar ( = 0.005) Spheres 1 31.20 3.62 59.69 6.97 129.60 15.06 216.07 16.32
    Pulsar ( = 0.02) Spheres 1 36.81 3.02 70.78 5.80 152.89 12.53 263.59 14.91
    Pulsar ( = 0.05) Spheres 1 53.29 3.09 102.48 5.95 221.35 12.85 379.06 15.19
    GL_POINTS 1-Pixel Points 1 0.3 × 0.54 × 1.09 × 1.85 ×
    Ours 1-Pixel Points 1 0.83 0.62 1.09 0.78 1.59 1.39 2.33 2.34
    Ours 1-Pixel Points 4 1.4 0.77 1.82 1.24 2.52 1.74 3.65 2.8
    Ours + Stoc. Disc. 1-Pixel Points 4 1.28 0.66 1.66 1.07 2.15 1.66 3.08 2.64
                           # Points 1,348,406 2,570,810 5,400,615 10,283,243
Table 5. Forward and backward render-time in milliseconds of a 1920 × 1080 image on a RTX 2080 Ti. In comparison to other dierentiable renderers, our approach is around two magnitudes more eicient. The highlighted row is our approach with stochastic point discarding (see Section 3.1) that we use for scene refinement and novel-view synthesis.
                                                                                   therefore a single layer can already be successfully used. The right
                                                                                   most columns of Table 5 show the forward and backward render
                                                                                   time of a 1920 × 1080 image for a point cloud with around 10M
                                                                                   points. Both Synsin and Pulsar are not real-time capable at such di-
                                                                                   mensions with forward timings of 7342 ms and 209 ms respectively.
                                                                                   Our approach takes two magnitudes less time than Pulsar with a
                                                                                   combined rendering time of 3.65 ms for all four layers. This result
                                                                                   is expected though because previous work has shown that software
                                                                                   one-pixel point rendering can outperform hardware rasterization
                                                                                   techniques [Schütz et al. 2021]. Point splatting and sphere render-
                                                                                   ing is inherently more complex because each point eects multiple
                                                                                   output pixels.
                                                                                      If we enable stochastic discarding (see Section 3.1), the rendering
                                                                                   performance is further increased. The largest gains are achieved
                                                                                   for multi-resolution rendering of large point clouds. For example,
                                                                                   generating a multi-layer image of the cloud with 10.4M points takes
                                                                                   19% less time with our stochastic discarding approach. However, if
                                                                                   only a single image layer is required, the speedup due to stochastic
                                                                                   discarding reduces to 3%, as points are only discarded if they ll less
                                                                                   than one pixel in the output image. The pixels inside the low resolu-
                                                                                   tion layers are much larger and therefore more points are discarded.
                                                                                   In comparison to native GL_POINTS rendering, our approach is
                                                                                   only slightly slower (by about 26%). This is an impressive result,
                                                                                   because we have implemented a three-pass blending approach with
                                                                                   fuzzy depth-test as described in Section 3.1. The GL_POINTS refer-
                                                                                   ence implementation in Table 5 uses a single pass without blending
                                                                                   and standard GL_LESS depth test.
Fig. 16. The initial camera pose estimates of the SLAM-System are slightly misaligned w.r.t. the LiDAR point cloud. Reprojecting the pixel color of several source views into a target view produces ghosting artifacts (center row). Our system is able to optimize the camera poses resulting in almost pixel perfect reprojections (boom row). our method, we also include the rendering time of four layers in dierent resolutions. This is a more fair comparison to the other methods because all four layers are required for the neural render- ing network. The output of Synsin and Pulsar is more complete and

## Document Summary

This document was converted from **03_Research_Papers/Books/ADOP Approximate Differentiable One-Pixel Point RenderingADOP Approximate Differentiable One-Pixel Point Rendering - 2110.06635.pdf** containing 14 pages.

**Usage with Claude Code:**

bash claude code "Collections/03_Research_Papers_Books_ADOP_Approximate_Differentiable_One-Pixel_Point_RenderingADOP_Approximate_Differentiable_One-Pixel_Point_Rendering_-_2110.06635/03_Research_Papers_Books_ADOP_Approximate_Differentiable_One-Pixel_Point_RenderingADOP_Approximate_Differentiable_One-Pixel_Point_Rendering_-_2110.06635.md" ```

**Library Location:** 03_Research_Papers is organized as:
- **Books/**: Complete texts and manuals
- **Articles/**: Papers, documents, and shorter works

*Conversion completed: Mon Aug 18 01:57:07 PM PDT 2025*
