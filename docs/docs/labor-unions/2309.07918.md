---
title: 2309.07918
author: Shanghai AI Laboratory
slug: 2309.07918
reconversion_status: ready_for_reconversion
---
# Unified Human-Scene Interaction via Prompted Chain-of-Contacts

Zeqi Xiao<sup>1, 2</sup>, Tai Wang<sup>1</sup>, Jingbo Wang<sup>1</sup>, Jinkun Cao<sup>3</sup>, Wenwei Zhang<sup>1</sup>, Bo Dai<sup>1</sup>, Dahua Lin<sup>1</sup>, and Jiangmiao Pang<sup>1,⊠</sup>

<sup>1</sup>Shanghai AI Laboratory <sup>2</sup>S-Lab, Nanyang Technological University <sup>3</sup>Carnegie Mellon University

{zeqixiao1, taiwang.me, jinkuncao, pangjiangmiao, doubledaibo}@gmail.com, {wj020, dhlin}@ie.cuhk.edu.hk, {wenwei001}@ntu.edu.sg

## ![](_page_0_Picture_6.jpeg)

Figure 1: UniHSI supports unified and long-horizon control following language commands, enjoying impressive features like fine-granularity control, diverse interactions with the same object, and multi-obj interaction.

#### **ABSTRACT**

Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of HSI. This paper presents a unified HSI framework, *UniHSI*, which supports unified control of diverse interactions through language commands. This framework is built upon the definition of interaction as *Chain of Contacts* (CoC): steps of human joint-object part pairs, which is inspired by the strong correlation between interaction types and human-object contact regions. Based on the definition, UniHSI constitutes a *Large Language Model (LLM) Planner* to trans-

<sup>⊠</sup> Corresponding author.

late language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To facilitate training and evaluation, we collect a new dataset named *Scene Plan* that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes. The project page is at https://github.com/Open Robot Lab/UniHSI.

### 1 Introduction

Human-Scene Interaction (HSI) constitutes a crucial element in various applications, including embodied AI and virtual reality. Despite the great efforts in this domain to promote motion quality (Holden et al., 2017; Starke et al., 2019; 2020; Hassan et al., 2021b; Zhao et al., 2022; Hassan et al., 2021a; Wang et al., 2022a) and physical plausibility (Holden et al., 2017; Starke et al., 2019; 2020; Hassan et al., 2021b; Zhao et al., 2022; Hassan et al., 2021a; Wang et al., 2022a), two key factors, versatile interaction control and the development of a user-friendly interface, are yet to be explored before HSI can be put into practical usage.

This paper aims to provide an HSI system that supports versatile interaction control through language commands, one of the most uniform and accessible interfaces for users. Such a system requires: 1) Aligning language commands with precise interaction execution, 2) Unifying diverse interactions within a single model to ensure scalability. To achieve this, the initial effort involves the uniform definition of different interactions. Inspired by Hassan et al. (2021b), we propose that interaction itself contains a strong prior in the form of human-object contact regions. For example, in the case of "lie down on the bed", it can be interpreted as "first the pelvis contacting the mattress of the bed, then the head contacting the pillow". To this end, we formulate interaction as steps of human joint-object part contact pairs, which we refer to as *Chain of Contacts*. Unlike previous contact-driven methods, which are limited to supporting specific interactions through manual design, our interaction definition is generalizable to versatile interactions and capable of modeling multi-round transitions. The recent advancements in Large Language Models have made it possible to translate language commands into the Chain of Contacts. The structured formulation then can be uniformly processed for the downstream controller to execute.

Following the above formulation, we propose **UniHSI**, the first **Uni**fied physical **HSI** framework with language commands as inputs. UniHSI consists of a high-level **LLM Planner** to translate language inputs into the task plans in the form of Chain of Contacts and a low-level **Unified Controller** for executing these plans. Combining language commands and background information such as body joint names and object part layout, we harness prompt engineering techniques to instruct LLMs to plan interaction step by step. To facilitate the unified execution, we devise the Task Parser as the core of the Unified Controller. Following the Chain of Contacts, the Task Parser collects information including joint poses and object point clouds from the physical environment, then formulates them into uniform task observations and task objectives.

As illustrated in Fig. 1, the design of the Unified Controller models whole-body joints and arbitrary parts of objects in the scenarios to enable fine-granularity control and multi-object interaction. With different language commands, we can generate diverse interactions with the same object. Unlike previous methods that only model a limited horizon of interactions, like "sitting down", we design the Task Parser to evaluate the completion of the current steps and sequentially fetch the next step, resulting in multi-round and long-horizon transition control. The Unified control leverages the adversarial motion prior framework (Peng et al., 2021) that uses a motion discriminator for realistic motion synthesis and a physical simulation (Makoviychuk et al., 2021) to ensure physical plausibility.

Another impressive feature of our framework is the training is interaction annotation-free. Previous methods typically require datasets that capture both target objects and the corresponding motion sequences, which demand numerous laboring. In contrast, we leverage the interaction knowledge of LLMs to generate interaction plans. It significantly reduces the annotation requirements and makes versatile interaction training feasible. To this end, we create a novel dataset named **Scene Plan**. It encompasses thousands of interaction plans based on scenarios constructed from Part Net (Mo et al., 2019) and Scan Net (Dai et al., 2017) datasets. We conduct comprehensive experiments on

| Methods | Unified<br>Interaction | Language<br>Input | Long-horizon<br>Transition | Interaction<br>Annotation-free | Control<br>Joints | Multi-object<br>Interaction |
|----------------------------------|------------------------|-------------------|----------------------------|--------------------------------|-------------------------|-----------------------------|
| NSM Starke et al. (2019) | | | ✓ | | 3 (pelvis, hands) | ✓ |
| SAMP Hassan et al. (2021a) | | | | | 1 (pelvis) | |
| COUCH Zhang et al. (2022b) | | | | | 3 (pelvis, hands) | ✓ |
| HUMANISE Wang et al. (2022b) | ✓ | ✓ | | | - | |
| Scen Diffuser Huang et al. (2023) | ✓ | ✓ | | | - | |
| PADL Juravsky et al. (2022) | | ✓ | ✓ | ✓ | - | |
| Inter Phys Hassan et al. (2023) | | | | | 4 (pelvis, head, hands) | |
## | Ours | ✓ | ✓ | ✓ | ✓ | 15 (whole-body) | ✓ |

Scene Plan. The results illustrate the effectiveness of the model in versatile interaction control and good generalizability on real scanned scenarios.

### 2 RELATED WORKS

**Human motion synthesis.** How to synthesize realistic human behavior is a long-standing topic. Most existing methods focus on promoting the quality and diversity of humanoid movements (Barsoum et al., 2018; Harvey et al., 2020; Pavllo et al., 2018; Yan et al., 2019; Zhang et al., 2022a; Tevet et al., 2023; Zhang et al., 2023b) but do not consider scene influence. Recently, there has been a growing interest in synthesizing motion with human-scene interactions, driven by its applications in various applications like embodied AI and virtual reality. Many previous methods (Holden et al., 2017; Starke et al., 2019; 2020; Hassan et al., 2021b; Zhao et al., 2022; Hassan et al., 2021a; Wang et al., 2022a; Zhang et al., 2022b; Wang et al., 2022b) use data-driven kinematic models to generate static or dynamic interactions. These methods are typically inferior in physical plausibility and prone to synthesizing motions with artifacts, such as penetration, floating, and sliding. The need for additional post-processing to mitigate these artifacts hinders the real-time applicability of these frameworks.

On the other hand, recent advancements in physics-based methods (Peng et al., 2021; 2022; Hassan et al., 2023; Juravsky et al., 2022; Pan et al., 2023) show promising potential in ensuring physical plausibility through the utilization of physics-aware simulators. However, these methods exhibit limitations in the following aspects: 1) Previous physics-based methods for human-scene interactions (Hassan et al., 2023; Pan et al., 2023) require distinct policy networks for each task, hindering their ability to learn versatile interactions within a unified controller. 2) These methods primarily focus on simple action-based control, such as walking and sitting, while overlooking finer-grained details of interaction control. 3) These methods heavily rely on motion sequences with annotated human-scene interactions, posing challenges in acquiring high-quality motion sequences of this nature. In contrast, our UniHSIre-designs different types of human-scene interactions into a uniform representation that can be prompted by world knowledge extracted from our high-level LLM Planner. This design enables us to train a unified controller with versatile interaction skills without the need for annotated human-scene interaction motion sequences. The detailed comparisons of key features are listed in Tab. 1.

Languages in motion synthesis. Incorporating language understanding into human motion control has become a recent research focus. Natural language serves as a universal and human-friendly interface for motion animation, significantly improving efficiency for using motion generation models in real-world applications. Zhang et al. (2022a); Chen et al. (2023) incorporated a diffusion model into the text-to-motion generation framework, achieving remarkable results. Tevet et al. (2022; 2023); Zhang et al. (2023a) leveraged CLIP (Radford et al., 2021) as a robust text encoder in the generation process. Furthermore, Zhang et al. (2023b); Jiang et al. (2023) integrated large language models for unified motion synthesis and multimodal controls.

However, these methods primarily focus on scene-agnostic motion synthesis. Generating humanscene interactions using language commands poses additional challenges because the output movements must align with the commands and be coherent with the environment. Zhao et al. (2022) generates static interaction gestures through rule-based mapping of language commands to specific tasks. Juravsky et al. (2022) utilized BERT (Devlin et al., 2019) to infer language commands, but their method requires pre-defined tasks and different low-level policies for task execution. Wang et al. (2022b) unified various tasks in a CVAE (Yao et al., 2022) network with a language interface, but their performance was limited due to challenges in grounding target objects and contact areas for the characters. In contrast, UniHSI utilizes large language models to transfer language commands into the formation of *Chain of Contacts* and design a robust unified controller to execute versatile interaction based on the structured formation.

### 3 METHODOLOGY

As shown in Fig. 2, UniHSI supports versatile human-scene interaction control following language commands. In the following subsections, we first illustrate how we design the unified interaction formulation as a Chain of Contacts (Sec. 3.1). Then we show how we translate language commands into the unified formulation by the LLM Planner (Sec. 3.2). Finally, we elaborate on the construction of the Unified Controller (Sec. 3.3).

#### 3.1 CHAIN OF CONTACTS

The initial effort of UniHSI lies in the unified formulation of interaction. Inspired by Hassan et al. (2021b) which infers contact regions of humans and objects based on the interaction gestures of humans, there exists high coherence between the contact regions and the interaction types. To this end, we can universally define interaction as a Chain of Contacts $\mathcal{C}$ , with the formulation as

## $$C = \{S_1, S_2, \dots\},\tag{1}$$

where $\mathcal{S}_i$ is the $i^{th}$ contact step. Each step $\mathcal{S}$ includes several contact pairs. For each contact, we control whether a joint contact the corresponding object part and the direction of the contact. We construct each contact pair with five elements: an object o, an object part p, a humanoid joint j, the contact type c of j and p, and the relative direction d from j to p. The contact type includes "contact", "not contact", and "not care". The relative direction includes "up", "down", "front", "back", "left" and "right". For example, one contact unit $\{o, p, j, c, d\}$ could be $\{\text{chair}, \text{ seat surface}, \text{ pelvis}, \text{ contact}, \text{ up}\}$ . In this way, we can formulate each $\mathcal{S}$ as

$$S = \{\{o_1, p_1, j_1, c_1, d_1\}, \{o_2, j_2, p_2, c_2, d_2\}, \dots\}.$$
## (2)

The chain of contacts is the output of the LLM Planner and the input of the Unified Controller.

#### 3.2 LARGE LANGUAGE MODEL PLANNER

We leverage LLMs as our planners to infer language commands $\mathcal{L}$ into manageable plans $\mathcal{C}$ . As shown in Fig. 3, the inputs of the LLM Planner include language commands $\mathcal{L}$ , background scenario information $\mathcal{B}$ , humanoid joint information $\mathcal{J}$ together with pre-set instructions, rules and examples. Specifically, $\mathcal{B}$ includes several objects $\mathcal{O}$ and their optional spatial layouts. Each object consists of several parts $\mathcal{P}$ , *i.e.* a chair could consist of arms, the back, and the seat. The humanoid joint information is pre-defined for all scenarios. We use prompt engineering to combine these elements together and instruct ChatGPT (OpenAI, 2020) to output task plans. By modifying instructions in the prompts, we can generate specified numbers of plans for diverse ways of interactions. We can also let LLMs automatically generate plausible commands given the scenes. In this way, we build our interaction datasets for training and evaluation of the Unified Controller.

### 3.3 Unified Controller

The Unified Controller takes multi-step plans C and background scenarios in the form of meshes and point clouds as input, and outputs realistic movements coherent to the environments.

**Preliminary.** We build the controller upon the Adversarial Motion Priors (AMP) method (Peng et al., 2021). AMP is a goal-conditioned reinforcement learning framework incorporated with an adversarial discriminator to model the motion prior. Its objective is defined by a reward function $R(\cdot)$ as

$$R(s_t, a_t, s_{t+1}, \mathcal{G}) = w^G R^G(s_t, a_t, s_{t+1}, \mathcal{G}) + w^S R^S(s_t, s_{t+1}).$$
## (3)

## ![](_page_4_Figure_1.jpeg)

Figure 2: An overview of UniHSI. The whole pipeline consists of two major components: the LLM Planner and the Unified Controller. The LLM planner takes language inputs $\mathcal L$ and background scenario information $\mathcal B$ as inputs and outputs multi-step plan $\mathcal C$ in the form of a Chain of Contacts. The Unified Controller then executes $\mathcal C$ step-by-step and output interaction movements.

The task reward $R^G$ defines the high-level goal $\mathcal G$ an agent should achieve. The style reward $R^S$ encourages the agent to imitate low-level behaviors from motion datasets. $w^G$ and $w^S$ are empirical weights of $R^G$ and $R^S$ , respectively. $s_t$ , $a_t$ , $s_{t+1}$ are the state at time t, the action at time t, the state at time t+1, respectively. The style reward $R^S$ is modeled using an adversarial discriminator D, which is trained according to the objective:

$$\underset{D}{\operatorname{arg \, min}} \quad -\mathbb{E}_{d^{\mathcal{M}}(\boldsymbol{s}_{t}, \boldsymbol{s}_{t+1})} \left[ \log \left( D(\boldsymbol{s}_{t}^{A}, \boldsymbol{s}_{t+1}^{A}) \right) \right] - \mathbb{E}_{d^{\pi}(\boldsymbol{s}, \boldsymbol{s}_{t+1})} \left[ \log \left( 1 - D(\boldsymbol{s}^{A}, \boldsymbol{s}_{t+1}^{A}) \right) \right] \\ + w^{\operatorname{gp}} \mathbb{E}_{d^{\mathcal{M}}(\boldsymbol{s}, \boldsymbol{s}_{t+1})} \left[ \left\| \nabla_{\phi} D(\phi) \right|_{\phi = (\boldsymbol{s}^{A}, \boldsymbol{s}_{t+1}^{A})} \right\|^{2} \right], \tag{4}$$

where $d^{\mathcal{M}}(s, s_{t+1})$ and $d^{\pi}(s, s_{t+1})$ denote the likelihood of a state transition from $s_t$ to $s_{t+1}$ in the dataset $\mathcal{M}$ and the policy $\pi$ respectively. $w^{\mathrm{gp}}$ is an empirical coefficient to regularize gradient penalty. $s^A = \Phi(s)$ is the observation for discriminator. The style reward $r^S = R^S(\cdot)$ for the policy is then formulated as:

$$R^{S}(s_{t}, s_{t+1}) = -\log(1 - D(s_{t}^{A}, s_{t+1}^{A})).$$
## (5)

We adopt the key design of motion discriminator for realistic motion modeling. Our main contribution to the controller parts lies in the unification of different tasks. As shown in the left part of Fig. 4 (a), AMP (Peng et al., 2021), as well as most of the previous methods (Juravsky et al., 2022;

## ![](_page_5_Figure_1.jpeg)

Figure 3: The process of translating language commands to Chains of Contacts

Zhao et al., 2023), design specified task observations, task objectives, and hyperparameters to train task-specified control policy. In contrast, we unify different tasks into Chains of Contacts and devise a Task Parser to process the uniform representation.

**Task Parser.** As the core of the Unified Controller, the Task Parser is responsible for formulating a Chain of Contacts into uniform task observations and task objectives, and sequentially fetching steps for multi-round interaction execution.

Given one specific contacting pair $\{o, p, j, c, d\}$ , for task observation, the Task Parser collects the corresponding position $\boldsymbol{v}^j \in \mathbb{R}^3$ of the joint j, and point clouds $\boldsymbol{v}^p \in \mathbb{R}^{m \times 3}$ of the object part p from the simulation environment, where m is the point number of point clouds. It selects the nearest point $\boldsymbol{v}^{np} \in \boldsymbol{v}^p$ from $\boldsymbol{v}^p$ to $\boldsymbol{v}^j$ as the target point for contact. Then we formulate task observation of the single pair as $\{\boldsymbol{v}^{np} - \boldsymbol{v}^j, c, d\}$ . For the task observation in the network, we map c and d into digital numbers, but here we still use the same notation for simplicity. Combining these contact pairs together, we get the uniform task observations $s^U = \{\{\boldsymbol{v}_1^{np} - \boldsymbol{v}_1^j, c_1, d_1\}, \{\boldsymbol{v}_2^{np} - \boldsymbol{v}_2^j, c_2, d_2\}, ..., \{\boldsymbol{v}_n^{np} - \boldsymbol{v}_n^j, c_n, d_n\}\}$ .

The task reward $r^G = R^G(\cdot)$ is the summarization of all contact pair rewards:

$$R^{G} = \sum_{k} w_{k} R_{k}, \ k = 1, 2, ..., n.$$
## (6)

We model each contact reward $R_k$ according to the contact type $c_k$ . When $c_k = \text{contact}$ , the contact reward encourages the joint j to be close to the part p, satisfying the specified direction d. When $c_k = \text{notcontact}$ , we hope the joint j is not close to the part p. If $c_k = \text{notcare}$ , we directly set the reward to max. Following the idea, the $k^{th}$ contact reward $R_k$ is defined as

$$R_k = \begin{cases} w_{\text{dis}} \exp(-w_{dk}||\boldsymbol{d}_k||) + w_{\text{dir}} \min(\overline{\boldsymbol{d}}_k \hat{\boldsymbol{d}}_k, 0), & c_k = \text{contact} \\ 1 - \exp(-w_{dk}||\boldsymbol{d}_k||), & c_k = \text{not contact} \\ 1, & c_k = \text{not care} \end{cases}$$
## (7)

where $d_k = v^{np} - v^j$ indicates the $k^{\text{th}}$ distance vector, $\overline{d}_k$ is the normalized unit vector of $d_k$ , $\hat{d}_k$ is the unit direction vector specified by direction $d_k$ , and $c_k$ is the $k^{\text{th}}$ contact type. $w_{dis}$ , $w_{dir}$ , $w_{dk}$ are corresponding weights. Here we set the scale interval of $R_k$ as [0,1]. We use exp to ensure the scale interval.

Similar to the formulation of contact reward, the Task Parser considers a step to be completed if All k = 1, 2, ..., n satisfy

$$\begin{cases} ||\boldsymbol{d}_{k}|| < 0.1 \text{ and } \overline{\boldsymbol{d}}_{k} \hat{\boldsymbol{d}}_{k} > 0.8, & c_{k} = \text{contact} \\ ||\boldsymbol{d}_{k}|| > 0.1, & c_{k} = \text{not contact} \\ \text{True.} & c_{k} = \text{not care} \end{cases}$$
## (8)

## ![](_page_6_Figure_1.jpeg)

Figure 4: Design Visualization. (a) Our framework unies different tasks into uniform designs through the unied interface of CoC and the Task Parser. (b) An illustration of ego-centric heightmap in one Scan Net scene in the form of green dots. Darker green indicates a larger height.

## Table 2: Performance on Scene Plan

| Success Rate (%) | | Contact Error ↓ | | | Success Steps | | | | |
|----------------------------|--------|-----------------|------|--------|---------------|-------|--------|-----|------|
| Source | Simple | Mid | Hard | Simple | Mid | Hard | Simple | Mid | Hard |
| Part Net (Mo et al., 2019) | 91.1 | 63.2 | 39.7 | 0.038 | 0.073 | 0.101 | 2.3 | 4.5 | 6.1 |
| wo Adaptive Weights | 21.2 | 5.3 | 0.1 | 0.181 | 0.312 | 0.487 | 0.7 | 1.2 | 0.0 |
| wo Heightmap | 61.6 | 45.7 | 0.0 | 0.068 | 0.076 | - | 1.8 | 3.4 | 0.0 |
| Scan Net (Dai et al., 2017) | 76.1 | 43.5 | 32.2 | 0.067 | 0.101 | 0.311 | 1.8 | 2.9 | 4.9 |

Adaptive Contact Weights. The formulation of 6 includes lots of weights to balance different contact parts of the rewards. Empirically setting them requires much laboring and is not generalizable to versatile tasks. To this end, we adaptively set these weights based on the current optimization process. The basic idea is to give parts of rewards that are hard to optimize high rewards while lowering the weights of easier parts. Given R1, R2, ..., Rn, we set their weights to

$$w_k = (1 - R_k)/(n - \sum_{k=1,2,\dots, n} R_k + e),$$
## (9)

Ego-centric Heightmap. To avoid collision when navigating or interacting in a scene, the humanoid must be scene-aware. Here we adopt similar approaches in Wang et al. (2022a); Won et al. (2022); Starke et al. (2019) that sample surrounding information as the humanoid's observation. We build a square ego-centric heightmap that samples the height of surrounding objects (Fig. 4 (b)). It is rather important when we extend our methods into real scanned scenarios such as Scan Net (Dai et al., 2017) in which various objects are densely distributed and easy to be collided.

## 4 EXPERIMENTS

Existing methods and datasets related to human-scene interactions mainly focus on short and limited tasks (Hassan et al., 2021a; Peng et al., 2021; Hassan et al., 2023; Wang et al., 2022b; Araujo ´ et al., 2023). To the best of our knowledge, we are the rst method that supports arbitrary horizon interactions with language commands as input. To this end, we construct a novel dataset for training and evaluation. We also conduct various ablations with vanilla baselines and key components of our framework.

### 4.1 DATASETS AND METRICS

To facilitate the training and evaluation of UniHSI, we construct a novel dataset named Scene Plan, that comprises various indoor scenarios and interaction plans. The indoor scenarios are collected and constructed from object datasets and scanned scene datasets. We leverage our LLM Planner to generate interaction plans based on these scenarios. The training of our model also requires motion datasets to train the motion discriminator which constrains our agents to interact in natural ways. We follow the practice of Hassan et al. (2023) to evaluate the performance of our method.

## ![](_page_7_Figure_1.jpeg)

Figure 5: Visual examples of tasks of different difficulty

**Scene Plan.** we collect scenarios for Scene Plan from the Part Net (Mo et al., 2019) and Scan Net (Dai et al., 2017) datasets. Part Net (Mo et al., 2019) provides various indoor objects with fine-grained part annotations, which are suitable for LLM Planners. We collect diverse objects from Part Net and composite them into scenarios. Scan Net (Dai et al., 2017) involves diverse scenes scanned from real indoor rooms. We collect scenes and annotate key object parts based on their fragmented area annotations. We then use LLM Planner to generate various interaction plans based on these scenarios. Specifically, the training set comprises 40 objects from Part Net. We generate 5~20 plausible interaction steps for each of these objects. During training, we randomly select 1~4 objects from the training set for each scenario and randomly pick the steps of these objects as interaction plans. The evaluation set comprises 40 objects from Part Net and 10 scenarios from Scan Net. We manually or randomly construct objects from Part Net into scenarios. We generated a total of 1,040 interaction plans on Part Net scenarios and 100 interaction plans on Scan Net scenarios for evaluation. These plans involve a diverse range of interactions, including versatile interaction types, various horizons, and multiple objects.

**Motion Datasets.** We use the SAMP dataset (Hassan et al., 2021a) and CIRCLE (Araújo et al., 2023) as our motion dataset. SAMP includes 100 minutes of Mo Cap clips, covering common walking, sitting, and lying down behaviors. CIRCLE contains 10 hours (more than 7000 sequences) of both right and left-hand reaching data. We use all clips in SAMP and pick 20 representative clips in CIRCLE for training.

**Metrics.** We follow Hassan et al. (2023) that uses *Success Rate* and *Contact Error* (*Precision* in Hassan et al. (2023)) as the main metrics to quantitatively measure the quality of interactions. Success Rate records the percentage of trials that humanoids successfully complete every step of the whole plan. In our experiments, we consider a trial of n steps to be successfully completed if humanoids finish it in $n \times 10$ seconds. We also record the average error of all contact pairs, which can be formulated as

Contact Error = 
$$\sum_{i, c_i \neq 0} er_i / \sum_{i, c_i \neq 0} 1$$
## , (10)

where

$$er_i = \begin{cases} ||\boldsymbol{d}_k||, & c_i = \text{contact} \\ \min(0.3 - ||\boldsymbol{d}_k||, 0), & c_i = \text{not contact} \end{cases}$$
## (11)

We further record the convergence time of different training settings. We test the success rate every 5k steps and consider the training is converged when the successful rate is higher than 80%. We further record *Success Steps* and *Convergencee time* in the following experiments as a supplement to evaluate our model in detail.

#### 4.2 IMPLEMENTATION DETAILS

We follow Peng et al. (2021) to construct the low-level controller, which includes a policy network and a discriminator network. The policy network comprises a critic network and an actor network, both of which are modeled as a CNN layer followed by two MLP layers with [1024, 1024, 512] units. The discriminator is modeled with two MLP layers having [1024, 1024, 512] units. We use

| Methods | | Success Rate (%) ↑ | | Contact Error ↓ | | |
|-------------------------------------------|------|--------------------|-------|-----------------|----------|-------|
| | Sit | Lie Down | Reach | Sit | Lie Down | Reach |
| NSM - Sit (Starke et al., 2019) | 75.0 | - | - | 0.19 | - | - |
| SAMP - Sit (Hassan et al., 2021a) | 75.0 | - | - | 0.06 | - | - |
| SAMP - Lie Down(Hassan et al., 2021a) | - | 50.0 | - | - | 0.05 | - |
| Inter Phys - Sit (Hassan et al., 2023) | 93.7 | - | - | 0.09 | - | - |
| Inter Phys - Lie Down(Hassan et al., 2023) | - | 80.0 | - | - | 0.30 | - |
| AMP (Peng et al., 2021)-Sit | 77.3 | - | - | 0.090 | - | - |
| AMP-Lie Down | - | 21.3 | - | - | 0.112 | - |
| AMP-Reach | - | - | 98.1 | - | - | 0.016 |
| AMP-Vanilla Combination (VC) | 62.5 | 20.1 | 90.3 | 0.093 | 0.108 | 0.032 |
| UniHSI | 94.3 | 81.5 | 97.5 | 0.032 | 0.061 | 0.016 |

Table 3: Ablation on baseline models and vanilla implementations

PPO (Schulman et al., 2017) as the base reinforcement learning algorithm for policy training and employ the Adam optimizer Kingma & Ba (2014) with a learning rate of 2e-5. Our experiments are conducted on the Isaac Gym (Makoviychuk et al., 2021) simulator using a single Nvidia A100 GPU with 8192 parallel environments.

### 4.3 PERFORMANCE ON SCENEPLAN

We initially conduct experiments on our Scene Plan dataset. To measure performance in detail, we categorize task plans into three levels: simple, medium, and hard. We classify plans within 3 steps as simple tasks, those with more than 3 steps but with a single object as medium-level tasks, and those with multiple objects as hard tasks. Simple task plans typically involve straightforward interactions, such as getting close to a bed, sitting on the bed, and then lying on the bed. Mediumlevel plans encompass more diverse interactions with multiple rounds of transitions, like resting hands on armrests or placing the left foot on the right knee. Hard task plans introduce multiple objects, requiring agents to navigate between these objects and interact with one or more objects simultaneously. Examples of tasks with varying difculty are illustrated in Fig. 5. As shown in Table 2, UniHSI performs well in simple task plans, exhibiting a high Success Rate and low error. However, as task plans become more diverse and complex, the performance of our model experiences a noticeable decline. Nevertheless, the Success Steps metric continues to increase, indicating that our model still performs well in parts of the plans. It's important to note that the scenarios in the Scene Plan test set are unseen during training, and scenes from Scan Net even exhibit a modality gap with the training set. The overall performance on the test set demonstrates the versatile capability, robustness, and generalization ability of UniHSI.

### 4.4 ABLATION STUDIES

We rst perform ablations on the key components of our model (Sec. 4.4.1). Next, we validate the superiority of our unied design compared to previous methods.

### 4.4.1 KEY COMPONENTS ABLATION

Adaptive Weights. As shown in Table 2, the removal of Adaptive Weights from our controller results in a signicant drop in performance across all levels of tasks. This is because Adaptive Weights are essential for balancing the optimization of different contact pairs. When certain pairs are not used or are easy to learn in specic interactions, Adaptive Weights automatically reduce their weights and increase the weights of less straightforward pairs, making it particularly important as tasks become more complex.

Ego-centric Heightmap. Eliminating the Ego-centric Heightmap also leads to performance degradation, particularly for hard tasks. The Ego-centric Heightmap plays a crucial role in agent navigation within scenes. It enables the perception of surroundings and prevents agents from colliding with other objects. This explains why our model faces difculties with hard tasks that involve complex

## ![](_page_9_Figure_1.jpeg)

Figure 6: **Visual Ablations.** (a) Our model performs more naturally and accurately than the baselines in tasks like "Sit" and "Lie Down". (b) The training of our model is more efficient and effective.

scenarios with numerous objects. The Ego-centric Heightmap is also the key factor enabling our model to generalize to real scanned scenes.

#### 4.4.2 DESIGN COMPARISON WITH PREVIOUS METHODS

Baseline Settings. In comparison with previous methods, we select simple interaction tasks that can be accomplished by those methods, including "Sit," "Lie Down," and "Reach". Due to differences in training datasets and the unavailability of code for the most related method (Hassan et al., 2023), direct comparisons are challenging. We report their results from (Hassan et al., 2023) for a rough comparison. To ensure fairness and clarity in the comparison, we integrate the key designs of Hassan et al. (2023) into our baseline model (Peng et al., 2021). We manually formulate task observations and task objectives for different tasks following Hassan et al. (2023). These task objectives can be generally expressed as:

$$R^{G} = \begin{cases} 0.7R^{\text{near}} + 0.3R^{\text{far}}, & \text{if distance} > 0.5\text{m} \\ 0.7R^{\text{near}} + 0.3, & \text{otherwise} \end{cases}$$
## (12)

Here $R^{\text{far}}$ encourages the character to move towards the object, and $R^{\text{near}}$ encourages the character to perform specific tasks once it is close, requiring task-specific designs.

We also implement a vanilla baseline that combines different tasks within the same model. In this setup, we concatenate the task observations of various tasks and encode task choices within the task observations. During training, we randomly select tasks and train them with their corresponding rewards

**Quantitative Comparison.** As shown in Table 3, UniHSI achieves higher or comparable performance across all these metrics. The performance gain increases as tasks become more complex, with the most significant improvement observed in the "Lie Down" task, the most complex among them. The primary reason for our model outperforming baseline implementations is the decomposition of tasks into multi-step plans, significantly reducing the complexity of interaction tasks. Moreover, different tasks often share similar motion transitions. For instance, the "Lie Down" task typically involves first sitting down on the target object. As a result, our model can learn complex tasks more easily and efficiently adapt to different tasks. Figure 6 (b) demonstrates that our methods not only achieve a higher success rate but also converge more quickly than baseline implementations.

Another noteworthy result is that the vanilla combination of AMP (Peng et al., 2021) leads to a noticeable drop in performance across all tasks, whereas our methods remain effective. This is due to the fact that the vanilla combination of different tasks results in interference and inefficient training. In contrast, our approach unifies these tasks into uniform representations and optimizes them with consistent objectives, effectively benefiting multi-task learning.

Qualitative Comparison. In Figure 6 (a), we qualitatively visualize the performance of baseline methods and our model. Our model performs more naturally and accurately than the baselines in tasks like "Sit" and "Lie Down". This is primarily attributed to the differences in task objectives. Baseline objectives (Eq. 12) model the combination of sub-tasks, such as walking close and sitting down, as simultaneous processes. Consequently, agents tend to perform these different goals simultaneously. For example, they may attempt to sit down even if they are not in the correct position or throw themselves like a projectile onto the bed, disregarding the natural task progression. On the other hand, our methods decompose tasks into natural movements through language planners, resulting in more realistic interactions.

#### 5 DISCUSS AND CONCLUSION

In this work, we take a step forward toward the unified HSI system that supports versatile interactions and language commands. By defining interaction as Chain of Contacts, we model different interactions into steps of human joint-object part contact pairs. Following this definition, we build the framework of UniHSI involving a Large Language Planner to translate language commands into prompted CoC and a Unified Controller that turns CoC into uniform task execution. We train and evaluate our controller through our newly developed dataset, Scene Plan, which includes thousands of task plans in diverse scenarios. We validate the effectiveness as well as generalizability through comprehensive experiments conducted on the Scene Plan dataset, which shaw benefit future work to achieve a more versatile and user-accessible HSI system.

**Limitations and Future Work.** Apart from the advantages of our framework, there are a few limitations. First, our framework can only control humanoids to interact with fixed objects. We do not take moving or carrying objects into consideration. Enabling humanoids to interact with movable objects is an important future direction. Besides, we do not integrate LLM seamlessly into the training process. In the current design, we use pre-generated plans. Involving LLM in the training pipeline will promote the scalability of interaction types and make the whole framework more integrated.

### A DETAILED PROMPTING EXAMPLE OF THE LLM PLANNER

As shown in Table. 4. We present the full prompting example of the input and output of the LLM Planner that is demonstrated in Fig. 2 and Fig. 3. The output is generated by OpenAI (2020). In the future study, we will collect a list of language commands and integrate ChatGPT OpenAI (2020) and GPT OpenAI (2023) into the loop to evaluate the performance of the whole framework of UniHSI.

## ![](_page_11_Picture_1.jpeg)

## Figure 7: An example of multi-obj interaction

## ![](_page_11_Picture_3.jpeg)

Figure 8: An example of multi-step interaction with the same object

# B DETAILS OF THE SCENEPLAN

We present three examples of different levels of interaction plans in the Scene Plan in Table. 5, 6, and 7, respectively. Simple-level interaction plans involve interactions within 3 steps and with 1 object. Medium-level interaction plans involve interactions of more than 3 steps and with 1 object. Hardlevel interaction plans involve interactions of more than 3 steps and more than 1 object. Specically, each interaction plan has an item number and two subitems named "obj" and "chain of contacts". The "obj" item includes information about objects like object id, name, and transformation parameters. The "chain of contacts" item includes steps of contact pairs in the form of CoC.

We provide the list of interaction types that are included in the training and evaluation of our framework in Table. 8 and 9.

## ![](_page_12_Figure_1.jpeg)

Figure 9: An example of "multi-agent" interaction. Note: the "multi-agent interaction" can only be achieved at the commands level at the current stage.

### C MORE VISUALIZATIONS

We further provide more quantitative results in Fig. 7, 8, 9.

Table 4: Detailed prompting example of the LLM Planner. It shows a full example of the input

### and output of the LLM Planner demonstrated in Fig. 2 and Fig. 3. Input Instruction: I want to play video games for a while, then go to sleep. Given the instruction, generate 1 task plan according to the following background information, rules, and examples. [start of background Information] The room has OBJECTS: [bed, chair, table, laptop]. The [OBJECT: laptop] is upon the [OBJECT: table]. The [OBJECT: table] is in front of the [OBJECT: chair]. The [OBJECT: bed] is several meters away from [OBJECT: table]. The human is several meters away from these objects. The [OBJECT: bed] has PARTS: [pillow, mattress]. The [OBJECT: chair] has PARTS: [back soft surface, seat surface, left armrest hard surface, right armrest hard surface]. The [OBJECT: table] has PARTS: [board]. The [OBJECT: laptop] has PARTS: [screen, keyboard]. The human has JOINTS: [pelvis, left hip, left knee, left foot, right hip, right knee, right foot, torso, head, left shoulder, left elbow, left hand, right shoulder, right elbow, right hand]. [end of background Information] [start of rules] 1. Each task plan should be composite into detailed steps. If the human is not close to the target object, the rst step should be to get close to the object. 2. Each step should contain meaningful joint-part pairs. 3. Each joint-part pair should be formatted into {JOINT, PART of OBJECT, Contact type, Contact Direcion}. Or if the step is getting close to an object, the step should be formatted into {none, none, none, none, relative direction of the target object}. JOINT in the format should be replaced by JOINT in the background information. Important: PART in the format should only be replaced by PART or JOINT in the background information. The choices of Contact type include [contact, not contact]. The choices of Contact Direction include [front, back, left, right, up, down, none]. 4. Be plausible. Do not generate uncommon interactions. 5. Only interact with still objects. Do not move objects. [end of rules] [start of an example] [start of background Information] The room has a [OBJECT: chair]. The [OBJECT: chair] has PARTS: [back soft surface, armrest hard surface, armrest hard surface, seat surface]. The human has JOINTS: [pelvis, left hip, left knee, left foot, right hip, right knee, right foot, torso, head, left shoulder, left elbow, left hand, right shoulder, right elbow, right hand]. The human is several meters away from the [OBJECT: chair]. [end of background Information] Step 1: Get close to the chair. Pair 1: {none, none, none, none, front} Step 2: Sit on the chair. Pair 1: {chair, seat surface, pelvis, contact, up}. Step 3: Rest the right hand on the right arm of the chair. Pair 1: {chair, seat surfac, pelvis, contact, up}. Pair 2: {chair, armrest hard surface, right hand, contact, up}. Step 4: Place the right foot on the left knee. Pair 1: {chair, seat surface, pelvis, contact, up}. Pair 2: {chair, left knee, right foot, contact, up}. Step 5: Stand up. Pair 1: {none, none, none, none, front} Step 6: Sit on the chair. Pair 1: {chair, seat surface, pelvis, contact, up}. [end of an example] Output Step 1: Get close to the chair. Pair 1: {pelvis, none, none, none, front} Step 2: Sit on the chair. Pair 1: {chair, seat surface, pelvis, contact, up} Step 3: Use the laptop to play video games. Pair 1: {chair, seat surface, pelvis, contact, up}

```
Pair 2: {laptop, keyboard, left hand, contact, up}
Pair 3: {laptop, keyboard, right hand, contact, up}
Step 4: Stand up from the chair.
Pair 1: {chair, none, none, none, front}
Step 5: Get close to the bed.
Pair 1: {bed, none, none, none, front}
Step 6: Sit on the bed.
Pair 1: {bed, mattress, pelvis, contact, down}
Step 7: Lie down on the bed.
Pair 1: {bed, mattress, pelvis, contact, down}
Pair 2: {bed, mattress, torso, contact, down}
Pair 3: {bed, pillow, head, contact, up}
## ```

Table 5: An example of simple-level interaction plans in Scene Plan. Simple-level interaction plans involve interactions within 3 steps and with 1 object.

```
{
     "0000":
     {
          "obj":
          {
## "000":
## {
## "id": "12747",
## "name": "bed",
                         "rotate": [[1.5707963267948966, 0, 0], [0, 0, -1.5707963267948966]],
## "scale": 2.5,
## "transfer": [0,-2,0],
## }
          },
          "chain of contacts": [[["bed000", "none", "none", "none", "front"]],
## [["bed000", "mattress25", "pelvis", "contact", "up"],
## ["bed000", "mattress25", "head", "not contact", "up"]],
## [["bed000", "mattress25", "pelvis", "contact", "up"],
## ["bed000", "mattress25", "left foot", "contact", "up"],
## ["bed000", "mattress25", "right foot", "contact", "up"],
## ["bed000", "mattress25", "head", "contact", "up"]]]
}
## ```

Table 6: An example of medium-level interaction plans in Scene Plan. Medium-level interaction plans involve interactions of more than 3 steps and with 1 object.

```
{
     "0000":
     {
          "obj": {
## "000":{
## "id": "45005",
## "name": "chair",
                         "rotate": [[1.5707963267948966, 0, 0], [0, 0, -1.5707963267948966]],
## "scale": 1.5,
## "transfer": [0,-2,0],
## },
          "chain of contacts": [[["chair000", "none", "none", "none", "front"]],
                                    [["chair000", "seat soft surface42", "pelvis", "contact", "up"]],
                                    [["chair000", "seat soft surface42", "pelvis", "contact", "up"],
                                    ["chair000", "back soft surface47", "torso", "contact", "none"]],
                                    [["chair000", "seat soft surface42", "pelvis", "contact", "up"],
                                    ["chair000", "back soft surface47", "torso", "contact", "none"]],
                                    [["chair000", "seat soft surface42", "pelvis", "contact", "up"],
                                    ["chair000", "arm sofa style44", "left hand", "contact", "up"],
                                    ["chair000", "arm sofa style48", "right hand", "contact", "up"]],
                                    [["chair000", "seat soft surface42", "pelvis", "contact", "up"],
                                    ["chair000", "arm sofa style44", "left hand", "not contact", "up"],
                                    ["chair000", "arm sofa style48", "right hand", "not contact", "up"]],
                                    [["chair000", "seat soft surface42", "pelvis", "contact", "up"],
                                    ["chair000", "left knee", "right foot", "contact", "none"]],
                                    [["chair000", "seat soft surface42", "pelvis", "contact", "up"],
                                    ["chair000", "back soft surface47", "torso", "not contact", "none"]],
## [["chair000", "none", "none", "none", "front"]]]}
}
## ```

Table 7: An example of hard-level interaction plans in Scene Plan. Hard-level interaction plans involve interactions of more than 3 steps and more than 1 object.

```
{
     "0000":
     {
          "obj":
          {
## "000":
## {
## "id": "37825",
## "name": "chair",
                    "rotate": [[1.5707963267948966, 0, 0], [0, 0, -1.5707963267948966]],
## "scale": 1.5,
## "transfer": [0,-2,0]
## },
## "001":
## {
## "id": "21980",
## "name": "table",
                    "rotate": [[1.5707963267948966, 0, 0], [0, 0, 1.5707963267948966]],
## "scale": 1.8,
## "transfer": [1,-2,0]
## },
## "002":
## {
## "id": "11873",
## "name": "laptop",
                    "rotate": [[1.5707963267948966, 0, 0], [0, 0, 1.5707963267948966]],
## "scale": 0.6,
## "transfer": [0.8,-2,0.65]
## },
## "003":
## {
## "id": "10873",
## "name": "bed",
                    "rotate": [[1.5707963267948966, 0, 0], [0, 0, -1.5707963267948966]],
## "scale": 3,
## "transfer": [-0.2,-4,0]
          },
          "chain of contacts": [[["chair000", "none", "none", "none", "front"]],
                              [["chair000", "seat soft surface58", "pelvis", "contact", "up"]],
                              [["chair000", "seat soft surface58", "pelvis", "contact", "up"],
                                   ["laptop002", "keyboard15", "left hand", "contact", "none"],
                                   ["laptop002", "keyboard15", "right hand", "contact", "none"]],
## [["chair000", "none", "none", "none", "front"]],
## [["bed003", "none", "none", "none", "front"]],
## [["bed003", "mattress16", "pelvis", "contact", "up"],
## ["bed003", "mattress16", "head", "not contact", "up"]],
## [["bed003", "mattress16", "pelvis", "contact", "up"],
## ["bed003", "mattress16", "left foot", "contact", "up"],
## ["bed003", "mattress16", "right foot", "contact", "up"],
## ["bed003", "pillow17", "head", "contact", "up"]],
## [["bed003", "mattress16", "pelvis", "contact", "up"],
## ["bed003", "mattress16", "head", "not contact", "up"]],
## [["bed003", "none", "none", "none", "front"]]]
## ```

## }

## Table 8: List of Interactions in Scene Plan-1

| Interaction Type | Contact Formation | | | |
|--------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|--|--|--|
| Get close to xxx | {xxx, none, none, none, dir} | | | |
| Stand up | {xxx, none, none, none, dir} | | | |
| Left hand reaches xxx | {xxx, part, left hand, contact, dir} | | | |
| Right hand reaches xxx | {xxx, part, right hand, contact, dir} | | | |
| Both hands reaches xxx | {{xxx, part, left hand, contact, dir},<br>{xxx, part, right hand, contact, dir}} | | | |
| Sit on xxx | {xxx, seat surface, pelvis, contact, up} | | | |
| Sit on xxx, left hand on left arm | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, left arm, left hand, contact, up}} | | | |
| Sit on xxx, right hand on right arm | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, right arm, right hand, contact, up}} | | | |
| Sit on xxx, hands on arms | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, left arm, left hand, contact, none},<br>{xxx, right arm, right hand, contact, none}} | | | |
| Sit on xxx, hands away from arms | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, left arm, left hand, not contact, none},<br>{xxx, right arm, right hand, not contact, none}} | | | |
| Sit on xxx, left elbow on left arm | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, left arm, left elbow, contact, up}} | | | |
| Sit on xxx, right elbow on right arm | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, right arm, right elbow, contact, up}} | | | |
| Sit on xxx, elbows on arms | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, left arm, left elbow, contact, none},<br>{xxx, right arm, right elbow, contact, none}} | | | |
| Sit on xxx, left hand on left knee | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, left knee, left hand, contact, up}} | | | |
| Sit on xxx, right hand on right knee | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, right knee, right hand, contact, up}} | | | |
| Sit on xxx, hands on knees | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, left knee, left hand, contact, none},<br>{xxx, right knee, right hand, contact, none}} | | | |
| Sit on xxx, left hand on stomach | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, pelvis, left hand, contact, none}} | | | |
| Sit on xxx, right hand on stomach | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, pelvis, right hand, contact, none}} | | | |
| Sit on xxx, hands on stomach | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, pelvis, left hand, contact, none},<br>{xxx, pelvis, right hand, contact, none}} | | | |
| Sit on xxx, left foot on right knee | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, right knee, left foot, contact, none}} | | | |
| Sit on xxx, right foot on left knee | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, left knee, right foot, contact, none}} | | | |
| Sit on xxx, lean forward | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, back surface, torso, not contact, none}} | | | |
| Sit on xxx, lean backward | {{xxx, seat surface, pelvis, contact, up},<br>{xxx, back surface, torso, contact, none}} | | | |

## Table 9: List of Interactions in Scene Plan-2

| Interaction Type | Contact Formation |
|-------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Lie on xxx | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up}} |
| Lie on xxx, left knee up | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up<br>{xxx, mattress, left knee, not contact, none}} |
| Lie on xxx, right knee up | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up},<br>{xxx, mattress, right knee, not contact, none}} |
| Lie on xxx, knees up | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up},<br>{xxx, mattress, left knee, not contact, none},<br>{xxx, mattress, right knee, not contact, none}} |
| Lie on xxx, left hand on pillow | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up},<br>{xxx, pillow, left hand, contact, none}} |
| Lie on xxx, right hand on pillow | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up},<br>{xxx, pillow, right hand, contact, none}} |
| Lie on xxx, hands on pillow | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up},<br>{xxx, pillow, left hand, contact, none},<br>{xxx, pillow, right hand, contact, none}} |
| Lie on xxx, on left side | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up},<br>{xxx, mattress, right shoulder, not contact, none}} |
| Lie on xxx, on right side | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up},<br>{xxx, mattress, left shoulder, not contact, none}} |
| Lie on xxx, left foot on right knee | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up},<br>{xxx, right knee, left foot, contact, up}} |
| Lie on xxx, right foot on left knee | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, contact, up},<br>{xxx, left knee, right foot, contact, up}} |
| Lie on xxx, head up | {{xxx, mattress, pelvis, contact, up},<br>{xxx, pillow, head, not contact, none}} |

# REFERENCES

- Joao Pedro Araujo, Jiaman Li, Karthik Vetrivel, Rishi Agarwal, Jiajun Wu, Deepak Gopinath, ´ Alexander William Clegg, and Karen Liu. Circle: Capture in rich contextual environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21211–21221, 2023.
- Emad Barsoum, John Kender, and Zicheng Liu. Hp-gan: Probabilistic 3d human motion prediction via gan. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1418–1427, 2018.
- Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18000–18010, 2023.
- Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5828–5839, 2017.
- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.
- Felix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in- ´ betweening. 39(4), 2020.
- Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J Black. Stochastic scene-aware motion prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11374–11384, 2021a.
- Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael J Black. Populating 3d scenes by learning human-scene interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14708–14718, 2021b.
- Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, and Xue Bin Peng. Synthesizing physical character-scene interactions. ar Xiv preprint ar Xiv:2302.00883, 2023.
- Daniel Holden, Taku Komura, and Jun Saito. Phase-functioned neural networks for character control. ACM Transactions on Graphics (TOG), 36(4):1–13, 2017.
- Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-based generation, optimization, and planning in 3d scenes. CVPR, 2023.
- Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a foreign language. ar Xiv preprint ar Xiv:2306.14795, 2023.
- Jordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng. Padl: Language-directed physicsbased character control. In SIGGRAPH Asia 2022 Conference Papers, pp. 1–9, 2022.
- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ar Xiv preprint ar Xiv:1412.6980, 2014.
- Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance gpu-based physics simulation for robot learning, 2021.
- Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-scale benchmark for ne-grained and hierarchical part-level 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 909–918, 2019.

- OpenAI. Gpt-3: Generative pre-trained transformer 3. https://openai.com/research/gpt-3, 2020.
- OpenAI. Gpt-4 technical report, 2023.
- Liang Pan, Jingbo Wang, Buzhen Huang, Junyu Zhang, Haofan Wang, Xu Tang, and Yangang Wang. Synthesizing physically plausible human motions in 3d scenes. ar Xiv preprint ar Xiv:2308.09036, 2023.
- Dario Pavllo, David Grangier, and Michael Auli. Quaternet: A quaternion-based recurrent model for human motion. ar Xiv preprint ar Xiv:1805.06485, 2018.
- Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. ACM Trans. Graph., 40 (4), July 2021. doi: 10.1145/3450626.3459670. URL http://doi.acm.org/10.1145/3450626.3459670.
- Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. ACM Trans. Graph., 41(4), July 2022.
- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763. PMLR, 2021.
- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ar Xiv preprint ar Xiv:1707.06347, 2017.
- Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for character-scene interactions. ACM Trans. Graph., 38(6):209–1, 2019.
- Sebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Zaman. Local motion phases for learning multi-contact character movements. ACM Transactions on Graphics (TOG), 39(4):54–1, 2020.
- Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXII, pp. 358–374. Springer, 2022.
- Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shar, Daniel Cohen-Or, and Amit H Bermano. Human motion diffusion model. ICLR, 2023.
- Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20460–20469, 2022a.
- Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. Humanise: Language-conditioned human motion generation in 3d scenes. Advances in Neural Information Processing Systems, 35:14959–14971, 2022b.
- Jungdam Won, Deepak Gopinath, and Jessica Hodgins. Physics-based character controllers using conditional vaes. ACM Transactions on Graphics (TOG), 41(4):1–12, 2022.
- Sijie Yan, Zhizhong Li, Yuanjun Xiong, Huahan Yan, and Dahua Lin. Convolutional sequence generation for skeleton-based action synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4394–4402, 2019.
- Heyuan Yao, Zhenhua Song, Baoquan Chen, and Libin Liu. Controlvae: Model-based learning of generative controllers for physics-based characters. ACM Trans. Graph., 2022.
- Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023a.

- Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. ar Xiv, 2022a.
- Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Vladimir Guzov, and Gerard Pons-Moll. Couch: Towards controllable human-chair interactions. In European Conference on Computer Vision, pp. 518–535. Springer, 2022b.
- Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. ar Xiv preprint ar Xiv:2306.10900, 2023b.
- Kaifeng Zhao, Shaofei Wang, Yan Zhang, Thabo Beeler, and Siyu Tang. Compositional humanscene interaction synthesis with semantic control. In European Conference on Computer Vision, pp. 311–327. Springer, 2022.
- Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and Siyu Tang. Synthesizing diverse human motions in 3d indoor scenes. ar Xiv preprint ar Xiv:2305.12411, 2023.
